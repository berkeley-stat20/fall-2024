[
  {
    "objectID": "2-summarizing-data/notes.html",
    "href": "2-summarizing-data/notes.html",
    "title": "Summarization",
    "section": "",
    "text": "In spring of 2022, the New York Times ran the following story1.\n“Consumer Prices” refers to the Consumer Price Index2, a weighted average of the prices of thousands of everyday consumer goods: sports equipment, soft drinks, sneakers, internet service, etc. An increase in that index is thought to correspond to rising inflation.\nLook carefully at the line plot. Which of the following four claims does it support?\nIn truth, this plot could be consistent with all these claims. They are, in turn, a summary, a prediction, a generalization, and a causal claim. This newspaper headline falls squarely in the first category, a summary, which seeks only to describe the data set that is on hand.\nAlthough 8.3% seems like a simple enough number, it is actually summarizing a vast data set of thousands of prices. The process of describing a data set invariably involves summarizing it, either with numerical summaries like 8.3% or with graphical summaries like the line plot show above.\nIn this unit, we will learn to critique and construct descriptive claims made with data. Although they sound elementary, descriptive claims are the most common form of claim made using data. They have the power to move, if not mountains, at least markets.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarization"
    ]
  },
  {
    "objectID": "2-summarizing-data/notes.html#footnotes",
    "href": "2-summarizing-data/notes.html#footnotes",
    "title": "Summarization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSmialek, Jeanna (2022, May 11). Consumer Prices are Still Climbing Rapidly. The New York Times. https://www.nytimes.com/2022/05/11/business/economy/april-2022-cpi.html↩︎\nTo learn more, check out the Wikipedia page on the CPI in the US and the exhaustive description of how the data is collected at the US Bureau of Labor Statistics↩︎",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarization"
    ]
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/tutorial.html",
    "href": "2-summarizing-data/04-conditioning/tutorial.html",
    "title": "Conditioning",
    "section": "",
    "text": "This specific set of notes contains references to many functions from the tidyverse library such as mutate(), select(), arrange(), summarize(). We delve more into some of these functions here. Each one of these are code cells are editable so after running them to see the output, play with them by modifying the code and understanding how these functions work – and how they break!\n\nmutate()\nThis function allows you to create a new column in a dataframe. In typical tidyverse fashion, the first argument is a dataframe. The second argument names and defines how that new column is created. Above, we saw:\n\n\n\n\n\n\n\n\nHere, the first argument, arbuthnot, is piped to mutate() and the second argument, total = boys + girls, creates a new column named total by adding together the columns boys and girls. You can use mutate() to create multiple columns at the same time:\n\n\n\n\n\n\n\n\nNote that switching the order of the two new columns created above such that girl_proportion = girls / total comes before total = boys + girls will produce an error because total is used before it is created.\n\n\nselect()\nThis function is defined above as “selecting a subset of the columns of a data frame.” You’ve seen how to use select() to select or “grab” certain columns, but you can also use select() to omit certain columns. The last block of code can be rewritten to produce the same output by placing a minus sign, -, in front of the columns to omit:\n\n\n\n\n\n\n\n\n\n\narrange()\nThis function arranges the rows of a data frame according to some logical ordering of a column. This ordering is straightforward for numeric columns; the smallest numbers are placed first and ascend to the larger ones. That is, unless you use desc() (which stands for descending).\nBut what if you pass a column of characters to arrange()? Let’s take a look:\n\n\n\n\n\n\n\n\nWhen arranged by species, Adelie penguins come first, followed by Chinstrap, then Gentoo. The penguins aren’t arranged in any specific order within a species, but we can change that by passing another column to arrange(). Passing additional columns to arrange() will systematically break ties. The below code arranges the data frame first by species (alphabetically) and then breaks ties by (ascending) bill length:\n\n\n\n\n\n\n\n\n\n\nsummarize()\nThis function summarizes a data frame into a single row. We can summarize a data frame by taking means or calculating the number of rows as above. We can also do other calculations like taking a median or calculating the variance of a column:\n\n\n\n\n\n\n\n\nHowever, if summarize() is preceded by group_by(), then it will output multiple rows according to groups specified by group_by():\n\n\n\n\n\n\n\n\nThis syntax looks a lot like the syntax used for mutate()! Like in mutate(), we name and define new columns: new_column = formula. The difference is that summarize() returns a brand new data frame that does not contain the columns of the original data frame where mutate() returns a data frame with all columns of the original data frame in addition to the newly defined ones.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "Conditioning"
    ]
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/tutorial.html",
    "href": "2-summarizing-data/02-summarizing-numerical-data/tutorial.html",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Once you have your data in front of you, you’ve seen how we can form visual summaries with ggplot2. But how can we calculate numerical summaries? Furthermore, what if we are concerned about summarizing a portion of our data, like just one species of penguin at a time? We will answer these questions below, and introduce some new functions from the dplyr package (within the tidyverse library) along the way. We’ll also look at how factor() can come in handy while plotting.\nIf you are playing along in RStudio while reading these notes (which we strongly recommend!), be sure to start off by loading the two packages that are necessary for the tutorial by running the following code.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "Summarizing Numerical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/tutorial.html#summary",
    "href": "2-summarizing-data/02-summarizing-numerical-data/tutorial.html#summary",
    "title": "Summarizing Numerical Data",
    "section": "Summary",
    "text": "Summary\nA summary of a summaries…this better be brief! Summaries of numerical data - graphical and numerical - often involve choices of what information to include and what information to omit. These choices involve a degree of judgement and knowledge of the criteria that were used to construct the commonly used statistics and graphics.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "Summarizing Numerical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/tutorial.html",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/tutorial.html",
    "title": "A Grammar of Graphics",
    "section": "",
    "text": "The Grammar of Graphics is an abstract framework of data visualization and so far we’ve kept things abstract. But to construct your own plots you’ll need to be able to implement them in software. We are fortunate to have a translation of the grammar into readible, flexible, and powerful code: the ggplot2 package in R (found within library(tidyverse)).\nThe basic template for describing a plot in code using ggplot2 is:\nWhere DATAFRAME is the name of your data frame, ATTR is the aesthetic attribute that you’ll be using, VAR is a variable in your data frame, and GEOMETRY is the name of a geometry.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "A Grammar of Graphics"
    ]
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/tutorial.html#building-a-ggplot",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/tutorial.html#building-a-ggplot",
    "title": "A Grammar of Graphics",
    "section": "Building a ggplot",
    "text": "Building a ggplot\nLet’s code up the penguin plot bit by bit.\n\n\n\n\n\n\n\n\nThe ggplot() function takes as its first argument a data frame. By itself, there’s not much to look at; it just creates our blank canvas.\nLet’s now indicate how we want to map our aesthetic attributes to variables in that data frame by including them inside the aes() function as the second argument to ggplot().\n\n\n\n\n\n\n\n\nNow we’re getting somewhere. Our axes are now set up with labels and values that are determined by looking into the data frame and checking the range of each variable. Note that we can pass more than one aesthetic mapping inside aes(), we just need to separate them with a comma.\nAll that is left is to actually put the observations on the plot by declaring what geometry we want. Let’s express each penguin as a point by using + to add on a new layer called geom_point().\n\n\n\n\n\n\n\n\nWhen you run this code, R gives you a Warning. These warnings appear when R did something that isn’t a problem necessarily, but thinks you should know about. In this case, it “removed 2 rows containing missing values or values outside the scale range”. In other words, there were two penguins who didn’t have a recorded value for either the x or the y, so there are 342 points on this plot, two less than the 344 rows in the data frame.\nThe plot that is created by this combination of aesthetic mappings and the point geometry is called a scatter plot. This is a good choice of plot for visualizing the relationship between two numerical variables. The original penguin plot, though, visualizes the relationship between three variables. To complete our plot, let’s add in that third variable by mapping species to the color aesthetic attribute.\n\n\n\n\n\n\n\n\nBeautiful!\n\nExercises\n\n\n\n\n\n\nTip\n\n\n\nThe code block that follows is an example of an exercise code block. Modify the code block to achieve the task laid out in the exercise. When you get it correct, it will praise your fine programming skills. If you get stuck, use the “Solution” button.\n\n\n\nA different penguins plot\nThe code cell below creates the penguins plot above. Modify the code to visualize the relationship between the flipper length of penguins on the x-axis (flipper_length_mm) and their body bass on the y-axis (body_mass_g).\n\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     color = species)) +\n    geom_point()\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     color = species)) +\n    geom_point()",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "A Grammar of Graphics"
    ]
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/tutorial.html#communicating-with-graphics",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/tutorial.html#communicating-with-graphics",
    "title": "A Grammar of Graphics",
    "section": "Communicating with Graphics",
    "text": "Communicating with Graphics\nAt this point in the course, you have a bevy of different types of statistical graphics under your belt: scatterplots, histograms, dot plots, violin plots, box plots, density curves, and bar plots of several kinds. You also have a broad framework to explain how these graphics are composed: the Grammar of Graphics. But to what purpose? Why plot data? For whom?\nEvery time you build a plot, you do so with one of two audiences in mind.\n\nYourself.\nSomeone else.\n\nThe process of building understanding from a data set is one that should be driven by curiosity, skepticism, and thoughtfulness. As a data scientist, you’ll find yourself in conversation with your data: asking questions of it, probing it for structure, and seeing how it responds. This thoughtful conversation is called exploratory data analysis (or EDA).\nDuring EDA, the aim is to uncover the shape and structure of your data and to uncover unexpected features. It’s an informal iterative process where you are your own audience. In this setting, you should construct graphics that work best for you.\nAt some point, you’ll find yourself confident in the claim that can be supported by your data and the focus changes to communicating that claim as effectively as possible with a graphic. Here, your audience shifts from yourself to someone else: other scientists, customers, co-workers in a different part of your company, or casual readers. You must consider the context in which they’ll be viewing your graphic: what they know, what they expect, what they want.\nIn this tutorial we’ll focus on three ways to hone the message of your data visualization.\n\nMapping versus setting\nLabels for clarity  \nChoosing a theme\nAnnotations\n\nWe will use two running examples: a line plot of the number of christenings in 17th century London which were collected by a man named John Arbuthnot, 1 and a scatter plot showing the bill sizes of penguins near Palmer Station, Antarctica2.\n\n\n\n\n\n\n\n\n\n\nMapping vs Setting\nOnce you have your first draft of a plot complete and you’re thinking about how to fine tune it for your audience, your eye will turn to the aesthetic attributes. Is that color right? What about the size of the points?\nConsider the first draft of the penguins plot above. It might feel a bit drab to have a large mass of points all in black, the same color as the labels and surrounding text. Let’s make the points blue instead to make them stand out a bit more. Click “Run Code” to see the new plot.\n\n\n\n\n\n\n\n\nThis is . . . unexpected. Why did it color the points red? Is this a bug?\nWhat we’ve stumbled into is a subtle but essential distinction in the grammar of graphics: mapping vs setting. When you put an aesthetic attribute (x, color, size, etc.) into the aes() function, you’re mapping that attribute in the plot to whatever data lives in the corresponding column in the data frame. Mapping was this process:\n\n\n\n\n\nThat’s not what we set out to do here. We just wanted to tweak the look of our aesthetic attributes in a way that doesn’t have anything to do with the data in our data frame. This process is called setting the attribute.\nTo set the color to blue3, we need to make just a small change to the syntax. Let’s move the color = \"blue\" argument outside of the aes() function and into the geom_() function.\n\n\n\n\n\n\n\n\nAh, that looks much better!\nColor isn’t the only aesthetic attribute that you can set. Let’s increase slightly the size of our points by setting the size to three times the size of the default.\n\n\n\n\n\n\n\n\nIt’s not clear that that improves the readability of the plot - there is more overlap between the points now - but the setting worked. One thing we might do to get more visibility on some of the points that were clumped is to use the alpha mapping!\n\n\n\n\n\n\n\n\nHow would it have looked if instead we had mapped the size? When you map, you need a map to a column in your data frame, so let’s map size to species.\n\n\n\n\n\n\n\n\nWe’ve made a mess of our plot now, but it is clear what happened. R looked inside the species column, found a categorical variable with three levels and selected a distinct size for each of those levels.\nAll in all, this is another area in which the grammar of graphic guides clear thinking when constructing a graphic. The aesthetic attributes of a plot can be determined either by variability found in a data set or by fixed values that we set. The former is present in all data visualization but it’s the latter that comes into play when fine-tuning your plot for an audience.\n\n\nAdding Labels for Clarity\nYou may have noticed that ggplot2 pulls the labels for the x-axis, the y-axis, and the legend directly from the names of the variables in the data frame. This results in labels like bill_length_mm, which is no problem when you’re making plots for yourself - you know what this variable means. But will an outside audience?\nYou can change the labels of your plot by adding a labs() layer.\n\n\n\n\n\n\n\n\nAxis and legend labels should be concise and often include the units of measurement. If you find them getting too wordy, know that you can clarify or expand on what is being plotted either in a figure caption or in the accompanying text.\nSpeaking of captions, those a can be added too, as well as a title.\n\n\n\n\n\n\n\n\nThe title of a plot is valuable real estate for communicating the primary story of your plot. It should highlight the most important structure in the data. In the plot above, there appears to be little correspondence between bill length and bill depth. Of course, that changes when we map species to color. Let’s make that plot and title it accordingly.\n\n\n\n\n\n\n\n\nThe practice of using the plot title to convey the main message of the plot is used to powerful effect by the visualization experts at the British publication, The Financial Times4. They have developed a wealth of visualizations to help readers understand what is happening with public health throughout the pandemic. The sobering graphic below uses the title to guide the viewer to the most important visual structure in the plot: the yawning vertical gap between dosage rates between high and low income countries.\n\n\n\n\n\n\n\n\n\nChoosing a Theme\nWhat piece of software did I use to produce the following plot?\n\n\n\n\n\n\n\n\n\nIf you said “Excel”, you are correct! Well… it is Excel in spirit. What makes this plot look like it was made in Excel are a series of small visual choices that were made: the background is a dark gray, there are black horizontal guide lines, and the plot and the legend is surrounded by a black box. Small decisions like these that effect the overall look and feel of the plot are called the theme. The theme used here belongs to the ggthemes library. Here’s the code used.\n\n\n\n\n\n\n\n\nLet’s look at a few more. Do they look familiar?\n\n\n\n\n\n\n\n\n\nThey are, from top to bottom, a theme based on The Wall Street Journal, The Economist, and one of the themes built into ggplot2 packaged called bw for “black and white” (there are no grays). The ggplot2 library has several themes to choose from and yet more live in other packages like ggthemes. To use a theme, all you need to do is add a layer called theme_NAME (e.g. for the black and white theme, use theme_bw()).\nThemeing your plots is an easy way to change the look of your plot. Tinker with a few different themes and considering adding them to your labs5. But, as with all design decisions around graphics, be sure to think about your audience. You might find the Excel aesthetics ugly and dated, but will your audience? If you’re presenting your plot to a community that works with Excel plots day in and day out, that’s probably a sound choice. If you are preparing a plot for submission to a scientific journal, a more minimalist theme is more appropriate.\n\n\nAnnotations\nIn the same way that a title highlights the main message of a plot, you can rely upon visual cues to draw attention to certain components or provide helpful context.\nIn the plot below, we visualize the change in total births recorded in London, England in the 17th century, stored in the arbuthnot data frame. Although these records seem very simple, they actually capture a wealth of historical information. We can add this information to our plot by adding annotations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWere you curious about what caused that dip in the number of births in 17th century London? It happens to correspond to the duration of the English Civil War, when the monarchy was overthrown by a dictator named Oliver Cromwell. This very important context can be conveyed by adding a text label and a line segment through two new annotate() layers.\nWithin ggplot2, annotations are a flexible way to add the context or comparisons that help guide readers in interpreting your data. You can add text, shapes, lines, points. To learn more, consult the documentation6.\nSo if the drop after 1642 corresponds to the English Civil War, what about the spike down around 1666? What about 1703? If you’re curious, explore Wikipedia to find out and add those events as annotations to this plot.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "A Grammar of Graphics"
    ]
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/tutorial.html#summary",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/tutorial.html#summary",
    "title": "A Grammar of Graphics",
    "section": "Summary",
    "text": "Summary\nOver the coming weeks, you’ll get lots of practice with ggplot2. It is an incredibly flexible and powerful piece of software that helps you not just build plots, but think about each of the design decisions that you make. Throughout your journey it is helpful to have a source of documentation to learn about the functionality of the tool. The help files on each function are only so useful. A better option is the official ggplot2 documentation: https://ggplot2.tidyverse.org/.\nThere are two main uses for data visualization. The first is as part of exploratory data analysis, when you are constructing plots for yourself to better understand the structure of the data. When you’re ready to communicate with an outside audience using graphics, more thought is needed: you must think about the difference between mapping and setting, the use of labels for clarity, choosing a theme, and emphasizing elements of the plot using annotations.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "A Grammar of Graphics"
    ]
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/tutorial.html#footnotes",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/tutorial.html#footnotes",
    "title": "A Grammar of Graphics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor data documentation, see the stat20data R package.↩︎\nFor data documentation, see the palmerpenguins R package.↩︎\nTo see the vast (and somewhat strange) palette of color names that R knows, type colors() at the console.↩︎\nVisualization drawn from the excellent collection of graphics at the Financial Times Covid Tracker https://ig.ft.com/coronavirus-vaccine-tracker/.↩︎\nExplore the themes available within ggplot2 by reading the documentation https://ggplot2.tidyverse.org/reference/ggtheme.html. For the additional themes held in the ggthemes package, read this: https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/.↩︎\nDocumentation for annotation layers in ggplot2: https://ggplot2.tidyverse.org/reference/annotate.html.↩︎",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "A Grammar of Graphics"
    ]
  },
  {
    "objectID": "2-summarizing-data/labs/flights/slides.html#section",
    "href": "2-summarizing-data/labs/flights/slides.html#section",
    "title": "Lab: Flights",
    "section": "",
    "text": "This headline is a good introduction to the impact that the pandemic had on flights and on SFO in particular."
  },
  {
    "objectID": "2-summarizing-data/labs/flights/slides.html#section-1",
    "href": "2-summarizing-data/labs/flights/slides.html#section-1",
    "title": "Lab: Flights",
    "section": "",
    "text": "The Bureau of Transportation Statistics is the federal agency that collects and hosts this data for public download.\nAs a side note, if students are interested in pulling down more recent data, this data was pulled using the allflights R package, which they can download and install in RStudio."
  },
  {
    "objectID": "2-summarizing-data/labs/flights/slides.html#section-2",
    "href": "2-summarizing-data/labs/flights/slides.html#section-2",
    "title": "Lab: Flights",
    "section": "",
    "text": "The goal of the “Think before you compute” design of these labs is that the students should be able to do their thinking without the distractions of technology. If you project this data frame, that should be sufficient for them to answer all of the questions in Part I.\n\n\n  \n    −\n    +\n \n 25:00"
  },
  {
    "objectID": "2-summarizing-data/labs/class-survey/part-2.html",
    "href": "2-summarizing-data/labs/class-survey/part-2.html",
    "title": "Class Survey",
    "section": "",
    "text": "You can access the real data from the Stat 20 Class Survey by adding the following line of code to a code cell at the top of your Quarto document.\n\nlibrary(tidyverse)\nclass_survey &lt;- read_csv(\"https://tinyurl.com/stat20-survey\")\n\nTo complete the following questions, you will need to find the columns in the class_survey data frame associated with each variable mentioned.\n\n\nDo students prefer to spend time at the beach or in the mountains?\nConstruct a plot that answers this question and calculate a measure of a typical observation (mean, median, or mode, as appropriate). Then use the plot and this summary to answer the original question in a sentence.\n\n\n\nIs there an association between students’ favorite season and terrain preference (beach or mountains)?\nConstruct a plot that answers this question. Use this plot to answer the question in one sentence.\n\n\n\nHow much coding experience do students have? (numerical scale)\nConstruct a plot that answers this question and calculate a measure of a typical observation (mean, median, or mode, as appropriate). Then use the plot and this summary to answer the original question in a sentence.\n\n\n\nWhat is the relationship between students’ optimism for cryptocurrency and their skepticism of the effect of technology on interpersonal relationships?\nConstruct a plot that answers this question. Use this plot to answer the question in one sentence.\n\nSix variables appear in the survey data frame that were derived from the original prof_label question: is_artist, is_humanist, is_nat_sci, is_soc_sci, is_comp_sci and is_entrepreneur. The is_artist variable is TRUE for those students who most identified as an artist and FALSE otherwise. The other five variables are defined similarly.\nFor the following four questions, please answer with a plot and a sentence or two written response given the structure in the plot.\n\n\n\nIs there an relationship between students most identifying as an entrepreneur and their optimism for cryptocurrency?\n\n\n\nIs there an relationship between students most identifying as a humanist and their optimism for cryptocurrency?\n\n\n\nPropose your own question involving one of the is_ variables and numerical variable of your choice. Construct a plot that addresses the question, calculate a measure of center for the two groups separately (e.g. comparing those who identify as artists with those who do not), then use them to answer your questions in one or two sentences.\n\n\n\nThis last one is a full choose-your-own adventure: propose your own question involving two or three variables and answer it using a plot with a written interpretation.\n\n\n\nWill you ensure that your submission to Gradescope…\n\nis of a pdf generated from a qmd file,\nhas all of your code visible to readers,\nand assigns each of the questions to all pages that show your work for that question?\n\n(This one is easy! Just answer “yes” or “no”)"
  },
  {
    "objectID": "2-summarizing-data/labs/class-survey/part-2.html#part-ii-computing-on-the-data",
    "href": "2-summarizing-data/labs/class-survey/part-2.html#part-ii-computing-on-the-data",
    "title": "Class Survey",
    "section": "",
    "text": "You can access the real data from the Stat 20 Class Survey by adding the following line of code to a code cell at the top of your Quarto document.\n\nlibrary(tidyverse)\nclass_survey &lt;- read_csv(\"https://tinyurl.com/stat20-survey\")\n\nTo complete the following questions, you will need to find the columns in the class_survey data frame associated with each variable mentioned.\n\n\nDo students prefer to spend time at the beach or in the mountains?\nConstruct a plot that answers this question and calculate a measure of a typical observation (mean, median, or mode, as appropriate). Then use the plot and this summary to answer the original question in a sentence.\n\n\n\nIs there an association between students’ favorite season and terrain preference (beach or mountains)?\nConstruct a plot that answers this question. Use this plot to answer the question in one sentence.\n\n\n\nHow much coding experience do students have? (numerical scale)\nConstruct a plot that answers this question and calculate a measure of a typical observation (mean, median, or mode, as appropriate). Then use the plot and this summary to answer the original question in a sentence.\n\n\n\nWhat is the relationship between students’ optimism for cryptocurrency and their skepticism of the effect of technology on interpersonal relationships?\nConstruct a plot that answers this question. Use this plot to answer the question in one sentence.\n\nSix variables appear in the survey data frame that were derived from the original prof_label question: is_artist, is_humanist, is_nat_sci, is_soc_sci, is_comp_sci and is_entrepreneur. The is_artist variable is TRUE for those students who most identified as an artist and FALSE otherwise. The other five variables are defined similarly.\nFor the following four questions, please answer with a plot and a sentence or two written response given the structure in the plot.\n\n\n\nIs there an relationship between students most identifying as an entrepreneur and their optimism for cryptocurrency?\n\n\n\nIs there an relationship between students most identifying as a humanist and their optimism for cryptocurrency?\n\n\n\nPropose your own question involving one of the is_ variables and numerical variable of your choice. Construct a plot that addresses the question, calculate a measure of center for the two groups separately (e.g. comparing those who identify as artists with those who do not), then use them to answer your questions in one or two sentences.\n\n\n\nThis last one is a full choose-your-own adventure: propose your own question involving two or three variables and answer it using a plot with a written interpretation.\n\n\n\nWill you ensure that your submission to Gradescope…\n\nis of a pdf generated from a qmd file,\nhas all of your code visible to readers,\nand assigns each of the questions to all pages that show your work for that question?\n\n(This one is easy! Just answer “yes” or “no”)"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/tutorial.html",
    "href": "2-summarizing-data/01-summarizing-categorical-data/tutorial.html",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "In this tutorial we’ll grow your R toolbox to visualize categorical data but first, a very general skill: how to ask for help.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "Summarizing Categorical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/tutorial.html#libraries",
    "href": "2-summarizing-data/01-summarizing-categorical-data/tutorial.html#libraries",
    "title": "Summarizing Categorical Data",
    "section": "Libraries",
    "text": "Libraries\n\n\n\nThe tidyverse library, which contains many useful packages for data science\n\n\nIn Taxonomy of Data, we learned about a few different functions that can be used on vectors, such as mean(). We also learned about a function called data.frame(), which allowed us to bring vectors together as part of a new structure called a data frame, with each of the vectors used as columns.\nWhile the base functionality provided by R is powerful, developers often seek to find more efficient ways to complete tasks. In doing so, they push the power of R forward. R has a vast ecosystem of libraries that add new functions. Any installed library can be loaded with the library() function. Here, we will load the tidyverse library, one of the core external libraries that we will be using this semester.\n\nlibrary(tidyverse)\n\nTo use these functions within this library (or any other library), you will need to run the above line of code each time you start an RStudio session!\nThe tidyverse library has many different packages which contain smaller pieces of functionality. Today, our focus is to summarize categorical data, and one avenue we have explored already is a visual summary; a bar chart. Let’s explore how the plots shown earlier in this set of notes were made. We will use ggplot2, tidyverse’s resident visualization package.\nFirst, we should load the penguins data into our environment. The penguins data is actually located in a special library made just for this course called stat20data. This library hosts the datasets which will be the subjects of your labs. Therefore, we need to load this library first.\n\nlibrary(stat20data)\n\nThen, we can load in the penguins data by using the aptly named data function.\n\ndata(penguins)\n\nOnce you do this, you will see the penguins dataset, in all of its glory, appear in the environment pane at the top right of your RStudio session (you may need to click in the area once or twice).\n\n\n\n\n\nYou can click the blue dropdown arrow to see each variable in the dataset, or for a more traditional view, you can click the white spreadsheet icon to the right:\n\n\n\n\n\nNow, we’re ready to write some ggplot2 code and make our first visualization of the year! We will create the exact stacked, normalized bar chart you saw earlier in the notes.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "Summarizing Categorical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/tutorial.html#a-first-visualization-with-ggplot2",
    "href": "2-summarizing-data/01-summarizing-categorical-data/tutorial.html#a-first-visualization-with-ggplot2",
    "title": "Summarizing Categorical Data",
    "section": "A first visualization with ggplot2",
    "text": "A first visualization with ggplot2\nThe main function within the ggplot2 package is, well, ggplot().\n\nggplot(data = penguins)\n\n\n\n\n\n\n\n\nThe ggplot() function takes as its first argument a data frame (the data argument). By itself, there’s not much to look at; it just creates a blank canvas.\nLet’s now indicate how we want to map our variables to each area of the plot. For now, let’s just focus on the species of penguin; we’ll handle the island later.\nWe had species of penguin on the horizontal (x) axis. This piece of information (which is known as an aesthetic attribute of the plot), goes into a second argument called mapping and within a function called aes().\n\nggplot(data = penguins, \n       mapping = aes(x = species))\n\n\n\n\n\n\n\n\nNow we’re getting somewhere. We can see the x axis has been set up with labels for the species of penguins.\nAll that is left is to actually put the observations on the plot. We do this by declaring what geometry we want and adding this information to our existing code. We can add this information via a new layer, which can be added on by using the + syntax and taking a newline.\n\nggplot(data = penguins, \n       mapping = aes(x = species)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nThe layer we added was a geometry layer corresponding to the bar chart, called geom_bar().\nNote that the bars are not colored like before, but are just gray. This is because we are missing the island variable! Island was represented by coloring in (filling) the bars for us according to the conditional proportions of penguins within a specific species belonging to a specific island. We can include this by adding to our aes() function in the original layer.\n\nggplot(data = penguins, \n       mapping = aes(x = species, \n                     fill = island)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nAlmost there! Note that what we have right now is just a stacked bar chart. There’s one more fix that we need to apply. Note that geom_bar() can be thought of as a function as well. One of its arguments, position, will help us solve this issue1. position defaults to a stacked bar chart, but we can fix this by supplying it with \"fill\"!\n\nggplot(data = penguins, \n       mapping = aes(x = species, \n                     fill = island)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nA side-by-side (dodged) bar chart can be produced by replacing \"fill\" with \"dodge\". To recover the original, stacked bar chart, use \"stack\"!\nThere we have it!",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "Summarizing Categorical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/tutorial.html#footnotes",
    "href": "2-summarizing-data/01-summarizing-categorical-data/tutorial.html#footnotes",
    "title": "Summarizing Categorical Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis argument is generally not important for the other plots we create in this course, which is why in the initial piece of code featuring geom_bar(), it was not specified at all.↩︎",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "Summarizing Categorical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/tutorial.html",
    "href": "2-summarizing-data/05-summarizing-associations/tutorial.html",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "cor()\nThe cor() function will calculate the Pearson correlation coefficient between two vectors. When working with a data frame, it is used in combination with summarize().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that in comparison to other numerical summaries that we have calculated with summarize(), the correlation is function of two variables. Also, as always, keep an eye out for missing values.\n\n\nlm()\nThe workhorse of the linear modeling in R is the lm() function. Generically, this function takes the form:\nlm(y ~ x, data = my_df)\nThis can be read as, “I’d like a linear model that explains y as a function of x, both coming from the my_df data frame.” Correspondingly, be sure that y and x are columns found in my_df.\nUnlike many of the functions we’ve been learning lately, you will generally call lm() on it’s own line and not as part of a data pipeline. To fit a model that explains bill_length_mm as a function of bill_depth_mm and save that model to a new object called m1, you would use:\n\n\n\n\n\n\n\n\nBecause you’ve saved the linear model as m1, when you run this line of code, it won’t return any output. However, now you can refer to the model later on in your code. To see the values of the estimated coefficients (the slope and the intercept), you can just enter the name of that model.\n\n\n\n\n\n\n\n\n\n\nfitted()\nThe fitted values, \\(\\hat{y}_i\\), are the \\(y\\) values where the linear model passes through the x coordinates of the observations (\\(x_i\\)). They give a sense of the y value that your linear model expects for that observation, given it’s x value. You can extract those fitted values from your linear model by simply calling the fitted() function on your model.\n\n\n\n\n\n\n\n\nNote that when you run this line of code, it gets messy! It returns a vector that is the same length as the number of (non-missing) observations; if you have \\(n\\) observations, you will have \\(n\\) fitted values.\nIt is often helpful to store these statistics that emerge from your linear model in your data frame right alongside the variables used to fit the model. We can do so using the mutate() function.\n\n\n\n\n\n\n\n\nYou can check how your fitted values compare to your original data by isolating those columns with select() and taking a peek at the first few rows.\n\n\n\n\n\n\n\n\nSure enough, your fitted values, \\(\\hat{y}_i\\), look fairly similar to the original \\(y_i\\) (bill_length_mm).\n\n\nresid()\nNow that you have your fitted values in your data frame, you could calculate residuals yourself by creating a new column that is the difference between \\(y_i\\) and \\(\\hat{y}_i\\). However, you can also get there by using the resid() function. This works similarly to fitted() - it operates on your linear model object and returns a vector of residuals - so lets jump straight to putting the residuals back into our data frame.\n\n\n\n\n\n\n\n\nLet’s see how that worked.\n\n\n\n\n\n\n\n\nIt worked just as we expected: in the first row, the value of e_hat is the observed y value, bill_length_mm, minus the expected/fitted y value, y_hat.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "Summarizing Numerical Associations"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course Culture\nStudents taking Stat 20 come from a wide range of backgrounds. We hope to foster an inclusive and supportive learning environment based on curiosity rather than competition. All members of the course community—the instructor, students, tutors, and readers—are expected to treat each other with courtesy and respect.\nYou will be interacting with course staff and fellow students in several different environments: in class, over the discussion forum, and in office hours. Some of these will be in person, some of them will be online, but the same expectations hold: be kind, be respectful, be professional.\nIf you are concerned about classroom environment issues created by other students or course staff, please come talk to us about it.\n\n\nMode of Instruction\nThis course is structured as a flipped class, meaning that you will first be encountering new concepts in statistics and data science outside of class. Class time is dedicated to expanding on the work you’ve done outside of class by working through questions solo, in groups, and as a class.\n\nBefore class\nIt is your responsibility to become familiar with the topics that appear in the course notes and to work through the reading questions on Gradescope by 11:59 pm on the day before class.\n\n\nDuring class on Tue/Wed and Thu/Fri\nClass time (2 hrs) will be spent on a range of activities, but the most common will be concept questions (using Poll Everywhere) and working through components of your Worksheets and Labs. Therefore, the most efficient way to complete your assignments is to be an active participant in class. Attendance is expected on these class days.\n\n\nDuring class on Monday\nMonday is shorter (1 hr) class that alternates between:\n\nWorkshops: less-structured class dedicated to finishing up work on your assignments\nQuizzes: see information on quizzes below.\n\n\n\n\nGroup tutoring\nTutors will offer group tutoring sessions several times each week. This is an opportunity to finish up any assignments that you’ve started in class or review any topics that are confusing for you. Each group tutoring session will be staffed by 2-4 tutors. You’re welcome to attend any session that works well for your schedule. Check the Office Hours page to see the group tutoring schedule.\nGroup tutoring is a great place to go to meet other students and collaborate on assignments with tutors on hand to help you get unstuck.\n\n\nInstructor Office Hours\nThe instructors will offer office hours each week across a range of times. We ask that you only visit the office hours of your instructor, but you are welcome to visit the tutoring sessions of any tutors, not just the ones who work in your section. We may adjust the office hour and group tutoring sessions schedule throughout the semester as we understand student needs and preferences. Please check the Office Hours page to see the times of the various office hour/group tutoring sessions.\nOffice hours are an opportunity to chat one-on-one with your instructor. Please come to office hours! Coming to office hours does not send a signal that you are behind or need extra help. On the contrary, coming to office hours early and often tends to co-occur with success in the course. Instructors are happy to chat about the course material, statistics in general, careers in statistics, and whatever other statistics or data science topics are on your mind!\n\n\nDon’t come to class if you’re sick!\nMaintaining your health and that of the Berkeley community is of primary importance to course staff, so if you are feeling ill or have been exposed to illness, whether it’s COVID-19 or something else, please do not come to class. All of the materials used in class will be posted to the course website. You’re encouraged to reach out to fellow students to discuss the class materials or stop by group tutoring or office hours to chat with a tutor or the instructor.\n\n\n\nMaterials\nThe primary materials for the course are the lecture notes, which will be posted to the course website in advance of class. We’ll teach you everything you need to know!\nThe following textbooks are useful supplementary texts but there is no need to purchase them:\n\nIntroduction to Modern Statistics by Çetinkaya-Rundel and Hardin (freely available online)\nStatistics, Freedman, Pisani, and Purves\nData Visualization: A Practical Introduction, Healy (freely available online)\nHands-on Programming with R, Grolemund (freely available online)\nR for Data Science, Wickham and Grolemund (freely available online)\n\n\nRStudio\nThe software that we’ll be using for our data analysis is the free and open-source language called R that we’ll be interacting with via software called RStudio. As a Berkeley student, you have your own version of RStudio waiting you for at: http://stat20.datahub.berkeley.edu. Most students taking Stat 20 have no experience programming; we’ll teach you everything you need to know!\n\n\n\nCourse communication\n\nbCourses\nWe don’t use bCourses for much. It is primarily used to disseminate announcements for the entire class, such as final exam information.\n\n\nDiscussion forum\nThe official discussion forum for the class will be hosted on Ed. Ed is a forum to ask and answer questions with your fellow students and course staff. It’s an indispensable resource for learning from your peers and seeking help from tutors and instructors.\nIf you have a question for staff, create a new post and mark it as “private” and it will go only to course staff. This is the best option to contact us if you have a personal concern. If your question does not include personal information and can be answered by other students, make sure it is public.\nIn a course this large, the instructors have a difficult time responding to individual emails, so please use the class forum or visit office hours if you wish to contact us!\n\n\nCourse website\nAll of the assignments will be posted here to the course website. This also holds the course notes, slides, and links to Gradescope, Ed, and RStudio.\n\n\n\nAssignments, Exams, and Grading\n\nTurning-in assignments\nYou will be turning in your assignments on a platform called Gradescope. This is also the platform where your assignments will be graded, so you can return there to get feedback on your work. You are welcome to file a regrade request if you notice that we made an error in applying the rubric to your work.\n\n\nLabs\nLabs are long-form assignments designed to apply the concepts from the lecture notes in the cause of doing an analysis of real data. This will involve both writing code and communicating your thoughts and findings in English. We’ll be working through some problems from the labs in class, but you may have to complete them on your own outside of class time. Some labs will be turned in individually; some will be turned in as groups.\nLabs are to be submitted as PDF files. These PDFs will be generated by rendering Quarto Documents (.qmd files) to HTML and then exporting the HTML into a PDF. Don’t worry if you’re not familiar with the Quarto Document as we will teach you about it!\nWe will be assessing most questions on labs for correctness but for others we will be giving credit based on completion.\n\n\nWorksheets\nDuring class, we will give you a second engagement with the day’s material in the form of a worksheet. These worksheets will run like traditional homework problems and drill the concepts in the reading notes rather than asking you to apply the concepts with a data set, like the lab does. These are graded credit / no credit, where full credit is given if you earnestly engage with the assignments.\n\n\nReading Questions\nReading questions serve to check your understanding and engagement while going through the lecture notes prior to class. There will be a handful of questions per lecture note. These questions be a mix of multiple choice, short answer, and coding questions. You can answer them directly on Gradescope. The reading questions will be due 11:59 pm on the day before class.\n\n\n\nQuizzes\nQuizzes reinforce the most important concepts from the lecture notes and provide you the opportunity to work through misunderstandings of concepts with peers and the instructor.\nThere is both an individual and group component to the quiz.\nThe individual component will last ~25 minutes. You are allowed one, A4, one-sided handwritten sheet of notes with your name in the upper right-hand corner. The group component will take place immediately after the individual component has been completed and will last ~15 minutes. Your final (composite) quiz grade will be the average of your group and individual quiz scores.\n\n\nExam\nThe final exam will be held in person during finals week. The time and date is Wednesday December 18th, 7 - 10 pm.\n\n\nGrading\nYour final grade in the course will be computed as follows:\n\nLabs 35%\nReading Questions: 3%\nQuizzes 25%\nWorksheets 7%\nFinal 30%\n\nThe grades will not be curved (&gt; 90% is some kind of A, etc.).\nIn order to provide flexibility around emergencies that might arise for you throughout the semester we will drop everyone’s lowest quiz grade before calculating your quiz average at the end of the semester. We will also drop your two lowest Reading Question scores; there are so many of those that it is understandable if you miss one or two.\n\n\n\nPolicies\n\nAccomodations for students with disabilities\nStat 20 is a course that is designed to allow all students to succeed. If you have letters of accommodations from the Disabled Students’ Program, please share them with your instructor as soon as possible, and we will work out the necessary arrangements.\n\n\nLate Work\nIf you don’t get your Labs or Worksheets in by the deadline, don’t fret. Fill out the extension request form to request an extension of 1-3 days. We will be approving all requests for extensions that are three days or fewer past the assignment deadline. For longer extensions, such as serious illness, speak to your instructor.\n\n\n\nCollaboration policy\nYou are encouraged to collaborate with your fellow students on worksheets and labs, but the work you turn in should reflect your own understanding and all of your collaborators must be cited. The individual component of quizzes, reading questions, and exams must reflect only your work.\nResearchers don’t use one another’s research without permission; scholars and students always use proper citations in papers; professors may not circulate or publish student papers without the writer’s permission; and students may not circulate or post non-public materials (quizzes, exams, rubrics-any private class materials) from their class without the written permission of the instructor.\nThe general rule: you must not submit assignments that reflect the work of others unless they are a cited collaborator.\nThe following examples of collaboration are allowed and in fact encouraged!\n\nDiscussing how to solve a problem with a classmate.\nShowing your code to a classmate along with an error message or confusing output.\nPosting snippets of your code to the discussion forum when seeking help.\nHelping other students solve questions on the discussion with conceptual pointers or snippets of code that doesn’t whole hog give away the answer.\nGoogling the text of an error message.\nCopying small snippets of code from answers on Stack Overflow.\n\nThe following examples are not allowed:\n\nLeaving a representation of your assignment (the text, a screenshot) where students (current and future) can access it. Examples of this include websites like course hero, on a group text chain, over discord/slack, or in a file passed on to future students.\nAccessing and submitting solutions to assignments from other students distributed as above. This includes copying written answers from other students and slightly modifying the language to differentiate it.\nGoogling for complete problem solutions.\nWorking on the reading questions or individual quizzes in collaboration with other people or resources. These assignments must reflect individual work.\nSubmitting work on an quiz or final that reflects consultation with outside resources or other people. These assessments must reflect individual work.\n\nIf you have questions about the boundaries of the policy, please ask. We’re always happy to clarify.\n\nViolations of the collaboration policy\nThe integrity of our course depends on our ability to ensure that students do not violate the collaboration policy. We take this responsibility seriously and forward cases of academic misconduct to the Center for Student Conduct.\nStudents determined to have violated the academic misconduct policy by the Center for Student Conduct will receive a grade penalty in the course and a sanction from the university which is generally: (i) First violation: Non-Reportable Warning and educational intervention, (ii) Second violation: Suspension/Disciplinary Probation and educational interventions, (iii) Third violation: Dismissal.\nAnd again, if you have questions about the boundaries of the collaboration policy, please ask!\n\n\n\nFrequently Asked Questions\n\nWhat should I do if I’m on the waitlist?\nAttend both lecture and section (remember, we are teaching it as one two hour class), and submit all assignments on time. Visit your instructor on the first day of class so you can be added to the course Ed and Gradescope.\nAre class sessions recorded?\nNo. Class sessions feature a mix of group problem solving, activities, and discussion and don’t lend themselves to recording. The course notes are the main reference source for the course. Any materials used during the class session will be posted to the course website.\nIs attendance required?\nNo, but it is difficult to succeed in this course if you are not regularly attending class. Class sessions are designed to be an effective and efficient format to make progress on important assignments. Plus, it’s a great way to meet your fellow students and learn from one another. This class is very unique in that regard.\nIf you can’t attend due to a religious observance, athletic competition, or something similarly important, don’t worry. Just reach out to us via a private Ed post, and we can let you know what to keep tabs on during the time you’re away.\nAre time conflicts allowed?\n\nStat 20 does not allow students to enroll with time conflicts.\n\nWhat if I join the class late?\nIf you join the class within the first two weeks, read the syllabus and lecture notes, take a look at Gradescope to get a sense of any assignments that may have already passed, and visit office hours to check that you’re up to date with things. The first two weeks of material are very important so you must be able to make up assignments.\nAfter two weeks into the semester, you’ll have too much material that you’ll need to make up, so you will have to wait to a subsequent semester to take Stat 20.\n\n\n\nCampus Resources\nIf you ever need someone to talk to about anything that you’re going through, please feel to reach out to the instructors. For some topics, the tutors might be an even better resource because they are students just like you. Tutors can also tell you what being an Academic Student Employee (ASE) is like.\nWith regards to reports of sexual misconduct/violence/assault, you may speak with us as well, but know that we will need to report our discussion to the Title IX officer. This is detailed below.\n\nAs UC employees, the instructors (and tutors) are “Responsible Employees” and are therefore required to report incidents of sexual violence, sexual harassment, or other conduct prohibited by University policy to the Title IX officer. We cannot keep reports of sexual harassment or sexual violence confidential, but the Title IX officer will consider requests for confidentiality. Note that there are confidential resources available to you through UCB’s PATH to Care Center, which serves survivors of sexual violence and sexual harassment; call their 24/7 Care Line at 510-643-2005.\n\nBelow are some campus resources that may be helpful for you:\n\nBerkeley Supportal- great place to start\nDisabled Students’ Program\nUniversity Health Services\nUCB Path to Care\nStudent Learning Center\nBerkeley International Office\nOmbuds Office for Students and Postdoctoral Appointees\nGender Equity Resource Center\nCenter for Educational Justice & Community Engagement\nUHS Counseling and Psychological Services (CAPS)\nCampus Academic Accommodations Hub\nASUC Student Advocate’s Office\nBasic Needs Center\nASUC Mental Health Resources Guide"
  },
  {
    "objectID": "glossary-fns.html",
    "href": "glossary-fns.html",
    "title": "Functions",
    "section": "",
    "text": "Questions and Data"
  },
  {
    "objectID": "office-hours.html",
    "href": "office-hours.html",
    "title": "Office Hours and Group Tutoring",
    "section": "",
    "text": "Office Hours and Group Tutoring\nPlease stop by introduce yourself! Stat 20 offers two types of session to ask chat about the course in person but outside of class.\nInstructor office hours are useful for to discuss topics that come in the reading or broader questions about statistics and data science. They are also a fine place to ask questions about the assignments you’re working on. Please attend only your instructor’s office hours.\nGroup tutoring (GT) sessions are held throughout the week on the 3rd floor of Evans Hall. They’re a great place to work on your assignments with fellow students and seek the help of tutors whenever you get stuck. One thing to know: tutors will expect that you have done the reading and generally been attending class; this is not meant to replace class time.\nYou’re welcome to attend whichever GT sessions work best for your schedule."
  },
  {
    "objectID": "3-generalization/01-prob-foundations/tutorial.html",
    "href": "3-generalization/01-prob-foundations/tutorial.html",
    "title": "Probability Foundations",
    "section": "",
    "text": "Our first step toward simulating experiments is introducing randomness in R. The following three functions are a good start.\n\nsample()\nDrawing from a box of tickets is easily simulated in R, since there is a convenient function sample() that does exactly what we need: draw tickets from a “box” (which needs to be a vector).\n\nArguments\n\nx: the vector to be sampled from, this must be specified\nsize: the number of items to be sampled, the default value is the length of x\nreplace: whether we replace a drawn item before we draw again, the default value is FALSE, indicating that we would draw without replacement.\n\n\nExample: one sample of size 2 from a box with tickets from 1 to 6\n\n\n\n\n\n\n\n\nWhat would happen if we don’t specify values for size and replace?\n\n\n\n\n\n\n\n\nWhat would we do differently if we wanted to simulate two rolls of a die?\n\n\nCheck your answer\n\nWe would sample twice from the vector die with replacement:\n\n\n\n\n\n\n\n\n\n\nset.seed()\nThe random number generator in R is called a “Pseudo Random Number Generator”, because the process can be controlled by a “seed number”. These are algorithmic random number generators, which means that if you provide the same seed (a starting number), R will generate the same sequence of random numbers. This makes it easier to debug your code, and reproduce your results if needed.\n\nArguments\n\nn: the seed number to use. You can use any number you like, for example 1, or 31415 etc You might have noticed that each time you run sample in the code chunk above, it gives you a different sample. Sometimes we want it to give the same sample so that we can check how the code is working without the sample changing each time. We will use the set.seed function for this, which ensures that we will get the same random sample each time we run the code.\n\n\nExample: one sample of size 2 from a box with tickets from 1 to 6\n\n\n\n\n\n\n\n\nExample: another sample of size 2 from a box with tickets from 1 to 6\n\n\n\n\n\n\n\n\nNotice that we get the same sample. You can try to run sample(die) without using set.seed() and see what happens.\nThough we used set.seed() twice here to demonstrate its purpose, generally, you will only need to run set.seed() once time per document. This is a line of code that fits perfectly at the beginning of your work, when you are also loading libraries and packages.\n\n\nseq()\nAbove, we created the vector die using die &lt;- c(1, 2, 3, 4, 5, 6), which is fine, but this method would be tedious if we wanted to simulate a 20-sided die, for instance. The function seq() allows us to create any sequence we like by specifying the starting number, how we want to increment the numbers, and either the ending number or the length of the sequence we want.\n\nArguments\n\nfrom: where to start\nby: size of jump from number to number (the increment)\n\n\nYou can end a sequence in one of two ways: - to: at what number should the sequence end - length: how long should the sequence be\nExample: sequence with the to argument\n\n\n\n\n\n\n\n\nExample: sequence with the length argument",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "Probability Foundations"
    ]
  },
  {
    "objectID": "3-generalization/03-probability-dsns/notes.html",
    "href": "3-generalization/03-probability-dsns/notes.html",
    "title": "Probability Distributions",
    "section": "",
    "text": "So far we have seen examples of outcome spaces, and descriptions of how we might compute probabilities, along with tabular representations of the probabilities. In this set of notes, we are going to talk about how to visualize probabilities using tables and histograms, as well as how to visualize simulations of outcomes from actions such as tossing coins or rolling dice.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "3-generalization/03-probability-dsns/notes.html#probability-distributions-and-histograms",
    "href": "3-generalization/03-probability-dsns/notes.html#probability-distributions-and-histograms",
    "title": "Probability Distributions",
    "section": "Probability distributions and histograms",
    "text": "Probability distributions and histograms\n\nProbability distributions\nRecall the example in which we drew a ticket from a box with 5 tickets in it:\n\n\n\n\n\nIf we draw one ticket at random from this box, we know that the probabilities of the four distinct outcomes can be listed in a table as:\n\n\n\n\n\n\n\n\n\n\nOutcome\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\\(4\\)\n\n\n\n\nProbability\n\\(\\displaystyle \\frac{1}{5}\\)\n\\(\\displaystyle \\frac{2}{5}\\)\n\\(\\displaystyle \\frac{1}{5}\\)\n\\(\\displaystyle \\frac{1}{5}\\)\n\n\n\nWhat we have described in the table above is a probability distribution. We have shown how the total probability of one or 100% is distributed among all the possible outcomes. Since the ticket \\(\\fbox{2}\\) is twice as likely as any of the other outcomes, it gets twice as much of the probability.\n\n\nProbability histograms\nA table is nice, but a visual representation would be even better.\n\n\n\n\n\n\n\n\n\n\nWe have represented the distribution in the form of a histogram, with the areas of the bars representing probabilities. Notice that this histogram is different from the ones we have seen before, since we didn’t collect any data. We just defined the probabilities based on the outcomes, and then drew bars with the heights being the probabilities. This type of theoretical histogram is called a probability histogram.\n\n\nEmpirical histograms\nWhat about if we don’t know the probability distribution of the outcomes of an experiment? For example, what if we didn’t know how to compute the probability distribution above? What could we do to get an idea of what the probabilities might be? Well, we could keep drawing tickets over and over again from the box, with replacement (that is, we put the selected tickets back before choosing again), keep track of the tickets we draw, and make a histogram of our results. This kind of histogram, which is the kind we have seen before, is a visual representation of data, and is called an empirical histogram.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the x-axis of this histogram, we have the ticket values; on the y-axis, we have the proportion of times that this ticket was selected out of the 50 with-replacement draws we took.We can see that the sample proportions looks similar to the values given by the probability distribution, but there are some differences. For example, we appear to have drawn more \\(3\\)s and less \\(4\\)s than what was to be expected. It turns out that the counts and proportions of the drawn tickets are:\n\n\n\n\n\nTicket\nNumber of times drawn\nProportion of times drawn\n\n\n\n\n\\(\\fbox{1}\\)\n10\n0.2\n\n\n\\(\\fbox{2}\\)\n24\n0.48\n\n\n\\(\\fbox{3}\\)\n10\n0.2\n\n\n\\(\\fbox{4}\\)\n6\n0.12\n\n\n\n\n\nWhat we have seen here is how when we draw at random, we get a sample that resembles the population, that is, a representative sample, but it isn’t exactly the true probabilities. If we increase our sample, however, say to 500, we will get something that more closely aligns with the truth.\n\n\nExamples\n\n\nRolling a pair of dice and summing the spots\nThe outcomes are already numbers, so we don’t need to represent them differently. We know that there are \\(36\\) total possible equally likely outcomes when we roll a pair of dice, but when we add the spots, we have only 11 possible outcomes, which are not equally likely (the chance of seeing a \\(2\\) is \\(1/36\\), but \\(P(6)=5/36\\)).\nThe probability histogram will have the possible outcomes listed on the x-axis, and bars of width \\(1\\) over each possible outcome. The height of these bars will be the probability, so that the areas of the bars represent the probability of the value under the bar. The height, which is the probability, is written on the top of each bar.\n\n\n\n\n\nWhat about the probability distribution? Make a table showing the probability distribution for rolling a pair of dice and summing the spots.\n\n\nCheck your answer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\n\\(2\\)\n\\(3\\)\n\\(4\\)\n\\(5\\)\n\\(6\\)\n\\(7\\)\n\\(8\\)\n\\(9\\)\n\\(10\\)\n\\(11\\)\n\\(12\\)\n\n\n\n\nProbability\n\\(\\displaystyle \\frac{1}{36}\\)\n\\(\\displaystyle \\frac{2}{36}\\)\n\\(\\displaystyle \\frac{3}{36}\\)\n\\(\\displaystyle \\frac{4}{36}\\)\n\\(\\displaystyle \\frac{5}{36}\\)\n\\(\\displaystyle \\frac{6}{36}\\)\n\\(\\displaystyle\\frac{5}{36}\\)\n\\(\\displaystyle \\frac{4}{36}\\)\n\\(\\displaystyle \\frac{3}{36}\\)\n\\(\\displaystyle \\frac{2}{36}\\)\n\\(\\displaystyle \\frac{1}{36}\\)\n\n\n\n\n\n\nTossing a fair coin 3 times and counting the number of heads\nWe have seen that there are 8 equally likely outcomes from tossing a fair coin three times: \\(\\{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\\}\\). If we count the number of \\(H\\) in each outcome, and write down the probability distribution of the number of heads, we get:\n\n\n\n\n\n\n\n\n\n\nOutcome\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\n\n\n\nProbability\n\\(\\displaystyle \\frac{1}{8}\\)\n\\(\\displaystyle \\frac{3}{8}\\)\n\\(\\displaystyle \\frac{3}{8}\\)\n\\(\\displaystyle \\frac{1}{8}\\)\n\n\n\nWhat would the probability histogram look like?\n\n\nCheck your answer\n\n\n\n\n\n\n\nWe are going to introduce some special distributions. We have seen most of these distributions, but will introduce some names and definitions. Before we do this, let’s recall how to count the number of outcomes for various experiments such as tossing coins or drawing tickets from a box (both with and without replacement).",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "3-generalization/03-probability-dsns/notes.html#basic-rule-of-counting",
    "href": "3-generalization/03-probability-dsns/notes.html#basic-rule-of-counting",
    "title": "Probability Distributions",
    "section": "Basic rule of counting",
    "text": "Basic rule of counting\nRecall that if we have multiple steps (say \\(n\\)) of some action, such that the \\(j^{\\text{th}}\\) step has \\(k_j\\) outcomes; then the total number of outcomes is \\(k_1 \\times k_2 \\times \\ldots k_n\\) and is obtained by multiplying the number of outcomes at each step. This principle is illustrated in the picture below: Geni the Gentoo penguin1 is trying to count how many outfits they have, if each outfit consists of a t-shirt and a pair of pants. The tree diagram below shows the number of possible outfits Geni can wear. In this example, \\(n=2\\), \\(k_1 = 3\\), and \\(k_2 = 2\\), since Geni has three t-shirts to choose from, and for each t-shirt, they have two pairs of pants, leading to a total of \\(3 \\times 2 = 6\\) outfits.\n\n\n\n\n\nThis example seems trivial, but it illustrates the basic principle of counting: we get the total number of possible outcomes of an action that has multiple steps, by multiplying together the number of outcomes for each step. All the counting that follows in our notes applies this rule. For example, let’s suppose we are drawing tickets from a box which has tickets marked with the letters \\(\\fbox{C}\\), \\(\\fbox{R}\\), \\(\\fbox{A}\\), \\(\\fbox{T}\\), \\(\\fbox{E}\\), and say we draw three letters with replacement (that means that we put each drawn ticket back, so the box is the same for each draw). How many possible sequences of three letters can we get? Using the counting rule,since we have \\(5\\) choices for the first letter, \\(5\\) for the second, and \\(5\\) for the third, we will have a total of \\(5\\times 5 \\times 5 = 125\\) possible words, allowing for repeated letters (that means that \\(CCC\\) is a possible outcome).\nHow many possible words are there if we draw without replacement? That is, we don’t put the drawn ticket back?\n\n\nCheck your answer\n\nWe have \\(5\\) choices for the first letter, \\(4\\) for the second, and \\(3\\) for the third, leading to \\(5 \\times 4 \\times 3 = 60\\) possible outcomes or words. Note that here we count the word \\(CRA\\) as different from the word \\(CAR\\). That is, the order in which we draw the letters matters.\nWe usually write the quantity \\(5 \\times 4 \\times 3\\) as \\(\\displaystyle \\frac{5!}{2!}\\) where \\(n! = n\\times(n-1)\\times(n-2)\\times \\ldots \\times 3 \\times 2 \\times 1\\)\n\n\nCounting the number of ways to select a subset\nWhat if, in this example of selecting \\(3\\) letters without replacement from \\(\\fbox{C}\\), \\(\\fbox{R}\\), \\(\\fbox{A}\\), \\(\\fbox{T}\\), \\(\\fbox{E}\\), the order does not matter - we don’t count the order in which the letters , just which letters were selected, that is, only the letters themselves matter. For example, the words \\(CRA,\\,CAR,\\,ARC,\\,ACR,\\,RAC,\\,RCA\\) all count as the same word, so we will count all \\(6\\) words as the same subset of letters. We have to take the number that we got from earlier and divide it by the number of words that can be made from \\(3\\) letters (number of rearrangements), which is \\(3 \\times 2\\times 1\\). This gives us the number of ways that we can choose \\(3\\) letters out of \\(5\\), which is \\[\n\\frac{\\left(5!/2!\\right)}{3!} = \\frac{5!}{2!\\; 3!},\n\\]\nand is called the number of combinations of \\(5\\) things taken \\(3\\) at a time.\nTo recap: when we draw \\(k\\) items from \\(n\\) items without replacement, we have two cases: either we care in what order we draw the \\(k\\) items and the different arrangements of the same set of \\(k\\) items have to be counted separately. The number of such arrangements is called the permutations of \\(n\\) things taken \\(k\\) at a time. In the example above, we have \\(5 \\times 4\\times 3 = 60\\) ways of choosing \\(3\\) things out of \\(5\\) when we count every sequence as different.\n\n\nPermutations\n\nThe number of possible arrangements or sequences of \\(n\\) things taken \\(k\\) at a time which is given by (the ordering matters): \\[\n\\frac{n!}{(n-k)!}\n\\]\n\n\n\n\n\nCombinations\n\nNumber of ways to choose a subset of \\(k\\) things out of \\(n\\) possible things which is given by \\[\n\\frac{n!}{k!\\; (n-k)!}\n\\] This number is just the number of distinct arrangements or permutations of \\(n\\) things taken \\(k\\) at a time divided by the number of arrangements of \\(k\\) things. It is denoted by \\(\\displaystyle \\binom{n}{k}\\), which is read as “n choose k”.\n\n\n\nExample How many ways can I deal \\(5\\) cards from a standard deck of 52 cards?\n\n\nCheck your answer\n\nWhen we deal cards, order does not matter, so this number is \\(\\displaystyle \\binom{52}{5} = \\frac{52!}{(52-5)!5!} = 2,598,960\\).",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "3-generalization/03-probability-dsns/notes.html#special-distributions",
    "href": "3-generalization/03-probability-dsns/notes.html#special-distributions",
    "title": "Probability Distributions",
    "section": "Special distributions",
    "text": "Special distributions\nThere are some important special distributions that every student of probability must know. Here are a few, and we will learn some more later in the course. We have already seen most of these distributions. All we are doing now is identifying their names. First, we need a vocabulary term:\n\n\nParameter of a probability distribution\n\nA constant(s) number associated with the distribution. If you know the parameters of a probability distribution, then you can compute the probabilities of all the possible outcomes.\n\n\n\nEach of the distributions we will cover below has a parameter(s) associated with it.\n\nDiscrete uniform distribution\nThis is the probability distribution over the numbers \\(1, 2, 3 \\ldots, n\\). We have seen it for dice above. This probability distribution is called the discrete uniform probability distribution, since each possible outcome has the same probability, that is, \\(1/n\\). We call \\(n\\) the parameter of the discrete uniform distribution.\n\n\nBernoulli distribution\nThis is a probability distribution describing the probabilities associated with binary outcomes that result from one action, such as one coin toss that can either land Heads or Tails. We can represent the action as drawing one ticket from a box with tickets marked \\(\\fbox{1}\\) or \\(\\fbox{0}\\), where the probability of \\(\\fbox{1}\\) is \\(p\\), and therefore, the probability of \\(\\fbox{0}\\) is \\((1-p)\\). We have already seen some examples of probability histograms for this distribution. We usually think of the possible outcomes of a Bernoulli distribution as success and failure, and represent a success by \\(\\fbox{1}\\) and a failure by \\(\\fbox{0}\\).\n\n\n\n\n\nFor the Bernoulli distribution, our parameter is \\(p = P\\left(\\fbox{1}\\right)\\). If we know \\(p\\), we also know the probability of drawing a ticket marked \\(\\fbox{0}\\).\nIn the figure above, the first histogram is for a Bernoulli distribution with parameter \\(p = 1/2\\), the second \\(p=3/4\\), and the third has \\(p = 2/3\\).\n\n\nBinomial Distribution\nThe binomial distribution, which describes the total number of successes in a sequence of \\(n\\) independent Bernoulli trials, is one of the most important probability distributions. For example, consider the outcomes from tossing a coin \\(n\\) times and counting the total number of heads across all \\(n\\) tosses, where the probability of heads on each toss is \\(p\\). Each toss is one Bernoulli trial, where a success would be the coin landing heads. The binomial distribution describes the probabilities of the total number of heads in three tosses. We saw what this distribution looks like in the case where \\(n = 3\\) for three tosses of a fair coin.\nWhat would the probability distribution and histogram for the number of heads in three tosses of a biased coin like, where \\(P(H) = 2/3\\)? Make sure you know how the probabilities are computed. For example, \\(P(HHH) = (2/3)^3 = 8/27\\).\n\n\nCheck your answer\n\n\n\n\n\n\nNote that the outcomes \\(HHT, HTH, THH\\) all have the same probability, as do the outcomes \\(TTH, THT, HTT\\), since the probability only depends on how many heads and how many tails we see in three tosses, not the order in which we see them. We get the probability of 2 heads in 3 tosses by adding the probabilities of all three outcomes \\(HHT, HTH, THH\\), since they are mutually exclusive. (In three tosses, we can see exactly one of the possible 8 sequences listed above.)\n\nMore generally, suppose that we have \\(n\\) independent trials, where each trial can either result in a “success” (like drawing a ticket marked \\(\\fbox{1}\\)) with probability \\(p\\); or a “failure” (like drawing \\(\\fbox{0}\\)) with probability \\(1-p\\). In the case of the Bernoulli distribution, \\(n = 1\\).\nThe multiplication rule for independent events tells us how to compute the probability of a sequence that consisted of the first \\(k\\) trials being successes and the rest of the \\(n-k\\) trials being failures. The probability of this particular sequence of \\(k\\) successes followed by \\(n-k\\) failures is (by multiplying their probabilities) given by: \\[\np^k \\times (1-p)^{n-k}\n\\] Now this is the probability of one particular sequence: \\(SSS\\ldots SSFF \\ldots FFF\\), but as we saw in the example above, only the number of successes and failures matter, not the particular order. So every sequence of \\(n\\) trials in which we have \\(k\\) successes and \\(n-k\\) failures has the same probability.\nHow many such sequences are there? We can count them using our rules above. We have \\(n\\) spots in the sequence, of which \\(k\\) have to be successes. The number of such sequences of length \\(n\\) consisting of \\(k\\) \\(S\\)’s and \\(n-k\\) \\(F\\)’s) is given by \\(\\displaystyle \\binom{n}{k}\\). Each such sequence has probability \\(\\displaystyle p^k \\times (1-p)^{n-k}\\). Adding up all these \\(\\displaystyle \\binom{n}{k}\\) probabilities (of each such sequence) gives us the formula for the probability of \\(k\\) successes in \\(n\\) trials: \\[\n\\binom{n}{k} \\times p^k \\times (1-p)^{n-k}\n\\]\nThe probability distribution described by the above formula is called the binomial distribution. It is named after \\(\\displaystyle \\binom{n}{k}\\), which is called the binomial coefficient. The binomial distribution has two parameters: the number of trials \\(n\\) and the probability of success on each trial, \\(p\\).\n\nExample\nToss a weighted coin, where \\(P(\\text{heads}) = .7\\) five times. What is the probability that you see exactly four heads across these five tosses?\n\n\nCheck your answer\n\nAcross five trials, we need to see four heads and one tails. Since each toss is independent, one possible way to obtain what we are looking for is \\(HHHHT\\). This is given by\n\\[\n(.7)^4 \\times (.3)^1\n\\]\nHowever, we need to consider all the possible orderings of tosses that involve four heads and one tail. This is given by the binomial coefficient \\(\\binom{5}{4}\\). Our final probability is therefore:\n\\[\n\\binom{5}{4} (.7)^4 \\times (.3)^1 \\approx 0.36\n\\]\n\n\n\n\nHypergeometric distribution\nIn the binomial scenario described above, we had \\(n\\) independent trials, where each trial resulted in a success or a failure. This is like sampling with replacement from a box of \\(0\\)’s and \\(1\\)’s. Now consider the situation when we have a box with \\(N\\) tickets marked with either \\(\\fbox{0}\\) or \\(\\fbox{1}\\).\nAs usual, the ticket marked \\(\\fbox{1}\\) represents a success. Say the box has \\(G\\) tickets marked \\(\\fbox{1}\\) (and therefore \\(N-G\\) tickets marked \\(\\fbox{0}\\) representing failures). Suppose we draw a simple random sample of size \\(n\\) from this box. A simple random sample is a sample drawn without replacement, and on each draw, every ticket is equally likely to be selected from among the remaining tickets. Then, the probability of drawing a ticket marked \\(\\fbox{1}\\) changes from draw to draw.\nWhat is the probability that we will have exactly \\(k\\) successes among these \\(n\\) draws? The probability distribution that gives us this answer is given by\n\\[\n\\frac{\\binom{G}{k}\\times \\binom{N-G}{n-k}}{\\binom{N}{n}}\n\\]\nand is called the hypergeometric distribution. It has three parameters, \\(n\\), \\(N\\) and \\(G\\). This is a wacky formula, so let’s explain it piece by piece, starting with the numerator and then moving to the denominator!\n\nNumerator\nWe count the number of samples drawn without replacement that have \\(k\\) tickets marked \\(\\fbox{1}\\). Since there are \\(G\\) tickets marked \\(\\fbox{1}\\) in the box, and \\(\\displaystyle \\binom{G}{k}\\) ways to choose exactly \\(k\\) of them. Similarly, there are \\(N-G\\) tickets marked \\(\\fbox{0}\\) in the box, and \\(\\displaystyle \\binom{N-G}{n-k}\\) ways to choose exactly \\(n-k\\) of them. The total number of ways to have \\(k\\) \\(\\fbox{1}\\)s and \\(n-k\\) \\(\\fbox{0}\\)s is therefore (by multiplication):\n\\[\n\\binom{G}{k}\\times \\binom{N-G}{n-k}\n\\]\n\n\nDenominator\nWe count the total number of simple random samples of size \\(n\\) that can be drawn from a pool of \\(N\\) observations. This is given by \\(\\displaystyle \\binom{N}{n}\\).\n\n\nExample\nSay we have a box of \\(10\\) tickets consisting of \\(4\\) tickets marked \\(\\fbox{0}\\) and \\(6\\) tickets marked \\(\\fbox{1}\\), and draw a simple random sample of size \\(3\\) from this box.\nWhat is the probability that two of the tickets drawn are marked \\(\\fbox{1}\\)?\n\n\nCheck your answer\n\nWe draw \\(3\\) tickets without replacement. We need two of these tickets to be marked \\(\\fbox{1}\\) and there are six in total to choose from; we need one of them to be marked \\(\\fbox{0}\\) and there are four in total to choose from. Therefore, there are\n\\[\n\\binom{6}{2}\\times \\binom{4}{1} = 60\n\\] different ways to pick three tickets in this manner.\nHow many total ways are there to draw \\(n=3\\) tickets from a box of \\(N=10\\)? This is given by \\(\\binom{10}{3} = 120\\).\nTherefore, our final answer is given by\n\\[\n\\frac{\\binom{6}{2}\\times \\binom{4}{1}}{\\binom{10}{3}} = \\frac{60}{120} =  \\frac{1}{2}\n\\]\n\n\n\n\nBinomial vs Hypergeometric distributions\nBoth these distributions deal with:\n\na fixed number of trials, or instances of the random experiment;\noutcomes that are deemed either successes or failures.\n\nThe difference is that for a binomial random variable, the probability of a success stays the same for each trial, and for a hypergeometric random variable, the probability changes with each trial.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "3-generalization/03-probability-dsns/notes.html#the-ideas-in-code",
    "href": "3-generalization/03-probability-dsns/notes.html#the-ideas-in-code",
    "title": "Probability Distributions",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nBefore discussing how to simulate the distributions, we are going to introduce three more useful functions.\n\nThree useful functions\n\n1. rep(): replicates values in a vector\nSometimes we need to create vectors with repeated values. In these cases, rep() is very useful.\n\nArguments\n\nx: the vector or list that is to be repeated. This must be specified\ntimes: the number of times we should repeat the elements of x. This could be a vector the same length as x detailing how many times each element is to be repeated, or it could be a single number, in which case the entire x is repeated that many times.\neach: the default is 1, and if specified, each element of x is repeated each times.\n\n\n\n\n\n2. replicate(): repeat a specific set of tasks a large number of times.\n\nArguments\n\nn: the number of times we want to repeat the task. This must be specified\nexpr: the task we want to repeat, usually an expression that is some combinations of functions, for example, maybe we take a sample from a vector, and then sum the sample values.\n\n\n\n\n\n\n3. geom_col(): plotting with probability\nWhen plotting probability histograms, we know exactly what the the height of each bar should be. This is as opposed to the bar charts you have seen before (and empirical histograms), where we are just trying to visualize the data that we have collected.\ngeom_col() creates a bar chart in which the heights represent numbers that can be specified via an aesthetic. In other words, the y variable will appear in our call to aes()!\n\n\n\n\nExample: Rolling a die twice and summing the spots\n\n\n\n\n\n\nCode along\n\n\n\nAs you read through the code in this section, keep RStudio open in another window to code along at the console. Keep in mind that we use set.seed() more than once for demonstration purposes only.\n\n\nSuppose we want to simulate the task of rolling a pair of die and summing the two spots. We can accomplish this task and examine our results using the functions we have just introduced. First, we will make a vector representing a fair, six-sided die.\n\nObtaining a sum\n\nMethod 1 - replicate()\nWe can use the sample() function to roll the die twice; this will output a vector with two die numbers. Then, we can take the sum of this vector by nesting the call to sample() inside of sum.\n\n\n[1] 7\n\n\nIf we would like to repeat this action many times (for instance, in a game of Monopoly, each player has to roll two dice on their turn and sum the spots), the replicate() function will come in handy. In the following line of code, we obtain 10 sums.\n\n\n [1] 11  8  8 12  5  7  8  7 10  7\n\n\n\n\nMethod 2 - rep()\nWe could also roll the die in advance and then sample from the possible sums: 2 through 12. However, when rolling the two die, there is only one way to get a sum of \\(2\\) (both dice need to be one), but six ways to get a sum of \\(7\\). This shows that if we want to represent this action of rolling a pair of dice and taking the sum of spots, we have to use a box in which values will be repeated to reflect their probability. We can use the times argument of the rep() function to make such a box. The number \\(2\\) is repeated once, the number \\(3\\) is repeated twice, and so on until the number \\(7\\) is repeated six times. We can then sample once from this box.\n\n\n [1]  2  3  3  4  4  4  5  5  5  5  6  6  6  6  6  7  7  7  7  7  7  8  8  8  8\n[26]  8  9  9  9  9 10 10 10 11 11 12\n\n\nTo get 10 sums as we did before, we just need to sample with ten times with replacement from this new box, correct_sums.\n\n\n [1] 7 4 8 9 2 7 4 7 6 8\n\n\n\n\n\n\nVisualizing our results\n\nMaking a probability histogram with geom_col()\nFirst, let’s create a vector with the probabilities associated with each possible that can be obtained from rolling two dice. We are taking these probabilities from the drawn probability histogram earlier in the notes.\nNow, using the above and the possible_sums vector from before, we can make a data frame with the information about the probability distribution and create a probability histogram, which in turn can be used to make a plot with geom_col().\n\n\n\n\n\n\n\n\n\nThe use of factor() is to make sure that for the purposes of the plot, that the sum values are treated categorically.\n\n\nPerforming a simulation and making an empirical histogram\nLet’s simulate rolling two die and and computing a sum fifty times Then, we can make a data frame out of our results and find the total amount of rolls, grouped by face. This can be done with the n() summary function– and if we divide by 50, we can get the sample proportions of each sum.\n\n\n# A tibble: 11 × 2\n   results props\n     &lt;dbl&gt; &lt;dbl&gt;\n 1       2  0.04\n 2       3  0.02\n 3       4  0.06\n 4       5  0.1 \n 5       6  0.08\n 6       7  0.24\n 7       8  0.1 \n 8       9  0.12\n 9      10  0.14\n10      11  0.08\n11      12  0.02\n\n\nNow, we can construct an empirical histogram using the empirical data frame.\n\n\n\n\n\n\n\n\n\n\n\nComparing our results to the truth\nYou may have wondered why we bothered to save the plot objects. The reason is that we can use a nifty library called patchwork which will help us to more easily visualize multiple plots at once by using mathematical and logical syntax. For instance, using + will put plots side by side.\n\n\n\n\n\n\n\n\n\nWith only 50 experiments run, we see that the empirical histogram doesn’t quite match. However, modify the above code by increasing the number of repetitions, and you will see the empirical histogram begin to resemble more closely true probability distribution. This is an example of long-run relative frequency.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "3-generalization/03-probability-dsns/notes.html#summary",
    "href": "3-generalization/03-probability-dsns/notes.html#summary",
    "title": "Probability Distributions",
    "section": "Summary",
    "text": "Summary\n\nDefined probability distributions\nStated the basic counting principle and introduced permutations and combinations\nDefined some famous named distributions (Bernoulli, discrete uniform, binomial, hypergeometric)\nVisualized probability distributions using probability histograms\nLooked at the relationship between empirical histograms and probability histograms.\nIntroduced functions rep(), replicate(), geom_col()\nSimulated random experiments such as die rolls and coin tosses to visualize the distributions.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "3-generalization/03-probability-dsns/notes.html#footnotes",
    "href": "3-generalization/03-probability-dsns/notes.html#footnotes",
    "title": "Probability Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPenguin taken from art by @allison_horst↩︎",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html",
    "href": "3-generalization/02-computing-probs/notes.html",
    "title": "Computing Probabilities",
    "section": "",
    "text": "Sally Clark after her successful appeal\n\n\nIn November 1999, Sally Clark, an English solicitor, was convicted of murdering her infant sons1. The first, Christopher, had been 11 weeks old when he died, in 1996, and the second, Harry, 8 weeks old, in January 1998, when he was found dead. Christopher was believed to have been a victim of “cot death”, called SIDS (Sudden Infant Death Syndrome) in the US. After her second baby, Harry, also died in his crib, Sally Clark was arrested for murder. The star witness for the prosecution was a well known pediatrician and professor, Sir Roy Meadow, who authored the infamous Meadow’s Law :“One sudden infant death is a tragedy, two is suspicious and three is murder until proved otherwise”2. Unfortunately it was easier to comprehend this “crude aphorism” than make the effort to understand the subtleties of conditional probability. The Royal Statistical Society protested the misuse of statistics in courts, but not early enough to prevent Sally Clark’s conviction. She was eventually acquitted and released, only to die at the age of 42 through alcohol poisoning3 The math presented by Meadow, in brief: Based on various studies, there is a probability of 1 in 8,543 of a baby dying of SIDS in a family such as the Clarks. As the Clarks suffered two deaths, Meadow multiplied 8,543 by 8,543 to arrive at 73 million. He told the jury that the chance or probability that the event of two “cot deaths” was 1 in 73 million. The defense did not employ a statistician to refute her claim, a choice that may have been disastrous for Sally Clark.\nWe will revisit this case at the end of these notes. In order to think about the probabilities involved, we need some more concepts. Let’s go back to one of the examples with die rolls that we have seen, the outcomes that puzzled the seventeenth century gambler, the Chevalier De Méré.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html#sally-clark-a-tragic-victim-of-statistical-illiteracy",
    "href": "3-generalization/02-computing-probs/notes.html#sally-clark-a-tragic-victim-of-statistical-illiteracy",
    "title": "Computing Probabilities",
    "section": "",
    "text": "Sally Clark after her successful appeal\n\n\nIn November 1999, Sally Clark, an English solicitor, was convicted of murdering her infant sons1. The first, Christopher, had been 11 weeks old when he died, in 1996, and the second, Harry, 8 weeks old, in January 1998, when he was found dead. Christopher was believed to have been a victim of “cot death”, called SIDS (Sudden Infant Death Syndrome) in the US. After her second baby, Harry, also died in his crib, Sally Clark was arrested for murder. The star witness for the prosecution was a well known pediatrician and professor, Sir Roy Meadow, who authored the infamous Meadow’s Law :“One sudden infant death is a tragedy, two is suspicious and three is murder until proved otherwise”2. Unfortunately it was easier to comprehend this “crude aphorism” than make the effort to understand the subtleties of conditional probability. The Royal Statistical Society protested the misuse of statistics in courts, but not early enough to prevent Sally Clark’s conviction. She was eventually acquitted and released, only to die at the age of 42 through alcohol poisoning3 The math presented by Meadow, in brief: Based on various studies, there is a probability of 1 in 8,543 of a baby dying of SIDS in a family such as the Clarks. As the Clarks suffered two deaths, Meadow multiplied 8,543 by 8,543 to arrive at 73 million. He told the jury that the chance or probability that the event of two “cot deaths” was 1 in 73 million. The defense did not employ a statistician to refute her claim, a choice that may have been disastrous for Sally Clark.\nWe will revisit this case at the end of these notes. In order to think about the probabilities involved, we need some more concepts. Let’s go back to one of the examples with die rolls that we have seen, the outcomes that puzzled the seventeenth century gambler, the Chevalier De Méré.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html#simulating-de-mérés-dice-games.",
    "href": "3-generalization/02-computing-probs/notes.html#simulating-de-mérés-dice-games.",
    "title": "Computing Probabilities",
    "section": "Simulating De Méré’s dice games.",
    "text": "Simulating De Méré’s dice games.\nRecall that De Méré wanted to bet on at least one six in four rolls of a fair die, and also at least one double six in twenty-four rolls of a pair of dice.\nIn earlier notes, you have seen the result of simulating these two games to estimate the probability of De Méré winning his bet. The simulation used the idea of thinking of the probability of an event as the long-run relative frequency, or the proportion of times we observe that particular event (or the outcomes in the event) if we repeat the action that can result in the event over and over again. Let’s do that again - that is, we estimate the probabilities using the long-run proportions. We will simulate playing the game over and over again and count the number of times we see at least one six in four rolls, and similarly for the second game. (De Méré did this by betting many times, and noticed that the number of times he won wasn’t matching the probability he had computed. Long-run relative frequency in real life!)\n\n\nNumber of simulations for game 1 with 4 rolls of a die = 1000\n\n\nThe chance of at least one six in 4 rolls of a die is about 0.514\n\n\nNumber of simulations for game 2 with 24 rolls of a pair of dice = 1000\n\n\nThe chance of at least one double six in 24 rolls of a pair of dice is about 0.487\n\n\nNow the question is how do we figure out this probability without simulations?\nWe know that if two events are mutually exclusive, we can compute the probability of at least one of the events occurring (\\(A\\cup B\\) aka \\(A \\text{ or } B\\)) using the addition rule \\(P(A \\cup B) = P(A) + P(B)\\). We cannot use the addition rule to compute the probabilities that we have simulated above, since rolling a six on the first roll and rolling a six on the second roll (for example) are not mutually exclusive. So how do we compute them? Read on…\n\nExample: Drawing red and blue tickets from a box\nConsider a box with four tickets in it, two colored red and two blue. Except for their color, they are identical: . Suppose we draw three times at random from this box, with replacement, that is, every time we draw a ticket from the box, we put it back before drawing the next ticket. List all the possible outcomes. What is the probability of seeing exactly 2 red cards among our draws?\n\n\nCheck your answer\n\nNote that since each of the cards is equally likely to be drawn, therefore all the sequences of three cards are equally likely. We can count the number of possible outcomes that contain exactly 2 red cards, and divide that number by the number of total possible outcomes to get the probability of drawing exactly 2 red cards:\n There are three outcomes that have exactly two cards, out of a total of 8 possible outcomes, so the probability of exactly two red cards in three draws at random with replacement is 3/8.\n\nNow suppose we repeat the procedure, but draw without replacement (we don’t put any tickets back). What is the probability of exactly 2 red cards in 3 draws?\n\n\nCheck your answer\n\n\n\n\n\n\nNotice that we have fewer possible outcomes (6 instead of 8, why?), though they are still equally likely. Again, there are 3 outcomes that have exactly 2 red cards, and so the probability of 2 red cards in three draws is now 3/6.\n\nWhat about the probabilities for the number of red cards in three draws? Write down all the possible values for the number of red cards in three draws from a box with 2 red cards and 2 blue cards, while drawing with replacement, and their corresponding probabilities. Repeat this exercise for the same quantity (number of red cards in three draws from a box with 2 red cards and 2 blue cards), when you draw the tickets without replacement:\n\n\nCheck your answer\n\n\n\n\n\n\n\n\n\n\n\nNumber of reds in 3 draws\nprobability, with replacement\nprobability, without replacement\n\n\n\n\n0 red tickets\n\\(\\displaystyle \\frac{1}{8}\\)\n\\(0\\)\n\n\n1 red ticket\n\\(\\displaystyle \\frac{3}{8}\\)\n\\(\\displaystyle \\frac{3}{6}\\)\n\n\n2 red tickets\n\\(\\displaystyle \\frac{3}{8}\\)\n\\(\\displaystyle \\frac{3}{6}\\)\n\n\n3 red tickets\n\\(\\displaystyle \\frac{1}{8}\\)\n\\(0\\)\n\n\n\n\n\nWhy are the numbers different? What is going on?\n\nBelow you see an illustration of what happens to the box when we draw without replacement, with the box at each stage being shown with one less ticket.\n\n\n\n\n\nWe see that the box reduces after each draw. After two draws, if the first 2 draws are red (as on the left most sequence) you can’t get another red ticket, whereas if you are drawing with replacement, you can keep on drawing red tickets. (Note that the outcomes in the bottom row are not equally likely, since on the left branch of the tree, blue is twice as likely as red to be the second card, so the outcome RB is twice as likely as RR, and the outcome BR on the right branch of the tree is twice as likely as BB.) Before going further, let’s recall what we know about the probabilities of events.\n\n\nRules of probability (recap)\n\n\\(\\Omega\\) is the set of all possible outcomes.\n\n\nWhat is the probability of \\(\\Omega\\)?\n\nThe probability of \\(\\Omega\\) is 1. It is called the certain event.\n\nWhen an event has no outcomes in it, it is called the impossible event, and denoted by \\(\\emptyset\\) or \\(\\{\\}\\).\n\n\nWhat is the probability of the impossible event?\n\nThe probability of the impossible event is 0.\n\nLet \\(A\\) be a collection of outcomes (for example, from the example above, \\(A\\) could be the event of two red tickets in 3 draws with replacement).\n\n\nThen the probability of \\(A\\) has to be ______ (fill in the blank with a suitable phrase)\n\nbetween \\(0\\) and \\(1\\) (inclusive of \\(0\\) and \\(1\\)).\n\nIf \\(A\\) and \\(B\\) are two events with no outcomes in common,\n\n\nthen they are called ______. (fill in the blanks with suitable phrases)\n\nmutually exclusive\n\nIf \\(A\\) and \\(B\\) have no outcomes in common, that is, \\(A \\cap B =  \\{\\}\\), then \\(P(A \\cup B) = P(A) + P(B)\\).\nConsider an event \\(A\\). The complement of \\(A\\) is not \\(A\\), and denoted by \\(A^C\\). The complement of \\(A\\) consists of all outcomes in \\(\\Omega\\) that are not in \\(A\\) and \\(P(A^C) = 1-P(A)\\). (Why?)",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html#conditional-probabilities",
    "href": "3-generalization/02-computing-probs/notes.html#conditional-probabilities",
    "title": "Computing Probabilities",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\nIn the first example above, we saw that the probability of a red ticket on a draw changes if we sample without replacement. If we get a blue ticket on the first draw, the probability of a red ticket on the second draw is 2/3 (since there are 3 tickets left, of which 2 are blue). If we get a red ticket on the first draw, the probability of a red ticket on the second draw is 1/3. These probabilities, that depend on what has happened on the first draw, are called conditional probabilities. If \\(A\\) is the event of a blue ticket on the first draw, and \\(B\\) is the event of a red ticket on the second draw, we say that the probability of \\(B\\) given \\(A\\) is 2/3, which is a conditional probability, because we put a condition on the first card, that it had to be blue.\nWhat about if we don’t put a condition on the first card? What is the probability that the second card is red?\n\n\nCheck your answer\n\nThe probability that the second card drawn is red is 1/2, if we don’t have any information about the first card drawn. To see this, it is easier to imagine that we can shuffle all the cards in the box and they are put in some random order in which each of the 4 positions is equally likely. There are 2 red cards, so the probability that a red card will occupy any of the 4 positions, including the second, is 2/4.\n\nThis kind of probability, where we put no condition on the first card, is called an unconditional probability - we don’t have any information about the first card.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html#the-multiplication-rule-computing-the-probability-of-an-intersection",
    "href": "3-generalization/02-computing-probs/notes.html#the-multiplication-rule-computing-the-probability-of-an-intersection",
    "title": "Computing Probabilities",
    "section": "The Multiplication Rule: computing the probability of an intersection",
    "text": "The Multiplication Rule: computing the probability of an intersection\nWe often want to know the probability that two (or more) events will both happen: What is the probability if we roll a pair of dice, that both will show six spots; or if we deal two cards from a standard 52 card deck, that both would be kings, or in a family with two babies, both would suffer SIDS. What do we know? We can draw a Venn diagram to represent intersecting events:\n\n\n\n\n\nThis picture tells us that \\(A\\cap B\\) (the purple shaded part) is inside both \\(A\\) and \\(B\\), so its probability should be less than each of \\(P(A)\\) and \\(P(B)\\): \\(P(A\\cap B) \\le P(A), P(B)\\). In fact, we write the probability of the intersection as:\n\\[P(A \\cap B) = P(A) \\times P(B|A)\\] We read the second probability on the right-hand side of the equation as the conditional probability of \\(B\\) given \\(A\\). Note that \\(B\\vert A\\) is not an event, but we use \\(P(B\\vert A)\\) as a shorthand for the conditional probability of \\(B\\) given \\(A\\).\nFor example, in the example with the box with two red and two blue cards, let \\(A\\) is the event of drawing a red card on the first draw, and \\(B\\) is the event of drawing a blue card on the second draw. If we draw two cards without replacement, then we have that \\(P(A) = \\displaystyle \\frac{2}{4}\\), \\(P(B | A) =  \\displaystyle \\frac{2}{3}\\) (the denominator reduces by one, since there are only 3 cards left in the box, of which 2 are blue). Therefore:\n\\[P(A \\cap B) = P(A) \\times P(B|A) = \\frac{2}{4} \\times \\frac{2}{3} = \\frac{1}{3}\\] This becomes more clear if we think about the long run frequencies. We draw a red card first about half the time in the long run (if we think about drawing a card over and over again). Of those times, we would also draw a blue card second about two-thirds of the time, since a drawing a blue card would be twice as likely as drawing a red card. Therefore drawing a red card first and then a blue card would happen two thirds of one half of the time, which is about a third of the time.\nNote that the roles of \\(A\\) and \\(B\\) could be reversed in the expression above:\n\\[ P(A \\cap B) = P(A) \\times P(B | A) = P(B) \\times P(A | B)\\] This gives us a way to define the conditional probability, as long as we are not dividing by \\(0\\):\n\n\nConditional probability of \\(B\\) given \\(A\\)\n\nThis is defined to be the probability of the intersection of \\(A\\) and \\(B\\), normalized by dividing by \\(P(A)\\):\n\n\n\\[  P(B | A) = \\frac{ P(A \\cap B)}{P(A)} \\]\n\nThe idea here is that we know that \\(A\\) happened, therefore the only outcomes we are concerned about are the ones in \\(A\\) - this is our new outcome space. We compute the relative size of the part of \\(B\\) that happen (\\(A\\cap B\\)) to the size of the new outcome space.\n\n\n\n\n\n\nImportant\n\n\n\nNote that \\(P(A | B)\\) and \\(P(B | A)\\) can be very different. Consider the scenario shown in the Venn diagram here:",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html#independence",
    "href": "3-generalization/02-computing-probs/notes.html#independence",
    "title": "Computing Probabilities",
    "section": "Independence",
    "text": "Independence\n\n\nIndependent events\n\nWe say that two events are independent if the probabilities for the second event remain the same even if you know that the first event has happened, no matter how the first event turns out. Otherwise, the events are said to be dependent.\n\n\n\nIf \\(A\\) and \\(B\\) are independent, \\(P(B\\vert A) = P(B)\\).\nConsequently, the multiplication rule reduces to:\n\\[ P(A \\cap B) = P(A) \\times P(B | A) = P(A) \\times P(B) \\]\nUsually the fastest and most convenient way to check if two events are independent is to see if the product of their probabilities is the same as the probability of their intersection.\n\nComputational check for independence Check if \\(P(A \\cap B) = P(A)\\times P(B)\\)\n\nFor example, consider our box of red and blue tickets. When we draw with replacement, the probability of a red ticket on the second draw given a blue ticket on the first draw remains at 1/2. If we had a red ticket on the first draw, the probability of the second ticket being red is still 1/2. The probability doesn’t change because it does not depend on the outcome of the first draw, since we put the ticket back.\nIf we draw the tickets without replacement, we have seen that the probabilities of draws change. The probability of a blue ticket on the second draw given a red ticket on the first draw is 2/3, but the probability of a red ticket on the second draw given a red ticket on the first is 1/3.\nThe lesson here is that when we draw tickets with replacement, the draws are independent - the outcome of the first draw does not affect the second. If we draw tickets without replacement, the draws are dependent. The outcome of the first draw changes the probabilities of the tickets for the second draw.\n\nExample: Selecting 2 people out of a group of 5\n\n(drawing without replacement)\nWe have a group of 5 people: Alex, Emi, Fred, Max, and Nan. Two of the five are to be selected at random to form a two person committee. Represent this situation using draws from a box of tickets.\n\n\nCheck your answer\n\nWe only care about who is picked, not the order in which they are picked. For instance, picking Alex first and then Emi results in the same committee as picking first Emi and then Alex.\n\n\n\n\n\nAll the ten pairs are equally likely. On the first draw, there are 5 tickets to choose from, and on the second there are 4, making \\(5 \\times 4 = 20\\) possible draws of two tickets, drawn from this box, one at a time, without replacement. We have only 10 pairs here because of those 20 pairs, there are only 10 distinct ones. When we count 20 pairs, we are counting Alex \\(+\\) Emi as one pair, and Emi \\(+\\) Alex as another pair.\n\nWhat is the probability that Alex and Emi will be selected? Guess! (Hint: you have seen all the possible pairs above, and they are equally likely. What will be the probability of any one of them?)\nWe could use the multiplication rule to compute this probability, which is much simpler than writing out all the possible outcomes. The committee can consist of Alex and Emi either if Alex is drawn first and Emi second, or Emi is drawn first and Alex second. The probability that Alex will be drawn first is \\(1/5\\). The conditional probability that Emi will be drawn second given that Alex was drawn is \\(1/4\\) since there are only 4 tickets left in the box. Using the multiplication rule, the probability that Alex will be drawn first and Emi second is \\((1/4) \\times (1/5) = 1/20\\). Similarly, the probability that Emi will be drawn first and Alex second is \\(1/20\\). This means that the probability that Alex and Emi will be selected for the committee is \\(1/20 + 1/20 = 1/10\\).\n\n\n\nExample: Colored and numbered tickets\nI have two boxes that with numbered tickets colored red or blue as shown below.\n\n\n\n\n\nAre color and number independent or dependent for box 1? What about box 2?\nFor example, is the probability of a ticket marked 1 the same whether the ticket is red or blue?\n\n\nCheck your answer\n\nFor box 1, color and number are dependent, since the probability of 3 given that the ticket is red is 1/3, but the probability of 3 given that the ticket is blue is 0 (and similarly for the probability of 4).\nEven though the probability for 1 or 2 given the ticket is red is the same as the probability for 1 or 2 given the ticket is blue, we say that color and number are dependent because of the tickets marked 3 or 4.\nNow you work it out for box 2.\n\n\n\nExample: Tickets with more than one number on them\nNow I have two boxes that with numbered tickets, where each ticket has two numbers on them, as shown. For each box, are the two numbers independent or dependent? For example, if I know that the first number is 1 does it change the probability of the second number being 6 (or the other way around: if I know the second number is 6, does it change the probability of the first number being 1)?\n\n\n\n\n\n\n\nCheck your answer\n\nFor box 1, the first number and second number are independent, as shown below, using 1 and 6 as examples. If we know that the first number is 1, the box reduces as shown. The probability of the second number being 6 does not change for box 1. The probability does change for box 2, increasing from 1/2 to 2/3, since the second number is more likely to be 6 if the first number is 1.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html#back-to-sally-clark",
    "href": "3-generalization/02-computing-probs/notes.html#back-to-sally-clark",
    "title": "Computing Probabilities",
    "section": "Back to Sally Clark",
    "text": "Back to Sally Clark\nProfessor Roy Meadow claimed that the probability of two of Sally Clark’s sons dying of SIDS was 1 in 73 million. He obtained this number by multiplying 8543 by 8543, using the multiplication rule, treating the two events as independent. The question is, are they really independent? Was a crime really committed? Unfortunately for Sally Clark, two catastrophic errors were committed in her case by the prosecution, and not caught by the defense. (She appealed the decision, and was acquitted and released, but after spending 4 years in prison after being accused of murdering her babies. You can imagine how she was treated.)\nThe first error was in treating the deaths as independent, and the second was in looking at the wrong probability. Let’s look at the first mistake. It turns out that the probability of a second child dying of “cot death” or SIDS is 1/60 given that the first child died of SIDS. This was a massive error, and it turned out that the prosecution suppressed the pathology reports for the second baby, who had a very bad infection and might have died of that. It is also believed that male infants are more likely to suffer cot death.\nThe second error is an example of what is called the Prosecutor’s Fallacy. What is needed is \\(P(\\text{innocence }\\vert \\text{ evidence})\\), but it is often confused with (the much smaller) \\(P(\\text{evidence }\\vert \\text{ innocence})\\). They should have actually compared the probability of innocence given the evidence with the probability of murder given the evidence. These multiple errors ruined many lives. Though Sally Clark was eventually acquitted, helped by the Royal Statistical Society’s evidence, her life was shattered, and she died soon after being released. The moral of this story is to be very careful while multiplying probabilities. You must check to see if the events are actually independent.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html#de-mérés-paradox",
    "href": "3-generalization/02-computing-probs/notes.html#de-mérés-paradox",
    "title": "Computing Probabilities",
    "section": "De Méré’s paradox",
    "text": "De Méré’s paradox\nLet’s finally compute the probability of rolling at least one six in 4 rolls of a fair six-sided die. This is much easier to compute if we use the complement rule. The complement of at least one six is no sixes in 4 rolls. Each roll is independent of the other rolls because what you roll does not affect the values of future rolls. This means that we can use the multiplication rule to figure out the chance of no sixes in any of the rolls. The chance of no six in any particular roll is \\(5/6\\) (there are five outcomes that are not six).\nThe chance of no sixes in any of the 4 rolls is therefore \\(\\displaystyle \\left( \\frac{5}{6}\\right)^4\\) (because the rolls are independent). Using the complement rule, we get that: \\[ P(\\text{at least one six in 4 rolls}) = 1 - P(\\text{no sixes in any of the 4 rolls}) = 1 - \\left( \\frac{5}{6}\\right)^4 \\approx 0.518\\]\nSimilarly, the probability of at least 1 double six in 24 rolls of a pair of dice is given by \\[ 1- P(\\text{no double sixes in any of the 24 rolls}) = 1 - \\left( \\frac{35}{36}\\right)^{24} \\approx 0.491\\]\nBy the way, notice that the simulation was pretty accurate!\nBefore we wrap up these notes, let’s review two important things. The first has to do with a very common misconception, confusing mutually exclusive and independent events.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html#mutually-exclusive-vs-independent-events",
    "href": "3-generalization/02-computing-probs/notes.html#mutually-exclusive-vs-independent-events",
    "title": "Computing Probabilities",
    "section": "Mutually exclusive vs independent events",
    "text": "Mutually exclusive vs independent events\nNote that if two events \\(A\\) and \\(B\\), both with positive probability, are mutually exclusive, they cannot be independent. If \\(P(A \\cap B) = 0\\), but neither \\(P(A) =0\\) nor \\(P(B) = 0\\), then \\(P(A \\cap B) = 0 \\ne P(A)\\times P(B)\\). However, if two events are not independent, that does not mean they are mutually exclusive.\nThe second item generalizes the multiplication rule that we have already seen, by extending it to events that are not mutually exclusive. It is called the “inclusion-exclusion formula”.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html#inclusion-exclusion-generalized-addition-rule",
    "href": "3-generalization/02-computing-probs/notes.html#inclusion-exclusion-generalized-addition-rule",
    "title": "Computing Probabilities",
    "section": "Inclusion-exclusion (generalized addition rule)",
    "text": "Inclusion-exclusion (generalized addition rule)\nNow that we know how to compute the probability of the intersection of two events, we can compute the probability of the union of two events:\n\n\n\n\n\n\\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\]\nYou can see that if we just add the probabilities of \\(A\\) and \\(B\\), we double count the overlap. By subtracting it once, we can get the correct probability, and we know how to compute the probability of \\(A\\cap B\\). This is known as the inclusion-exclusion principle.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html#summary",
    "href": "3-generalization/02-computing-probs/notes.html#summary",
    "title": "Computing Probabilities",
    "section": "Summary",
    "text": "Summary\nIn this lecture, we do a deep dive into computing probabilities. It is well known that people are just not good at estimating probabilities of events, and we saw the tragic example of Sally Clark (who, even more sadly, is not a unique case)4.\nWe defined conditional probability and independence, and the multiplication rule, considering draws at random with and without replacement. We finally computed the probabilities in the dice games from 17th century France by combining the multiplication rule and the complement rule.\nWe noted that independent events are very different from mutually exclusive events, and finally we learned how to compute probabilities of unions of events that may not be mutually exclusive with the inclusion-exclusion or generalized addition rule.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/notes.html#footnotes",
    "href": "3-generalization/02-computing-probs/notes.html#footnotes",
    "title": "Computing Probabilities",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(https://www.theguardian.com/uk-news/2021/nov/20/sally-clark-cot-death-mothers-wrongly-jailed)↩︎\nFrom the archives of The Guardian newspaper https://www.theguardian.com/uk/2001/jul/15/johnsweeney.theobserver↩︎\nThe thumbnail image of the headline from the Manchester Evening News and some details of the case are from http://www.inference.org.uk/sallyclark↩︎\nhttps://www.theguardian.com/uk-news/2021/nov/20/sally-clark-cot-death-mothers-wrongly-jailed↩︎",
    "crumbs": [
      "Notes",
      "Generalization",
      "Computing Probabilities"
    ]
  },
  {
    "objectID": "3-generalization/02-computing-probs/slides.html#do-you-want-a-car-or-do-you-want-a-goat",
    "href": "3-generalization/02-computing-probs/slides.html#do-you-want-a-car-or-do-you-want-a-goat",
    "title": "Computing Probabilities",
    "section": "Do you want a car or do you want a goat?",
    "text": "Do you want a car or do you want a goat?\n\n\nOf course, this is the Monty Hall problem. The text for this comic is “A few minutes later, the goat behind door C drives away in the car. Can have some fun and ask them to if they chose door A and door B had a goat, would they stay with A or would they switch to C? (pollev) Setup: There are three doors A, B, C. Car behind one of them, goats behind other two. You pick a door at random (say you pick A), host opens one of the doors you didn’t pick (say they open B), and shows you a goat. Now should you stick with your original choice of A or should you switch to C. Will be fun to see what they input. The right answer is that they should switch. Waving hands, the total mass of 1 was split 1/3 to 2/3 (win to not win) with the choice of A. A is kept aside, and B is opened.Now the P(A doesn’t win) = 2/3 is moved to C.Therefore better to switch. Not worth doing it with formal notation even after going over Bayes"
  },
  {
    "objectID": "3-generalization/02-computing-probs/slides.html#agenda",
    "href": "3-generalization/02-computing-probs/slides.html#agenda",
    "title": "Computing Probabilities",
    "section": "Agenda",
    "text": "Agenda\n\nAnnouncements\nPS 6\nConcept Review\nConcept Questions\nBreak\nPS 7 (computing probabilities)\nBreak\n\n\n\nPS 7 ~ 20/25 minutes: time for them to work, and to review the PS (If you have already given enough time, do the coin flipping activity)\nLecture on conditional probability and independence\nConcept Review\nConcept questions\nBreak ~3 mins\nHandout: PS 8 (computing probabilities)"
  },
  {
    "objectID": "3-generalization/02-computing-probs/slides.html#announcements",
    "href": "3-generalization/02-computing-probs/slides.html#announcements",
    "title": "Computing Probabilities",
    "section": "Announcements",
    "text": "Announcements\n\nProblem Sets 6 and 7 (paper, max. 3) due Tuesday at 9am\n\n\n\nNo lab this week.\n\n\n\n\nRQ: Probability Distributions due Mon/Tues 11:59pm"
  },
  {
    "objectID": "3-generalization/02-computing-probs/slides.html#rules",
    "href": "3-generalization/02-computing-probs/slides.html#rules",
    "title": "Computing Probabilities",
    "section": "Rules",
    "text": "Rules\n\nConditional Probabilty\n\nFor two events \\(A\\) and \\(B\\), \\(P(A \\vert B) = \\displaystyle \\frac{P(A \\text{ and } B)}{P(B)}\\)\n\nMultiplication rule\n\nFor two events \\(A\\) and \\(B\\), \\(P(A \\text{ and } B) = P(A \\vert B) P(B)\\)\n\nComplement rule\n\n\\(P(A^C) = 1 - P(A)\\)\n\nYou can tell them here that \\(A^C\\) is “not A”, and then maybe use dice or coin toss examples. Review mutually exclusive vs independent events, emphasizing that these are very different ideas, though both apply to pairs of events.\n\nMutually exclusive events means that the occurrence of one event prevents the occurrence of the other. (that is, it reduces the chance of the other occurring to 0.)\n\nIndependent events means that the occurrence of one event does not change the chance of the other occurring."
  },
  {
    "objectID": "3-generalization/02-computing-probs/slides.html#concept-question-1",
    "href": "3-generalization/02-computing-probs/slides.html#concept-question-1",
    "title": "Computing Probabilities",
    "section": "Concept Question 1",
    "text": "Concept Question 1\n\n\n\n−+\n01:00\n\n\n\nFlip 3 coins, one at a time. Define the following events:\n\\(A\\) is the event that the first coin flipped shows a head\n\\(B\\) is the event that the first two coins flipped both show heads\n\\(C\\) is the event that the last two coins flipped both show tails\n\nThe events A and B are: ________\n\n\nAfter they do this and next, write out the outcome space for the results of flipping 3 coins. And go through what are A, B, and C. A and B are neither independent nor mutually exclusive."
  },
  {
    "objectID": "3-generalization/02-computing-probs/slides.html#concept-question-2",
    "href": "3-generalization/02-computing-probs/slides.html#concept-question-2",
    "title": "Computing Probabilities",
    "section": "Concept Question 2",
    "text": "Concept Question 2\n\n\n\n−+\n01:00\n\n\n\nFlip 3 coins, one at a time. Define the following events:\n\\(A\\) is the event that the first coin flipped shows a head\n\\(B\\) is the event that the first two coins flipped both show heads\n\\(C\\) is the event that the last two coins flipped both show tails\n\nThe events \\(A\\) and \\(C\\) are: ________\n\n\nAfter they do this and next, write out the outcome space for the results of flipping 3 coins. And go through what are A, B, and C. A and C are independent. Show that P(A & C ) = 1/8 = P(A)P(C) = 1/4 * 1/2"
  },
  {
    "objectID": "3-generalization/02-computing-probs/slides.html#concept-question-3",
    "href": "3-generalization/02-computing-probs/slides.html#concept-question-3",
    "title": "Computing Probabilities",
    "section": "Concept Question 3",
    "text": "Concept Question 3\nSuppose we draw 2 tickets at random without replacement from a box with tickets marked {1, 2, 3, . . . , 9}. Let A be the event that at least one of the tickets drawn is labeled with an even number, let B be the event that at least one of the tickets drawn is labeled with a prime number (recall that the number 1 is not regarded as a prime number). Suppose the numbers on the tickets drawn are 3 and 9.\n\nWhich of the following events occur?\n\n\\(A\\)\n\\(B\\)\n\\(A\\) and \\(B\\) (\\(A \\cap B\\))\n\\(A\\) and \\(B^C\\)\n\\(A^C\\) and \\(B\\)\n\n\n\nii and v, I told them they can immediately see that A^C is true since no even number, so definitely (v) and then we do have a prime number and both are odd so (ii) is true.\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "3-generalization/02-computing-probs/slides.html#section",
    "href": "3-generalization/02-computing-probs/slides.html#section",
    "title": "Computing Probabilities",
    "section": "",
    "text": "−+\n02:00\n\n\n\n\n\nThe Houston Astros beat the Philadelphia Phillies in the 2022 World Series. The winners in the World Series have to win a majority of 7 games, so the first team to win 4 games wins the series (best of 7). The Astros were heavily favored to win, so the outcome wasn’t really a suprise. Suppose we assumed that the probability that the Astros would have beaten the Phillies in any single game was estimated at 60%, independently of all the other games. What was the probability that the Astros would have won in a clean sweep?\n(Clean sweep means that they won in the first 4 games - which didn’t happen, they won in 6 games.)\n\n\nStraightforward application of multiplication rule for independent events. probability is 0.6^4 = 0.1296. Follow up comment/question: if the teams were more evenly matched, say probability of either team winning is 50%, the probability of going to 7 games and probability of winning in 6 games is equal given 5 games are played."
  },
  {
    "objectID": "3-generalization/02-computing-probs/slides.html#concept-question-5",
    "href": "3-generalization/02-computing-probs/slides.html#concept-question-5",
    "title": "Computing Probabilities",
    "section": "Concept Question 5",
    "text": "Concept Question 5\n\n\n\n−+\n01:00\n\n\n\n\n\nSuppose we assume, instead, that the probability that the Astros would have beaten the Phillies in any single game was 50%, independently of all the other games. In this case, was the probability that the series would have gone to 6 games higher than the probability that the series would have gone to 7 games, given that 5 games were played?\n\n\nif 5 games are played it means one of the teams is leading 3-2. Therefore there are two scenarios: the team that is ahead wins game 6 and the series, or the team that is behind wins game 6 and they go to 7 games. Both are equally likely."
  },
  {
    "objectID": "5-prediction/05-logistic-regression/notes.html",
    "href": "5-prediction/05-logistic-regression/notes.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "The framework that we used to build a predictive model for regression followed four distinct steps:\nWe used this process to build our first simple linear regression model to predict high school graduation rates but the same four steps are used by Zillow to build ZestimateTM, their deep learning model to predict house price.\nThese are also the same four steps that we will use as we shift in these notes to the task of classification. In a classification task, we seek to predict a response variable that is categorical, very often a two-level categorical variable. Classification models are everywhere: they help doctors determine whether or not a patient has a disease, whether or not an image contains a particular object (say, a person or a cat), and whether or not customer will purchase an item."
  },
  {
    "objectID": "5-prediction/05-logistic-regression/notes.html#example-spam-filters",
    "href": "5-prediction/05-logistic-regression/notes.html#example-spam-filters",
    "title": "Logistic Regression",
    "section": "Example: Spam filters",
    "text": "Example: Spam filters\nEmail spam, also referred to as junk email or simply spam, is unsolicited messages sent in bulk by email (spamming). The name comes from a Monty Python sketch in which the name of the canned pork product Spam is ubiquitous, unavoidable, and repetitive1. A spam filter is a classification model that determines whether or not a message is spam based on properties of that message. Every mainstream email client, including Gmail, has a spam filter built in to ensure that the messages that get through to the user are genuine messages.\n\nThe Data\nIn order to build a spam filter, we’ll need a data set to train our model on. This data set must have as the unit of observation a single email message, record whether or not the message was spam (the response), and record various features of the message that are associated with being spam (the predictors). Such a data set, with nearly 4000 observations, can be found in the email data frame in the openintro library.\n\n\n# A tibble: 3,921 × 21\n   spam  to_multiple from     cc sent_email time                image attach\n   &lt;fct&gt; &lt;fct&gt;       &lt;fct&gt; &lt;int&gt; &lt;fct&gt;      &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n 1 0     0           1         0 0          2012-01-01 06:16:41     0      0\n 2 0     0           1         0 0          2012-01-01 07:03:59     0      0\n 3 0     0           1         0 0          2012-01-01 16:00:32     0      0\n 4 0     0           1         0 0          2012-01-01 09:09:49     0      0\n 5 0     0           1         0 0          2012-01-01 10:00:01     0      0\n 6 0     0           1         0 0          2012-01-01 10:04:46     0      0\n 7 0     1           1         0 1          2012-01-01 17:55:06     0      0\n 8 0     1           1         1 1          2012-01-01 18:45:21     1      1\n 9 0     0           1         0 0          2012-01-01 21:08:59     0      0\n10 0     0           1         0 0          2012-01-01 18:12:00     0      0\n# ℹ 3,911 more rows\n# ℹ 13 more variables: dollar &lt;dbl&gt;, winner &lt;fct&gt;, inherit &lt;dbl&gt;, viagra &lt;dbl&gt;,\n#   password &lt;dbl&gt;, num_char &lt;dbl&gt;, line_breaks &lt;int&gt;, format &lt;fct&gt;,\n#   re_subj &lt;fct&gt;, exclaim_subj &lt;dbl&gt;, urgent_subj &lt;fct&gt;, exclaim_mess &lt;dbl&gt;,\n#   number &lt;fct&gt;\n\n\nWe see in the left most column the response variable, spam, coded as 0 when a message is not spam and 1 when it is. The first predictor, to_multiple, is a 1 if the message was sent to multiple people and 0 otherwise. cc records the number of people who are cc’ed on the email. winner (listed as a variable below the glimpse of the dataframe) records whether or not the word “winner” showed up in the message. The remaining predictors you may be able to intuit by their names, but you can also read the help file that describes each one.\nThese variables seem like they might be useful in predicting whether or not an email is spam, but take a moment to consider: how were we able to get our hands on data like this?\nThis particular data set arose from selecting a single email account, saving every single messages that comes in to that address over the course of a month, processing each message to create values for the predictor variables, then visually classifying whether or not the message is spam. That’s to say: this data represents a human’s best effort to classify spam2. Can we build a model that will be able to identify the features that mark a message as spam to be able to automatically classify future messages?\n\n\n\n\n\nExploratory Data Analysis\nLet’s see how well a few tempting predictors work at separating spam from not spam by performing some exploratory data analysis. We might expect messages containing the word “winner” to be more likely to be spam than those that do not. A stacked, normalized bar chart can answer that question.\n\n\n\n\n\n\n\n\n\nIndeed, it looks like around 30% of emails with “winner” were spam, compared to roughly 10% of those without. At this point, we could consider a very simple spam filter: if the message contains “winner”, then classify it as spam.\nAlthough this is tempting, it is still a pretty weak classifier. Most of the messages with “winner” are not spam, so calling them spam will result in most of them being misclassified.\nSo if “winner” isn’t the silver bullet predictor we need, let’s try another: num_char. This variable records the total number of characters present in the message – how long it is. We probably have no prior sense of whether spam would be more likely to consist of short or long emails, so let’s visualize them. This predictor is numerical continuous, so let’s overlay two density plots to get a sense of the distribution between spam and not spam.\n\n\n\n\n\n\n\n\n\nThe original plot on the left is very difficult to read because this variable is heavily right-skewed: there are a small number of very long messages that obscure much of the data in the plot. On the right is a more useful visualization, one of the log-transformed version of the same variable.\nWe see a reasonable separation here: spam messages tend to be shorter than non-spam messages. We could consider another very simple spam filter: if the log number of characters is less than 1.2, classify it as spam.\nHowever, this simple filter suffers from the same problem as the first. Although these density plots have the same area, there are in fact far fewer overall instances of spam than not-spam. That means that there are far more not-spam messages with a log number of characters less than 1.2 than there are spam message. This filter, like the first, would misclassify much of the training data.\nWhat we need is a more general framework to fold the strength of these two variables - as well as many of the other ones in the data set - into a model that can produce a single, more accurate prediction. We will start from the framework of multiple linear regression and move to something that can be used for classification."
  },
  {
    "objectID": "5-prediction/05-logistic-regression/notes.html#from-linear-to-logistic-regression",
    "href": "5-prediction/05-logistic-regression/notes.html#from-linear-to-logistic-regression",
    "title": "Logistic Regression",
    "section": "From Linear to Logistic Regression",
    "text": "From Linear to Logistic Regression\nLet’s start by taking our existing simple linear regression model from the past few notes and applying it in this classification setting. We can visualize the relationship between log_num_char and spam using an ordinary scatter plot.\n\n\n\n\n\nThis is a strange looking scatter plot - the dots can only take y values of 0 or 1 - but it does capture the overall trend observed in the density plots, that spam messages are longer. Since we have a scatter plot, we can fit a simple linear regression model using the method of least squares (in gold).\n\\[ \\hat{y} = b_0 + b_1 x \\]\nWhile this is doable, it leaves us with a bit of a conundrum. For a low value of log_num_char it’s possible that we would predict \\(\\hat{y} = 1\\) and for a high value it’s possible that we’d predict \\(\\hat{y} = 0\\). But what if log_num_char is somewhere in the middle? What does it mean if we predict that the value of spam is .71?\nOne approach to resolving this is to treat our prediction not as a value of \\(y\\), but as a estimate of the probability, \\(\\hat{p}\\), that \\(y = 1\\). We can rewrite the model as:\n\\[ \\hat{p} = b_0 + b_1 x \\]\nThis resolves the conundrum of how to think about a prediction of .71. That is now the model’s determination of the probability that the message is spam. This tweak solves one problem, but it introduces another. How do we interpret predictions at very high values of log_num_char, where \\(\\hat{p}\\) is negative? Surely a probability cannot be negative!\nWe can fix this by changing the mathematical form of the model used to predict the response. Instead of it being a line, we can use an alternative function that prevents predictions greater than 1 and less than zero. The most commonly used function is called the standard logistic function:\n\\[ f(z) = \\frac{1}{1 + e^{-z}}\\]\n\\(z\\) can be any number of the real number line. As \\(z\\) gets large, \\(f(z)\\) approaches 1; as \\(z\\) is negative, \\(f(z)\\) approaches 0; when \\(z\\) is 0, \\(f(z)\\) is .5.\nThis is a very clever idea. It allows us to combine all of the information from our predictors into a single numerical score, \\(z\\). We can obtain \\(z\\) through a form that is familiar to us: \\(b_0 + b_1 x_1 + \\ldots + b_p x_p\\). This score can then be sent through the logistic function to estimate the probability that \\(y = 1\\). This method is called logistic regression.\n\nLogistic Regression (for prediction)\n\nA model to predict the probability that 0-1 response variable \\(y\\) is 1 using the inverse logit of a linear combination of predictors \\(x_1, x_2, \\ldots, x_p\\).\n\\[ \\hat{p} = \\frac{1}{1 + e^{-\\left(b_0 + b_1 x_1 + \\ldots + b_p x_p\\right)}} \\]\nCan be used as a classification model by setting up a rule of the form: if \\(\\hat{p}_i\\) &gt; threshold, then \\(\\hat{y}_i = 1\\).\n\n\nWe can visualize the approach that logistic regression takes by sketching the predictions as a green s-shaped curve on top of our scatter plot."
  },
  {
    "objectID": "5-prediction/05-logistic-regression/notes.html#classification-errors",
    "href": "5-prediction/05-logistic-regression/notes.html#classification-errors",
    "title": "Logistic Regression",
    "section": "Classification errors",
    "text": "Classification errors\nNow that we have a model archetype to move forward with, we are almost ready to build it and see how it does. First though, we should first consider what happens when our predictions are wrong. Hopefully, we will classify most of our e-mails correctly, but we will fail at least some of the time. We might predict that an e-mail is spam when it is not. In other cases, we might predict that an e-mail is not spam when it actually is. Therefore, there are two types of error we should consider.\nRecall that based on the email dataset, 1 refers to an e-mail which is truly spam. In a binary classification problem, we often call 1 a positive result. Likewise, 0 refers to an e-mail which is genuine. We call 0 a negative result. This sets up formal definitions for the two types of errors mentioned above.\n\nFalse Positives\n\nPredicting a 1 that is in fact a 0\n\nFalse Negatives\n\nPredicting a 0 that is in fact a 1\n\n\n\nMisclassification rate\nWhile thinking about failing is not exciting, it does give us a way to think about how well our classification model is doing. We would like the number of misclassifications over a large number of e-mails to be as small as possible. This fraction can be quantified as the misclassification rate or \\(\\text{MCR}\\) for short.\nMisclassification Rate\n\\[\\text{MCR} = \\frac{\\text{no. FP} + \\text{no. FN}}{\\text{no. of predictions}} = \\frac{\\text{no. of misclassifications}}{\\text{no. of predictions}}\\]\nJust like in a multiple linear regression context, we can fit a suite of different models on the training set and evaluate their performance on the testing set. Now, our model type of choice is logistic regression and our evaluation metric is \\(\\text{MCR}\\)."
  },
  {
    "objectID": "5-prediction/05-logistic-regression/notes.html#the-ideas-in-code",
    "href": "5-prediction/05-logistic-regression/notes.html#the-ideas-in-code",
    "title": "Logistic Regression",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\n\nFitting Logistic Regression with glm()\nThe computational machinery for fitting logistic regression looks almost identical to what we used for linear least squares regression. The primary function we’ll use is glm(). We will also use a different broom function called tidy() to display the coefficients \\(b_0\\) and \\(b_1\\). tidy() can also be used in a linear regression context as well.\n\n\n# A tibble: 2 × 3\n  term          estimate std.error\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     -1.72     0.0606\n2 log(num_char)   -0.544    0.0365\n\n\nThese coefficients are a bit more challenging to interpret since they’re no longer linearly related to the response. The sign of the coefficient for log(num_char), however, is informative: it tells us that messages with more characters will be predicted to have a lower probability of being spam.\n\n\nEvaluating model performance\n\nCalculating \\(\\text{MCR}\\)\nLet’s take a look at the predictions that this model makes back into the data set that it was trained on. When using the predict() function on logistic regression models, there are several different types of predictions that it can return, so be sure to use the additional argument type and supply it with \"response\". This will return \\(\\hat{p}_i\\) (probability of 1) for each observation.\nWe can then move from values of \\(\\hat{p}_i\\) to values of \\(\\hat{y}_i\\). In general, our rule will be to check and see whether each value of \\(\\hat{p}_i\\) is greater than .5. If so, we will assign the value of 1; otherwise, we will assign it 0. This sounds similar to creating a logical variable, but assigning something different than TRUE and FALSE to each observation in the dataset. We will use a function called ifelse(), therefore, which will allow us to assign we want (1 and 0).\nThe following block of code completes both of these two steps and saves the results back into the email data frame.\nWe are now ready to calculate \\(\\text{MCR}\\). First, we can find all of the emails for which the y and y_hat_m1 don’t match. Then, we can take the proportion of those e-mails out of all e-mails in the dataset.\n\n\n# A tibble: 1 × 1\n     MCR\n   &lt;dbl&gt;\n1 0.0956\n\n\nOverall, we are misclassifying around 10 percent of all e-mails (both spam and genuine), which doesn’t look like a bad start, but we might want to take a little deeper of a dive to see how the model is misclassifying observations.\n\n\n\nFalse positives versus false negatives\nIndeed, we can see if the model is failing at classifiying one type of e-mail more than another. To do this, we can find the number of false positives and false negatives. We can group our observations by their actual class versus their predicted class. There are four possibilities, so there will be four groups.\n\nemail |&gt;\n  group_by(spam, y_hat_m1) |&gt;\n  summarise(n = n())\n\n# A tibble: 4 × 3\n# Groups:   spam [2]\n  spam  y_hat_m1     n\n  &lt;fct&gt;    &lt;dbl&gt; &lt;int&gt;\n1 0            0  3541\n2 0            1    13\n3 1            0   362\n4 1            1     5\n\n\nWe actually see that the model is doing great at predicting correctly when the e-mail is genuine (few false positives), but doing horribly at detecting spam (many, many false negatives). Only \\(5\\) out of the \\(367\\) spam e-mails are being classified correctly. Essentially all of our mistakes are in the form of false negatives! We also can see here that there are way more genuine e-mails than spam e-mails in the dataset, so our misclassification rate is being inflated as a result. Clearly, more than just the length of an e-mail is necessary to help us detect spam.\n\nTraining and testing sets\nOne final note: this \\(\\text{MCR}\\) was calculated over all of the data. The best way to evaluate the performance of the model is to split the data into training and testing sets, fit the model on the training set and evaluate it on the testing set. We can calculate both training and testing versions of \\(\\text{MCR}\\) and compare them to see if the model is doing well on e-mails it hasn’t yet seen."
  },
  {
    "objectID": "5-prediction/05-logistic-regression/notes.html#summary",
    "href": "5-prediction/05-logistic-regression/notes.html#summary",
    "title": "Logistic Regression",
    "section": "Summary",
    "text": "Summary\nIn these notes we introduced the concept of classification using a logistic regression model. Logistic regression uses the logistic function to transform predictions into a probability that the response is 1. These probabilities can be used to classify y as 0 or 1 by checked to see if they exceed a threshold (often .5).\nWe then went through the process of fitting logistic regression to help us classify spam e-mails, and evaluated our results using the misclassification rate."
  },
  {
    "objectID": "5-prediction/05-logistic-regression/notes.html#footnotes",
    "href": "5-prediction/05-logistic-regression/notes.html#footnotes",
    "title": "Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDefinition from Wikipedia, along with the image, by freezelight/flickr.↩︎\nThis data collection and manual processing was done by a graduate student in statistics at UCLA. One of the many humdrum but valuable tasks asked of graduates students . . .↩︎"
  },
  {
    "objectID": "5-prediction/02-improving-predictions/slides.html#agenda",
    "href": "5-prediction/02-improving-predictions/slides.html#agenda",
    "title": "Evaluating and Improving Predictions",
    "section": "Agenda",
    "text": "Agenda\n\nAnnouncements\nConcept Questions\nProblem Set\nLab"
  },
  {
    "objectID": "5-prediction/02-improving-predictions/slides.html#announcements",
    "href": "5-prediction/02-improving-predictions/slides.html#announcements",
    "title": "Evaluating and Improving Predictions",
    "section": "Announcements",
    "text": "Announcements\n\nProblem Sets:\n\nPS 16 (one-side) released Tuesday and due next Tuesday at 9am\nPS 17 (one-side) released today and due next Tuesday at 9am\nExtra Practice released Thursday (non-turn in)\n\n\n\n\nLab 5:\n\nLab 5.1 released Tuesday and due next Tuesday at 9am\nLab 5.2 released Thursday and due next Tuesday at 9am\nLab 5 Workshop next Monday"
  },
  {
    "objectID": "5-prediction/02-improving-predictions/slides.html#section",
    "href": "5-prediction/02-improving-predictions/slides.html#section",
    "title": "Evaluating and Improving Predictions",
    "section": "",
    "text": "Which four models will exhibit the highest \\(R^2\\)?\n\n\n\n\n−+\n01:00\n\n\n\n\nThis is a repeat of a previous CQ, but now with a linear model and a different characteristic of interest: R^2. The two highest R^2 values should be clearly B and E. The second two are very difficult to discern based on these plots.\nThis is a good time to mention that for a least squares linear models, R^2 is indeed just the square of the Pearson correlation coefficient. This isn’t true of non-linear models."
  },
  {
    "objectID": "5-prediction/02-improving-predictions/slides.html#section-1",
    "href": "5-prediction/02-improving-predictions/slides.html#section-1",
    "title": "Evaluating and Improving Predictions",
    "section": "",
    "text": "# A tibble: 4 × 5\n  name    hours cuteness food_eaten is_indoor_cat\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;lgl&gt;        \n1 castiel    12      9          175 TRUE         \n2 frank      18     10          200 TRUE         \n3 luna       19      9.5        215 FALSE        \n4 luca       10      8          218 FALSE        \n\n\n\nm1 &lt;- lm(formula = hours ~ cuteness + food_eaten + is_indoor_cat, \n         data = cats)\n\n\n\n      (Intercept)          cuteness        food_eaten is_indoor_catTRUE \n    -3.800000e+01      6.000000e+00      2.815002e-16     -4.000000e+00 \n\n\n\nHow many hours does the model predict Frank will sleep each day? Write out the linear equation of the model from the model output to help you.\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "5-prediction/02-improving-predictions/slides.html#section-2",
    "href": "5-prediction/02-improving-predictions/slides.html#section-2",
    "title": "Evaluating and Improving Predictions",
    "section": "",
    "text": "Which is the most appropriate non-linear transformation to apply to time_being_pet?\n\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "5-prediction/03-overfitting/notes.html",
    "href": "5-prediction/03-overfitting/notes.html",
    "title": "Overfitting",
    "section": "",
    "text": "Below is data we collected about the association between number of hours studied and students’ test scores in a math class. Our goal is to predict the exam score from number of hours studied. Both plots below show the same data, but show the predictions from two different predictive models.\nWhich model looks more appropriate: the blue, or the red? More specifically,\nThe blue model seems more reasonable: studying more should steadily increase your score. The predictive model on the right seems like it took the particular data points “too seriously!” This will be an issue if a new set of students from the same class comes along and we want to predict what their exam scores will be based on the amount of hours studied. Let’s use the blue and red models to predict scores from more students from this same class.\nWe see that the blue line is prepared to predict the exam scores well enough for these students–even though the model was not fit using them! The red model, however, does poorly. It is so beholden to the first group of students that it doesn’t know how to manage when the students are even slightly different. In statistics, we say that the red model was overfit."
  },
  {
    "objectID": "5-prediction/03-overfitting/notes.html#overfitting-with-polynomials",
    "href": "5-prediction/03-overfitting/notes.html#overfitting-with-polynomials",
    "title": "Overfitting",
    "section": "Overfitting with polynomials",
    "text": "Overfitting with polynomials\nUsually, overfitting occurs as a result of applying a model that is too complex, like the red one we saw for the math class data above. We created that overfitted predictive model on the right by fitting a polynomial with a high degree. Polynomials are quite powerful models and are capable of creating very complex predictive functions. The higher the polynomial degree, the more complex function it can create.\nLet’s illustrate by fitting polynomial models with progressively higher degrees to the data set above.\n\n\n\n\n\n\n\n\n\nThe higher the polynomial degree, the closer the prediction function comes to perfectly fitting the data1. Therefore, when it comes to evaluating which model is the best for prediction, we would say the degree seven polynomial is best. Indeed, based on our knowledge so far, it would have the highest \\(R^2\\). The true test is yet to come, though. Let’s measure these three models on how well they predict to the second group of students that weren’t used to fit the model.\n\n\n\n\n\n\n\n\n\nAs we increase the degree, the polynomial begins to perform worse on this new data as it bends to conform to the original data. For example, we see that for the student having studied around five and a half hours, the fifth degree polynomial does well, but the seven degree polynomial does horribly! To put a cherry on top, the red model we showed you in the beginnng of these notes was a twenty degree polynomial!\nWhat we see is that the higher the degree, the more risk we run of using a model that overfits."
  },
  {
    "objectID": "5-prediction/03-overfitting/notes.html#training-and-testing-sets-a-workflow-to-curb-overfitting",
    "href": "5-prediction/03-overfitting/notes.html#training-and-testing-sets-a-workflow-to-curb-overfitting",
    "title": "Overfitting",
    "section": "Training and testing sets: a workflow to curb overfitting",
    "text": "Training and testing sets: a workflow to curb overfitting\nWhat you should have taken away so far is the following: we should not fit the model (set the \\(b_0, b_1\\) coefficients) and evaluate the model (judge its predictive power) with the same data set!\nWe can further back up this idea quantitatively. The plot below shows the \\(R^2\\) value for math class models fit with different polynomial degrees.\n\n\n\n\n\n\n\n\n\nThe \\(R^2\\) value goes steadily upwards as the polynomial degree goes up. In fact this is mathematically guaranteed to happen: for a fixed data set the \\(R^2\\) value for a polynomial model with higher degree will always be higher than a polynomial model with lower degree.\nThis should be disconcerting, especially since we earlier saw that the model with the highest \\(R^2\\) did the worst on our unseen data. What you might also notice is that the \\(R^2\\) isn’t increasing by that much between degrees as the degree gets higher. This suggests that adding that additional degree isn’t improving our general predictive power much; it’s just helping the model tailor itself to the specific data we have.\nDoes that mean \\(R^2\\) is not a good metric to evaluate our model? Not necessarily. We can just change our workflow slightly. Instead of thinking in terms of a single data set, we can partition, or split the observations of the data set into two separate sets. We can use one of these data sets to fit the model, and the other to evaluate it.\n\nTraining Set\n\nThe set of observations used to fit a predictive model; i.e. estimate the model coefficients.\n\nTesting Set\n\nThe set of observations used to assess the accuracy of a predictive model. This set is disjoint from the training set.\n\n\nThe partition of a data frame into training and testing sets is illustrated by the diagram below.\n\n\n\n\n\n\n\n\ny\nx1\nx2\nx3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe original data frame consists of 10 observations. For each observation we have recorded a response variable, \\(y\\), and three predictors, \\(x_1, x_2\\), and \\(x_3\\). If we do an 80-20 split, then 8 of the rows will randomly be assigned to the training set (in blue). The 2 remaining rows (rows 2 and 6) are assigned to the testing set (in gold).\nSo to recap, our new workflow for predictive modeling involves:\n\nSplitting the data into a training and a testing set\nFitting the model to the training set\nEvaluating the model using the testing set\n\n\nMore on splitting the data\nAs in the diagram above, a standard partition is to dedicate 80% of the observations to the training set and the remainder to the testing set (a 80-20 split), though this is not a rule which is set in stone. The other question is how best to assign the observations to the two sets. In general, it is best to do this randomly to avoid one set that is categorically different than the other.\n\n\nMean square error: another metric for evaluation\nWhile \\(R^2\\) is the most immediate metric to evaluate the predictive quality of a linear regression, it is quite specific to linear modeling. Therefore, data scientists have come up with another, more general metric called mean square error (MSE). Let \\(y_i\\) be observations of the response variable in the testing set, and \\(\\hat{y}_i\\) be your model’s predictions for those observations. Then \\(\\text{MSE}\\) is given by\n\\[ \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y_i})^2\\] You may notice that for a linear regression model, \\(\\text{MSE} = \\frac{1}{n}\\text{RSS}\\).\nA common offshoot is root mean square error (\\(\\text{RMSE}\\)), which you can obtain by taking the square root of \\(\\text{MSE}\\). Much like what standard deviation does for variance, \\(\\text{RMSE}\\) allows you to think above the average error on a regular scale rather than on a squared scale."
  },
  {
    "objectID": "5-prediction/03-overfitting/notes.html#the-ideas-in-code",
    "href": "5-prediction/03-overfitting/notes.html#the-ideas-in-code",
    "title": "Overfitting",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nLet’s shift the subject to mathematics to biology, and illustrate the training and testing approaching to evaluating predictions for the exam scores from a biology class with 200 students using as a predictor the number of hours that they have studied. Let’s visualize these data first.\n\n\n\n\n\n\n\n\n\nHere we are going to compare two models: a simple linear model versus a 5th degree polynomial, both fit using the method of least squares.\n\n\\(\\textbf{Model 1:} \\quad \\widehat{score} = b_0 + b_1 \\times hours\\)\n\\(\\textbf{Model 2:} \\quad \\widehat{score} = b_0 + b_1 \\times hours + b_2 \\times hours^2 + b_3 \\times hours^3 + b_4 \\times hours^4 + b_5 \\times hours^5\\)\n\n\nStep 1: Split data\nWe’ll use an 80-20 split, with each observation assigned to its set randomly. There are many ways to do this via code: here is one using functions we’ve seen.\n\nGenerate a vector of \\(n\\) observations (in this case, our data has 200 observations) in which approximately 80 percent of the observations are \"train\" and 20 percent of the observations are \"test\". To do this, we can make use of the sample() function.\n\n\nset.seed(20)\n\ntrain_or_test &lt;- sample(x = c(\"train\", \"test\"), \n                   size = 200, \n                   replace = TRUE, \n                   prob = c(0.8, 0.2))\n\n\nmutate this vector onto our data frame (our data frame here is called biology). Below, you can see which rows in the data frame have been assigned to \"train\" and which have been assigned to \"test\".\n\n\nbiology &lt;- biology |&gt;\n    mutate(set_type = train_or_test)\n\n\n\n# A tibble: 6 × 3\n  hours score set_type\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1  6.30  74.6 test    \n2  6.30  73.4 train   \n3  7.40  76.6 train   \n4  9.97  95.1 test    \n5  9.58  82.4 train   \n6  8.19  84.0 train   \n\n\n\nsplit the data based on whether the observations are in the \"train\" or \"test\" set.\n\n\nbiology_train &lt;- biology |&gt;\n    filter(set_type == \"train\")\n\nbiology_test &lt;- biology |&gt;\n    filter(set_type == \"test\")\n\n\n\nStep 2: Fit the model to the training set\nNow fit two models on the training data. We will be using lm(), and for both models, the data argument is given by biology_train.\n\nlm_slr &lt;- lm(score ~ hours, data = biology_train)\nlm_poly &lt;- lm(score ~ poly(hours, degree = 20, raw = T),\n              data = biology_train)\n\n\nWe can evaluate the \\(R^2\\)’s for both models’ performance on the training data just like before with glance(). Which model do you expect to have a better training set \\(R^2\\) value?\n\nlibrary(broom)\n\nglance(lm_slr) %&gt;%\n    select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.693\n\nglance(lm_poly) %&gt;%\n    select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.715\n\n\nJust as we might have guessed from looking at the model fits, the polynomial model has a better \\(R^2\\) value when evaluated on the training set.\n\n\nStep 3: Evaluate the model on the testing set.\nThe real test of predictive power between the two models comes now, when we will make exam score predictions using the testing set: data which the model was not used to fit and hasn’t seen.\nWe will still be using the predict() function for this purpose. Now, we can just plug biology_test into the newdata argument!\n\nscore_pred_linear &lt;- predict(lm_slr, newdata = biology_test)\nscore_pred_poly &lt;- predict(lm_poly, newdata = biology_test)\n\nOnce these predictions \\(\\hat{y}_i\\) are made, we then can use dplyr code to:\n\nmutate on the predictions to our testing data\nset up the \\(R^2\\) formula and calculate2. In the code below, we are using the formula\n\n\\[R^2 = 1-\\frac{\\text{RSS}}{\\text{TSS}} = 1-\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2}\\]\n\nWe can also calculate \\(\\text{MSE}\\) and \\(\\text{RMSE}\\) as \\(\\frac{1}{n}\\text{RSS}\\) and \\(\\frac{1}{n}\\sqrt{\\text{RSS}}\\), respectively.\n\n\nbiology_test %&gt;%\n    mutate(score_pred_linear = score_pred_linear,\n           score_pred_poly = score_pred_poly,\n           resid_sq_linear = (score - score_pred_linear)^2,\n           resid_sq_poly = (score - score_pred_poly)^2) %&gt;%\n    summarize(TSS = sum((score - mean(score))^2),\n              RSS_linear = sum(resid_sq_linear),\n              RSS_poly = sum(resid_sq_poly),\n              n = n()) %&gt;%\n    mutate(Rsq_linear = 1 - RSS_linear/TSS,\n           Rsq_poly = 1 - RSS_poly/TSS,\n           MSE_linear = RSS_linear/n,\n           MSE_poly = RSS_poly/n,\n           RMSE_linear = sqrt(MSE_linear),\n           RMSE_poly = sqrt(MSE_poly)) |&gt;\n  select(Rsq_linear, Rsq_poly, MSE_linear, MSE_poly)\n\n# A tibble: 1 × 4\n  Rsq_linear Rsq_poly MSE_linear MSE_poly\n       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1      0.664    0.629       26.6     29.3\n\n\nVoila the linear model’s test set \\(R^2\\) is better than the polynomial model’s test \\(R^2\\)! We also see the \\(\\text{MSE}\\) for the linear model is lower than that for the polynomial model.\nSo which is the better predictive model: Model 1 or Model 2? In terms of training, Model 2 came out of top, but Model 1 won out in testing.\nAgain, while training \\(R^2\\) can tell us how well a predictive model explains the structure in the data set upon which it was trained, it is deceptive to use as a metric of true predictive accuracy. The task of prediction is fundamentally one applied to unseen data, so testing \\(R^2\\) is the appropriate metric. Model 1, the simpler model, is the better predictive model. After all, the data we are using looks much better modeled by a line than a five degree polynomial."
  },
  {
    "objectID": "5-prediction/03-overfitting/notes.html#summary",
    "href": "5-prediction/03-overfitting/notes.html#summary",
    "title": "Overfitting",
    "section": "Summary",
    "text": "Summary\nThis lecture is about overfitting: what happens when your model takes the particular data set it was built on too seriously. The more complex a model is, the more prone to overfitting it is. Polynomial models are able to create very complex functions thus high-degree polynomial models can easily overfit. Fitting a model and evaluating it on the same data set can be problematic; if the model is overfitted the evaluation metric (e.g. \\(R^2\\)) might be very good, but the model might be lousy on predictions on new data.\nA better way to approach predictive modeling is to fit the model to a training set then evaluate it with a separate testing set."
  },
  {
    "objectID": "5-prediction/03-overfitting/notes.html#footnotes",
    "href": "5-prediction/03-overfitting/notes.html#footnotes",
    "title": "Overfitting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe say a function that perfectly predicts each data point interpolates the data. See the first red curve for the math class exams.↩︎\nBecause \\(\\hat{y}_i\\) involve information from the training data and \\(y_i\\) and \\(\\bar{y}\\) come from the testing data, the decomposition of the sum of squares does not work. So, we cannot interpret testing \\(R^2\\) as we would training \\(R^2\\), and you may have a testing \\(R^2\\) less than 0. However, higher \\(R^2\\) values still signal that the model has good predictive power.↩︎"
  },
  {
    "objectID": "5-prediction/01-method-of-least-squares/slides.html#agenda",
    "href": "5-prediction/01-method-of-least-squares/slides.html#agenda",
    "title": "Method of Least Squares",
    "section": "Agenda",
    "text": "Agenda\n\nAnnouncements\nConcept Questions\nProblem Set 16: The Method of Least Squares\nLab 5.1: Understanding the Context of the Data"
  },
  {
    "objectID": "5-prediction/01-method-of-least-squares/slides.html#announcements",
    "href": "5-prediction/01-method-of-least-squares/slides.html#announcements",
    "title": "Method of Least Squares",
    "section": "Announcements",
    "text": "Announcements\n\nProblem Sets:\n\nPS 16 released Tuesday and due next Tuesday at 9am\nExtra Practice released Thursday (non-turn in)\n\n\n\n\nLab 5:\n\nLab 5.1 released Tuesday and due next Tuesday at 9am\nLab 5.2 released Thursday and due next Tuesday at 9am\nLab 5 Workshop next Monday"
  },
  {
    "objectID": "5-prediction/01-method-of-least-squares/slides.html#concept-question-1",
    "href": "5-prediction/01-method-of-least-squares/slides.html#concept-question-1",
    "title": "Method of Least Squares",
    "section": "Concept Question 1",
    "text": "Concept Question 1\nAn engineer working for Waymo self-driving cars is working to solve a problem. When it rains, reflections of other cars in puddles can disorient the self-driving car. Their team is working on a model to determine when the self-driving car is seeing a reflection of a car vs a real car.\n\nThink of a potential response and predictor, and about whether this is a regression or classification problem.\n\n\n\n\n−+\n01:00\n\n\n\n\nThis is a serious challenge encountered by self-driving cars at the moment. This is probably best thought of as a classification problem, with the response being either reflection / not or puddle / not. The predictors could be sensor input (cameras, lidar) as well as weather info (to know if it’s been raining)."
  },
  {
    "objectID": "5-prediction/01-method-of-least-squares/slides.html#concept-question-2",
    "href": "5-prediction/01-method-of-least-squares/slides.html#concept-question-2",
    "title": "Method of Least Squares",
    "section": "Concept Question 2",
    "text": "Concept Question 2\n\nHere is a function f.\n\n\nf &lt;- function(x, y) {\n  y*(x + 3) \n}\n\n\n\nWhat will the following line of code return?\n\n\n\n\nf(3,5)\n\n\nThis question reminds students that if function arguments are unnamed, they are assumed to have been input in the order they were originally specified as in the function definition. x will be assigned 3, and y will be assigned 5, so the correct answer is 30."
  },
  {
    "objectID": "5-prediction/labs/08-cancer/slides.html#fine-needle-aspiration-biopsy",
    "href": "5-prediction/labs/08-cancer/slides.html#fine-needle-aspiration-biopsy",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Fine needle aspiration biopsy",
    "text": "Fine needle aspiration biopsy\n\n\nOften when someone is suspected to have cancer (e.g. a bump is found) a fine needle aspiration biopsy is taken to determine whether or not the growth is cancerous (malignant – may grow dangerously out of control) or not (benign)."
  },
  {
    "objectID": "5-prediction/labs/08-cancer/slides.html#artificial-intelligence-in-medicine",
    "href": "5-prediction/labs/08-cancer/slides.html#artificial-intelligence-in-medicine",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Artificial intelligence in medicine",
    "text": "Artificial intelligence in medicine\n\nAutomating certain diagnostic tasks can increase access to healthcare\nGlobal shortage of pathologists, especially outside of wealthy healthcare systems\n\nExpert pathologists take years to be fully trained (4 year medical school + 4 year residency)\n\n\n\nhttps://www.linkedin.com/pulse/how-ai-can-help-address-global-shortage-pathologists-colangelo/"
  },
  {
    "objectID": "5-prediction/labs/08-cancer/slides.html#lab-6-breast-cancer-diagnosis",
    "href": "5-prediction/labs/08-cancer/slides.html#lab-6-breast-cancer-diagnosis",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Lab 6: breast cancer diagnosis",
    "text": "Lab 6: breast cancer diagnosis\n\nSamples are 568 biopsies\n\nEach biopsy has 30 features\n\nGoal: classify biopsy as benign or malignant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe tissue is purple because it is stained with Hematoxylin and Eosin. These are purple and pink stains that make the tissue structure easier to see under a microscope.\nMost of the visible objects in these images are cell nuclei, not the full cell; the cytoplasm is mostly invisible on these images.\n(From https://stanfordhealthcare.org/medical-conditions/cancer/cancer.html): Tumors can be benign (noncancerous) or malignant (cancerous). Benign tumors tend to grow slowly and do not spread. Malignant tumors can grow rapidly, invade and destroy nearby normal tissues, and spread throughout the body."
  },
  {
    "objectID": "5-prediction/labs/08-cancer/slides.html#nuclear-morphology",
    "href": "5-prediction/labs/08-cancer/slides.html#nuclear-morphology",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Nuclear morphology",
    "text": "Nuclear morphology\n\nMorphology = what the cell looks like under a microscope\n\nsize, shape, texture\n\nCells in malignant biopsies tend to\n\nbe larger\nirregularly shaped\nhighly variable\n\nOnly measure morphology of cell nucleus"
  },
  {
    "objectID": "5-prediction/labs/08-cancer/slides.html#nuclear-morphology-features",
    "href": "5-prediction/labs/08-cancer/slides.html#nuclear-morphology-features",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "10 nuclear morphology features",
    "text": "10 nuclear morphology features\n\n\n\n\n\n\nWe compute 10 morphological features for each cell nucleus in the biopsy image.\nIt may be helpful to first draw one unit (one glass slide, that you would see under the microscope). Then in this slide, you make a data frame with cells, and then group by-summarise. Then the resulting calculations become one row in the final dataset."
  },
  {
    "objectID": "5-prediction/labs/08-cancer/slides.html#biopsy-features",
    "href": "5-prediction/labs/08-cancer/slides.html#biopsy-features",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "30 biopsy features",
    "text": "30 biopsy features\n\n\n\n\n\n\nEach biopsy has 30 features; these come from computing 3 summary statistics (mean, max, standard deviation) of each of the 10 cell features to summarize the population of cells in the biopsy."
  },
  {
    "objectID": "5-prediction/labs/08-cancer/slides.html#lab-worktime",
    "href": "5-prediction/labs/08-cancer/slides.html#lab-worktime",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Lab worktime",
    "text": "Lab worktime\n\n\n\n−+\n25:00"
  },
  {
    "objectID": "5-prediction/labs/07-baseball/lab.html",
    "href": "5-prediction/labs/07-baseball/lab.html",
    "title": "Lab 5: Baseball",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "5-prediction/labs/07-baseball/lab.html#part-i-understanding-the-context-of-the-data",
    "href": "5-prediction/labs/07-baseball/lab.html#part-i-understanding-the-context-of-the-data",
    "title": "Lab 5: Baseball",
    "section": "Part I: Understanding the Context of the Data",
    "text": "Part I: Understanding the Context of the Data\n\nLab 5.1: Baseball"
  },
  {
    "objectID": "5-prediction/labs/07-baseball/lab.html#part-ii-computing-on-the-data",
    "href": "5-prediction/labs/07-baseball/lab.html#part-ii-computing-on-the-data",
    "title": "Lab 5: Baseball",
    "section": "Part II: Computing on the Data",
    "text": "Part II: Computing on the Data\nThe goal of this lab is to build three different regression models to predict the number of wins of a Major League Baseball team.\nUse the following code to load in the Teams dataset from the Lahman database (library). Recall that you can query the help file for a data set by running ?Teams at the console.\n\nSubset the Teams data set to only include years from 2000 to present day (this is the data set that you’ll use for the remainder of this lab. There might be another year post-2000 that you might want to filter out: why?). What are the dimensions of this filtered data set?\nPlot the distribution of wins. Describe the shape of the distribution and compare it to your speculations from part 1 of the lab.\nPlot the relationship between runs and wins. Describe the relationship (form, direction, strength of association, presence of outliers) and compare it to your speculations from part 1 of the lab.\nPlot the relationship between runs allowed and wins. Describe the relationship. How does it compare to the relationship between runs and wins?\nFit a simple linear model to predict wins by runs and call it model_1. Write out the equation for the linear model (using the estimated coefficients) and interpret the \\(R^2\\) in the context of the problem in at least one sentence.\nWhat is the average number of season runs and wins? Based on the previous model, how many games would you predict a team that scored the average number of runs would win?\nWhat about a team that scored 600 runs? What about 850 runs? What about 10,000 runs? Would any of these predictions be inaccurate and why?\nFit a multiple linear regression model to predict wins by runs and runs allowed (RA) and save it as model_2. Write out the equation for the linear model and report the \\(R^2\\). How does this model compare to the simple linear regression from the previous question?\nFit a third, more complex model to predict wins and call it model_3. This model should use\n\nat least three variables from this data set,\nat least one non-linear transformation or polynomial term.\n\nWrite out the equation for the resulting linear model and report the \\(R^2\\).\nRevisit the definition of causation. If your predictive model has a positive coefficient between one of the predictors and the response, is that evidence that if you increase that predictor variable for a given observation, the response variable will increase? That is, can you (or a sports management team) use this model to draw causal conclusions? Why or why not? Answer in at least three sentences."
  },
  {
    "objectID": "assets/final-exam-slide.html#section",
    "href": "assets/final-exam-slide.html#section",
    "title": "Final Exam",
    "section": "",
    "text": "Before the exam begins\n\n🧹 Clear your desk of everything except a pen/pencil, your cheat sheet, and your ID.\n🧢 Remove hats, hoods, and sunglasses.\n📱 All electronic devices (laptop, phone, airbuds, smart watch) should be remain in your bag for the entire class period. All items should be on the floor not on nearby seats.\nSpread out as much as possible. We may reseat you to space people out.\nWhen you get an exam, do not begin.\n\n\nDuring the exam\n\n✅ All answers must be marked on the answer sheet.\n👀 If a proctor sees your eyes wandering to other students’ work, you will be reseated. If it happens again, it is considered academic misconduct.\nIf you need to use the bathroom, bring phone and exam materials to the front of the class.\n⌛ When you are done with this exam, please bring your exam, answer sheet, and cheat sheet to a proctor at the front of the class.\n\n\nGood luck! 🍀\n\n\n\n−+\n90:00\n\n\n\n\n\n\n−+\n90:00"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Prerequisites\nInstall Docker.\nIf you are running on a computer with Apple Silicon (using a new “M” processor), - Update macOS to Venture 13 or newer. - Install Apple’s Rosetta emulation by running softwareupdate --install-rosetta. - In Docker Desktop, enable Settings &gt; Features in development &gt; Use Rosetta for x86/amd64 emulation on Apple Silicon.\nIf you are running Microsoft Windows, - Install WSL. - In a Command or Power Shell window, run wsl --install -d Ubuntu. - In an Ubuntu window, run apt update, then apt install make. - Run exec ssh-agent bash to start your SSH agent.\nCheckout this repository into a new working directory.\n\n\nRun the Container\nIn a terminal window, change into the working directory and run make up. This initializes your environment by creating the file .env. It then runs docker compose.\nWhen your environment changes, for example if you log out and back in, or if you manually copy your working directory to another platform, run make clean or manually delete the .env file. Then run make up.\n\n\nBuild a Custom Image (WIP)\nThe container is normally built automatically by a CI process on GitHub Actions. If you need to alter the image, for example if you want to locally test a new library addition, you can do the following:\n\nCheckout the https://github.com/stat20/stat20-docker repository.\nMake any changes\nRun docker build -t somename:sometag .. If on an alternative hardware platform like Apple’s ARM (modern Apple computers), you can run docker build -t somename:sometag --platform linux/amd64 ..\nCreate a new file names docker-compose.override.yml with the following contents:\n\nversion: \"3.8\"\nservices:\n  stat20:\n    image: somename:sometag"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html",
    "title": "The Taxonomy of Data",
    "section": "",
    "text": "The concepts of a variable, its type, and the structure of a data frame are useful because they help guide our thinking about the nature of a data. But we need more than definitions. If our goal is to construct a claim with data, we need a tool to aid in the construction. Our tool must be able to do two things: it must be able to store the data and it must be able to perform computations on the data. This is where R comes in!\nFirst, we will discuss how R can store and perform computations on data. Then, we will relate these basics to the Taxonomy of Data we have just discussed.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#r-and-rstudio",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#r-and-rstudio",
    "title": "The Taxonomy of Data",
    "section": "R and RStudio",
    "text": "R and RStudio\nR is one of the most powerful languages for doing statistics and data science. One of the reasons for its power and popularity is that it is both free and open-source. This turns languages like R into something that resembles Wikipedia: a collaborative effort that is constantly evolving. Extensions to the R language have been authored by professional programmers1, people working in industry and government2, professors3, and students like you4.\nYou’ll be writing and running code through an app called RStudio. Beyond writing R code, RStudio allows you to manage your files and author polished documents that weave together code and text. RStudio can be run through a browser and we have set up an account for you that you can access by sending a browser tab to https://stat20.datahub.berkeley.edu/ or clicking the  link in the upper right corner of the course website.\nWhen you log into RStudio, the place where you can type and run R code is called the console and it’s located right here:\n\n\n\n\n\n\nFigure 1: The R console in RStudio.\n\n\n\n\n\n\n\n\n\nCode along\n\n\n\nAs you read through these notes, keep RStudio open in another window to code along at the console.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#r-as-a-calculator",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#r-as-a-calculator",
    "title": "The Taxonomy of Data",
    "section": "R as a Calculator",
    "text": "R as a Calculator\nAlthough R is capable of running sophisticated statistical models, it’s also more than able to act as a calculator. Type the sum 1 + 2 into the console (the area to the right of the &gt;) and press Enter. What you should see is this:\n\n1 + 2\n\n[1] 3\n\n\nAll of the arithmetic operations work in R.\n\n1 - 2\n\n[1] -1\n\n1 * 2\n\n[1] 2\n\n1 / 2\n\n[1] 0.5\n\n\nEach of these four lines of code is called a command and the response from R is the output. The [1] at the beginning of the output is there just to indicate that it is the first element of the output. This helps you keep track of things when the output spans many lines.\nAlthough it is easiest to read code when the numbers are separated from the operator by a single space, it’s not necessary. R ignores all spaces when it runs your code, so each of the following also work.\n\n1/2\n\n[1] 0.5\n\n1   /         2\n\n[1] 0.5\n\n\nYou can add exponents by using ^, but don’t forget about the order of operations. If you want an alternative ordering, use parentheses.\n\n2 ^ 3 + 1\n\n[1] 9\n\n2 ^ (3 + 1)\n\n[1] 16",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#saving-objects",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#saving-objects",
    "title": "The Taxonomy of Data",
    "section": "Saving Objects",
    "text": "Saving Objects\nWhenever you want to save the output of an R command, add an assignment arrow &lt;- (less than, minus) as well as a name, such as “answer” to the left of the command.\n\nanswer &lt;- 2 ^ (3 + 1)\n\nWhen you run this command, there are two things to notice.\n\nThe word answer appears in the upper right hand corner of RStudio, in the “Environment” tab.\nNo output is returned at the console.\n\nEvery time you run a command, you can ask yourself: do I want to just see the output at the console or do I want to save it for later? If the latter, you can always see the contents of what you saved by just typing its name at the console and pressing Enter.\n\nanswer\n\n[1] 16\n\n\nThere are a few rules around the names that R will allow for the objects that you’re saving. First, while all letters are fair game, special characters like +, -, /, !, $, are off-limits. Second, names can contain numbers, but not as the first character. That means names like answer, a, a12, my_pony, and FOO will all work. 12a and my_pony! will not.\nBut just because I’ve told you that those names won’t work doesn’t mean you shouldn’t give it a try…\n\nmy_pony! &lt;- 2 ^ (3 + 1)\n\nError in parse(text = input): &lt;text&gt;:1:8: unexpected '!'\n1: my_pony!\n           ^\n\n\nThis is an example of an error message and, though they can be alarming, they’re also helpful in coaching you how to correct your code. Here, it’s telling you that you had an “unexpected !” and then it points out where in your code that character popped up.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#creating-vectors",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#creating-vectors",
    "title": "The Taxonomy of Data",
    "section": "Creating Vectors",
    "text": "Creating Vectors\nWhile it is helpful to be able to store a single number as an R object, to store data sets we’ll need to store a series of numbers. You can combine multiple values by putting them inside c() separated by commas.\n\nmy_fav_numbers &lt;- c(9, 11, 19, 28)\nmy_fav_numbers\n\n[1]  9 11 19 28\n\n\nThis is object is called a vector.\n\nVector (in R)\n\nA set of contiguous data values that are of the same type.\n\n\nAs the definition suggests, you can create vectors out of many different types of data. To store words as data, use the following:\n\nmy_fav_colors &lt;- c(\"green\", \"orange\", \"purple\")\nmy_fav_colors\n\n[1] \"green\"  \"orange\" \"purple\"\n\n\nAs this example shows, R can store more than just numbers as data. \"green\", \"orange“, and \"purple\" are each called character strings and when combined together with c() they form a character vector. You can identify a string because it is wrapped in quotation marks and gets highlighted a different color in RStudio.\nVectors are often called atomic vectors because, like atoms, they are the simplest building blocks in the R language. Most of the objects in R are, at the end of the day, constructed from a series of vectors.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#functions",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#functions",
    "title": "The Taxonomy of Data",
    "section": "Functions",
    "text": "Functions\nWhile the vector will serve as our atomic method of storing data in R, how do we perform computations on it? That is the role of functions.\nLet’s use a function to find the arithmetic mean of the vector my_fav_numbers.\n\nmean(my_fav_numbers)\n\n[1] 16.75\n\n\nA function in R operates in a very similar manner to functions that you’re familiar with from mathematics.\n\n\n\n\n\n\nFigure 2: A mathematical function as a box with inputs and outputs.\n\n\n\nIn math, you can think of a function, \\(f()\\) as a black box that takes the input, \\(x\\), and transforms it to the output, \\(y\\). You can think of R functions in a very similar way. For our example above, we have:\n\nInput: the vector of four numbers that serves as the input to the function, my_fav_numbers.\nFunction: the function name, mean, followed by parentheses.\nOutput: the number 16.75.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#functions-on-vectors",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#functions-on-vectors",
    "title": "The Taxonomy of Data",
    "section": "Functions on Vectors",
    "text": "Functions on Vectors\nmean() is just one of thousands of different functions that are available in R. Most of them are sensibly named, like the following, which compute square roots and natural logarithms.\n\n\nBy default, log() computes the natural log. To use other bases, see ?log.\n\nsqrt(my_fav_numbers)\n\n[1] 3.000000 3.316625 4.358899 5.291503\n\nlog(my_fav_numbers)\n\n[1] 2.197225 2.397895 2.944439 3.332205\n\n\nNote that with these two functions, the input was a vector of length four and the output is a vector of length four. This is a distinctive aspect of the R language and it is helpful because it allows you to perform many separate operations (taking the square root of four numbers, one by one) with just a single command.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#the-taxonomy-of-data-in-r",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#the-taxonomy-of-data-in-r",
    "title": "The Taxonomy of Data",
    "section": "The Taxonomy of Data in R",
    "text": "The Taxonomy of Data in R\nIn the last lecture notes, we introduced the Taxonomy of Data as a broad system to classify the different types of variables on which we can collect data. If you recall, a variable is a characteristic of an object that you can measure and record. When Dr. Gorman walked up to her first penguin (the unit of observation) and measured its bill length, she collected a single observation of the variable bill_length_mm. You could record that in R using,\n\nbill_length_mm &lt;- 50.7\n\nShe continued on to measure the next penguin, then the next, then the next… Instead of recording these as separate objects, it is more efficient to store them as a vector.\n\nbill_length_mm &lt;- c(50.7, 48.5, 52.8, 44.5, 42.0, 46.9, 50.2, 37.9)\n\nThis example shows that\n\nA vector in R is a natural way to store observations on a variable.\n\nso in the same way that we have asked, “what is the type of that variable?” we can now ask “what is the class of that variable in R?”.\n\nClass (R)\n\nA collection of objects, often vectors, that share similar attributes and behaviors.\n\n\nWhile there are many classes in R, you can get a long way only knowing three. The first is represented by our vector my_fav_numbers. Let’s check it’s class using the class() function.\n\nclass(my_fav_numbers)\n\n[1] \"numeric\"\n\n\nHere we learn that my_fav_numbers is a numeric vector. Numeric vectors, as the name suggests, are composed only of numbers and can include measurements from both discrete and continuous numerical variables.\nWhat about my_fav_colors?\n\nclass(my_fav_colors)\n\n[1] \"character\"\n\n\nR stores that as a character vector. This is a very flexible class that can be used to store text as data. But what if there are only a few fixed values that a variable can take? In that case, you can do better than a character vector by usinggit a factor. Factor is a very useful class in R because it encodes the notion of levels discussed in the last notes.\nTo illustrate the difference, let’s make a character vector but then enrich it by turning it into a factor using factor().\n\nchar_vec &lt;- c(\"cat\", \"cat\", \"dog\")\nfac &lt;- factor(char_vec)\nchar_vec\n\n[1] \"cat\" \"cat\" \"dog\"\n\nfac\n\n[1] cat cat dog\nLevels: cat dog\n\n\nThe original character vector stores the same three strings that we used as input. The factor adds some additional information: the possible values that this vector can take.\nThis is particularly useful when you want to let R know that these levels have a natural ordering. If you have strong opinions about the relative merit of dogs over cats, you could specify that using:\n\nordered_fac &lt;- factor(char_vec, levels = c(\"dog\", \"cat\"))\nordered_fac\n\n[1] cat cat dog\nLevels: dog cat\n\n\n\n\nThis example also demonstrates that you can create a (character) vector inside a function.\nWhile this doesn’t change the way the levels are ordered in the vector itself, it will effect the way they behave when we use them to create plots, as we’ll do in the next set of notes.\nThese three vector classes do a good job of putting into flesh and bone (or at least silicon) the abstract types captured in the Taxonomy of Data.\n\n\n\n\n\n\nFigure 3: The Taxonomy of Data with equivalent classes in R.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#data-frames-in-r",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#data-frames-in-r",
    "title": "The Taxonomy of Data",
    "section": "Data Frames in R",
    "text": "Data Frames in R\nWhile vectors in R do a great job of capturing the notion of a variable, we will need more than that if we’re going to represent something like a data frame. Conveniently enough, R has a structure well-suited to this task called…(drumroll…)\n\nDataframe (R)\n\nA two dimensional data structure used to store vectors of the same length. A direct analog of the data frame defined previously5.\n\n\nLet’s use R to recreate the penguins data frame collected by Dr. Gorman.\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nspecies\n\n\n\n\n43.5\n18.1\nChinstrap\n\n\n48.1\n15.1\nGentoo\n\n\n49.0\n19.5\nChinstrap\n\n\n45.4\n18.7\nChinstrap\n\n\n34.6\n21.1\nAdelie\n\n\n49.8\n17.3\nChinstrap\n\n\n40.9\n18.9\nAdelie\n\n\n45.3\n13.7\nGentoo\n\n\n\n\n\n\n\n\nCreating a data frame\nIn the data frame above, there are three variables; the first two numeric continuous, the last one categorical nominal. Since R stores variables as vectors, we’ll need to create three vectors.\n\nbill_length_mm &lt;- c(50.7, 48.5, 52.8, 44.5, 42.0, 46.9, 50.2, 37.9)\nbill_depth_mm &lt;- c(19.7, 15.0, 20.0, 15.7, 20.2, 16.6, 18.7, 18.6)\nspecies &lt;- factor(c(\"Chinstrap\", \"Gentoo\", \"Chinstrap\", \"Gentoo\", \"Adelie\", \n             \"Chinstrap\", \"Chinstrap\", \"Adelie\"))\n\nWhile bill_length_mm and bill_depth_mm are both being stored as numeric vectors, species was first collected into a character vector, then passed directly to the factor() function. This is an example of nesting one function inside of another and it combined two lines of code into one.\nWith the three vectors stored in the Environment, all you need to do is staple them together with data.frame().\n\npenguins_df &lt;- data.frame(bill_length_mm, bill_depth_mm, species)\npenguins_df\n\n  bill_length_mm bill_depth_mm   species\n1           50.7          19.7 Chinstrap\n2           48.5          15.0    Gentoo\n3           52.8          20.0 Chinstrap\n4           44.5          15.7    Gentoo\n5           42.0          20.2    Adelie\n6           46.9          16.6 Chinstrap\n7           50.2          18.7 Chinstrap\n8           37.9          18.6    Adelie",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#summary",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#summary",
    "title": "The Taxonomy of Data",
    "section": "Summary",
    "text": "Summary\nThis was our first introduction to R, a supercharged calculator for storing and computing on data. We learned how to do basic arithmetic, construct and save a vector, call functions, query the class of an object, and construct a data frame. This forms the foundation of our use of R. If that foundation feels shakey, don’t fret. We’ll get plenty of practice in class.\n\n\n\n\n\n\nFigure 4: The arc of learning R6.",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#exercises",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#exercises",
    "title": "The Taxonomy of Data",
    "section": "Exercises",
    "text": "Exercises\n\nEx: R as a Calculator\nMeet Leia, a fictitious undergrad student taking Stat 20. Leia loves to drink coffee in the morning, and she brews her own coffee at home. She even has a monthly budget of $20 to cover this type of expense. As you know, we can use R to create an object or variable coffee for Leia’s budget:\n\ncoffee &lt;- 20\n\nAlternatively, you can also use the equals sign = as an assignment operator:\n\ncoffee = 20\n\n\n\nYour Turn: Leia’s Expenses\nConsider the bills of Leia’s fixed monthly expenses:\n\nphone $80\ntransportation $20\ngroceries $600\nrent $1800\n\n\nMake more assignments to create variables phone, transportation, groceries, and rent with their corresponding amounts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nphone &lt;- 80\ntransportation &lt;- 20\ngroceries &lt;- 600\nrent &lt;- 1800\n\n\n\n\n/\n\nNow that you have all the variables, create a total object with the sum of her fixed monthly expenses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ntotal &lt;- phone + transportation + groceries + rent\ntotal\n\n\n\n\n/\n\nAssuming that Leia has the same expenses every month, how much would she spend during a school “semester”? (assume the semester involves five months).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ntotal * 5\n\n\n\n\n/\n\nMaintaining the same assumption about the monthly expenses, how much would Leia spend during a school “year”? (assume the academic year is 10 months).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ntotal * 10\n\n\n\n\n/\n\n\nEx: Taxonomy of Data and R Vectors\nFrom the taxonomy of data, you know that we have 4 flavors of variables, and their corresponding classes in R (shown below inside parenthesis) illustrated in the following examples:\n\n# continuous (numeric)\nx1 &lt;- c(1.2, 3.3, -0.5)\n\n# discrete (numeric)\nx2 &lt;- c(2, 4, 6)\n\n# ordinal (ordered factor)\nx3 &lt;- factor(c(\"sm\", \"md\", \"lg\", \"sm\"), levels = c(\"sm\", \"md\", \"lg\"))\n\n# nominal (character or factor)\nx4 &lt;- c(\"strawberry\", \"lemon\", \"vanilla\")\nx4bis &lt;- factor(c(\"strawberry\", \"lemon\", \"vanilla\"))\n\n\n\nYour Turn: Terrestrial Planets\nConsider the following data set—shown in the table below—containing variables of so-called Terrestrial planets. These planets include Mercury, Venus, Earth, and Mars. They are called like this because they are “Earth-like” planets: relatively small in size and in mass, with a solid rocky surface, and metals deep in its interior.\n\n\n\nname\ngravity\nmoons\n\n\n\n\nMercury\n3.7\n0\n\n\nVenus\n8.9\n0\n\n\nEarth\n9.8\n1\n\n\nMars\n3.7\n2\n\n\n\n/\n\nConsider the column name in the provided table of terrestrial planets. Use the c() function to create a character vector name containing the names of the Terrestrial planets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nname = c(\"Mercury\", \"Venus\", \"Earth\", \"Mars\")\n\n\n\n\n/\n\nConsider the column gravity in the provided table of terrestrial planets. Use the combine function c() to make a numeric vector gravity for the Terrestrial planets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngravity = c(3.7, 8.9, 9.8, 3.7)\n\n\n\n\n/\n\nConsider the column moons in the provided table of terrestrial planets. Use the combine function c() to make an ordinal factor moons.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nmoons = factor(c(0, 0, 1, 2), ordered = TRUE)\n\n\n\n\n/\n\n\nEx: Data Frames in R\nConsider again the data set of Terrestrial planets—shown in the table below.\n\n\n\nname\ngravity\nmoons\n\n\n\n\nMercury\n3.7\n0\n\n\nVenus\n8.9\n0\n\n\nEarth\n9.8\n1\n\n\nMars\n3.7\n2\n\n\n\n/\nUse the vectors that you defined in the previous section in order to create a data frame planets:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nplanets = data.frame(\n  \"name\" = name,\n  \"gravity\" = gravity,\n  \"moons\" = moons,\n  \"haswater\" = haswater\n)\n\n\n\n\n/\n\n\nEx: Challenge\nLet’s apply everything that you’ve learned so far in order to create a data frame students containing the following data, and the provided specifications listed below:\n\n\n\nname\nheight\nyear\nresident\n\n\n\n\nLeia\n160\nsophomore\nTRUE\n\n\nLuke\n170\nfreshman\nFALSE\n\n\nHan\n182\nsenior\nTRUE\n\n\nLando\n178\njunior\nFALSE\n\n\n\n/\n\nname: nominal variable (character)\nheight continuous variable (numeric)\nyear: ordinal variable (ordered factor)\nresident: nominal variable (logical)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nstudents = data.frame(\n  \"name\" = c(\"Leia\", \"Luke\", \"Han\", \"Lando\"),\n  \"height\" = c(160, 170, 182, 178),\n  \"year\" = factor(x = c(\"sophomore\", \"freshman\", \"senior\", \"junior\"),\n                  levels = c(\"freshman\", \"sophomore\", \"junior\", \"senior\")),\n  \"resident\" = c(TRUE, FALSE, TRUE, FALSE)\n)\n\n\n\n\n\n\nReferences and further reading\n\nHands on Programming with R by Garret Grolemund. A friendly introduction to the R language with fun examples.\nThe official (somewhat dense) documentation fo the R language. https://cran.r-project.org/doc/manuals/r-release/R-lang.html\nR for Data Science by Hadley Wickham and Garrett Grolemund. A comprehensive but approachable guide to doing data science with R. A good reference once you’re deeper into this course..",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#footnotes",
    "href": "1-questions-and-data/02-taxonomy-of-data/tutorial.html#footnotes",
    "title": "The Taxonomy of Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe googlesheets4 package, which reads spreadsheet data into R was authored by Jenny Bryan, a developer at Posit: :https://googlesheets4.tidyverse.org/.↩︎\nThe statistics office of the province of British Columbia maintains a public R package with all of their data: https://bcgov.github.io/bcdata/↩︎\nDr. Christopher Paciorek in the Department of Statistics at UC Berkeley maintains a package to fit a very broad class of statistical models called Bayesian Models: https://r-nimble.org/.↩︎\nSimon Couch wrote the stacks package for model ensembling while an undergraduate https://stacks.tidymodels.org/index.html.↩︎\nR is an unusual language in that the data frame has been for decades a core structure of the language. The analogous structure in Python is the data frame found in the Pandas library.↩︎\nR monster artwork by @allison_horst.↩︎",
    "crumbs": [
      "Tutorials",
      "R Tutorials",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/notes.html",
    "href": "1-questions-and-data/01-understanding-the-world/notes.html",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Welcome to Stat 20! We are very excited to have you here this semester. There are no reading questions for today’s content, but make sure that you have:\n\ngotten the name of your instructor and in-class tutors\nread the syllabus and asked any questions you have about it on your lecture’s corresponding Ed thread\nstarted the first lab assignment\n\nThe goal of our course is to construct and critique claims made using data. This raises the question: what type of claims can be made?",
    "crumbs": [
      "Notes",
      "Questions and Data",
      "Understanding the World with Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/notes.html#intro-and-syllabus",
    "href": "1-questions-and-data/01-understanding-the-world/notes.html#intro-and-syllabus",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Welcome to Stat 20! We are very excited to have you here this semester. There are no reading questions for today’s content, but make sure that you have:\n\ngotten the name of your instructor and in-class tutors\nread the syllabus and asked any questions you have about it on your lecture’s corresponding Ed thread\nstarted the first lab assignment\n\nThe goal of our course is to construct and critique claims made using data. This raises the question: what type of claims can be made?",
    "crumbs": [
      "Notes",
      "Questions and Data",
      "Understanding the World with Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/notes.html#types-of-claims",
    "href": "1-questions-and-data/01-understanding-the-world/notes.html#types-of-claims",
    "title": "Understanding the World with Data",
    "section": "Types of Claims",
    "text": "Types of Claims\n\n\n\n\n\n\n\nSummary\n\nA numerical, graphical, or verbal description of an aspect of data that is on hand.\n\n\n\nExample: Using data from the Stat 20 class survey, the proportion of respondents to the survey who reported having no experience writing computer code is 70%.\n\n\nGeneralization\n\nA numerical, graphical, or verbal description of a broader set of units than those on which data was been recorded.\n\n\n\n\nExample: Using data from the Stat 20 class survey, the proportion of Berkeley students who have no experience writing computer code is 70%.\n\n\nCausal Claim\n\nA claim that changing the value of one variable will influence the value of another variable.\n\n\n\nExample: Data from a randomized controlled experiment shows that taking a new antibiotic eliminates more than 99% of bacterial infections.\n\n\nPrediction\n\nA guess about the value of an unknown variable, based on other known variables.\n\n\n\nExample: Based on reading the news and the price of Uber’s stock today, I predict that Uber’s stock price will go up 1.2% tomorrow.",
    "crumbs": [
      "Notes",
      "Questions and Data",
      "Understanding the World with Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#agenda",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#agenda",
    "title": "Understanding the World with Data",
    "section": "Agenda",
    "text": "Agenda\n\nIntroductions\nThe Data Science Lifecycle\nTypes of Claims with Practice\nCourse Structure and Syllabus\nIntro to R and RStudio\nLooking forward"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "What’s going on here?"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-2",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-2",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "As a group, formulate at least three possible explanations for what’s going on in the picture.\n\n\n\n\n  \n    −\n    +\n \n 03:00"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-4",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-4",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "These three photos were taken in quick succession by a physician and amateur photographer who was vising the San Diego Zoo Safari Park. He was watching the shoebill as it was walking down a path in the reeds. As the shoebill was ambling along, it encountered a duck in the middle of the path. It leaned down, picked up the duck in its beak, turned to the side, dropped the duck in the reeds, then proceeded to amble down the path."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-5",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-5",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Understand\nthe World\n\n\n\nData"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-6",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-6",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Understand\nthe World\n\n\n\nData"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#takeaways-from-this-exercise",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#takeaways-from-this-exercise",
    "title": "Understanding the World with Data",
    "section": "Takeaways from this exercise",
    "text": "Takeaways from this exercise\nWe can call the process of:\n\nhaving a question,\n\n\n\nfinding data to investigate that question,\n\n\n\n\nreaching a conclusion,\n\n\n\n\nand then thinking of a next step which starts everything over again\n\n\n\n\nthe data science lifecycle.\n\nThis lifecycle involves constructing and critiquing claims made using data: which is the main goal of our course!"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#course-goal",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#course-goal",
    "title": "Understanding the World with Data",
    "section": "Course Goal",
    "text": "Course Goal\n\n\nTo learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-7",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-7",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-8",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-8",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-9",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-9",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-10",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-10",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-11",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-11",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-12",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-12",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-13",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-13",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "A numerical, graphical, or verbal description of an aspect of data that is on hand.\n\n\n\n\n\n\nExample\nUsing data from the Stat 20 class survey, the proportion of respondents to the survey who reported having no experience writing computer code is 70%."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-14",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-14",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "A numerical, graphical, or verbal description of a broader set of units than those on which data was been recorded.\n\n\n\n\n\n\n\nExample\nUsing data from the Stat 20 class survey, the proportion of Berkeley students who have no experience writing computer code is 70%."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-15",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-15",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "A claim that changing the value of one variable will influence the value of another variable.\n\n\n\n\n\n\nExample\nData from a randomized controlled experiment shows that taking a new antibiotic eliminates more than 99% of bacterial infections."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-16",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-16",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "A guess about the value of an unknown variable, based on other known variables.\n\n\n\n\n\n\nExample\nBased on reading the news and the price of Uber’s stock today, I predict that Uber’s stock price will go up 1.2% tomorrow."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#practice-concept-questions-1",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#practice-concept-questions-1",
    "title": "Understanding the World with Data",
    "section": "Practice Concept Questions",
    "text": "Practice Concept Questions\nWe will now re-examine a few pathways in the data science lifecycle:\n\nForming a question -&gt; collecting data\n\n\n\nCollecting data -&gt; making a claim"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#from-questions-to-data",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#from-questions-to-data",
    "title": "Understanding the World with Data",
    "section": "From Questions to Data",
    "text": "From Questions to Data\n\n\nIs the incidence of COVID on campus going up or down?\n\n\n\n\nDiscuss:\nA. What type of data can help answer this question? Consider\n\nWhich different people / institutions collect relevant data\nIs certain data not available? Why not?\n\nB. Will this question be answered by a summary, a prediction, a generalization, or a causal claim?\n\n\n  \n    −\n    +\n \n 06:00"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#from-data-to-claims",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#from-data-to-claims",
    "title": "Understanding the World with Data",
    "section": "From Data to Claims",
    "text": "From Data to Claims\nOne source of data:\n\n\n\n\n\n“The following dashboard provides information on COVID-19 testing performed at University Health Services or through the PCR Home Test Vending Machines on campus. It does not capture self-reported positive tests. It provides a look at new cases and trends, at a glance.”"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-17",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-17",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Formulate one claim that is supported by this data1.\n\n\n  \n    −\n    +\n \n 03:00\n \nThe positivity rate is the number of positive tests over the total number of tests."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-23",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-23",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Read lecture notes\nWork through reading questions\n\n\n\n\n\nWork through concept questions solo / in groups / as a class\nMake progress on assignments"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-24",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-24",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "All of the materials and links for the course can be found at:\nhttps://stat20.berkeley.edu/fall-2024"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#ed-discussion-forum",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#ed-discussion-forum",
    "title": "Understanding the World with Data",
    "section": "Ed Discussion Forum",
    "text": "Ed Discussion Forum\n\n\n\n\n\nForum to ask questions, answer questions, and course announcements\nPlease answer each other’s questions!\n\n\nPractice by asking/answering a question on the “Syllabus Discussion” thread on Ed via the link on the course website."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#load-the-lab-1-template-into-rstudio",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#load-the-lab-1-template-into-rstudio",
    "title": "Understanding the World with Data",
    "section": "Load the Lab 1 Template into RStudio",
    "text": "Load the Lab 1 Template into RStudio\nClick the link below…\n\n\n\n\nLoad Lab Templates into RStudio"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#general-lab-workflow",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#general-lab-workflow",
    "title": "Understanding the World with Data",
    "section": "General Lab Workflow",
    "text": "General Lab Workflow\n\nLab Questions will be posted to the course website.\nYou’ll author your Lab Reports as Quarto Documents that blend text and code. They should contain only answers.\nRender your .qmd file to a .pdf file then download that file from RStudio to your computer.\nGo to Gradescope and upload your .pdf lab report, being sure to assign questions to the pages."
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Documentation",
    "section": "",
    "text": "Welcome to the technical documentation of the Stat 20 curriculum and website."
  },
  {
    "objectID": "docs/website.html",
    "href": "docs/website.html",
    "title": "Configuring this Website",
    "section": "",
    "text": "One repo to rule them all.\n\n\n\nOrganization: files should be organized in a way that prioritizes thinking of it as a curriculum (instead of a website or a series of assignments). (subpoint: modularity)\nReproducibility:\nDocumentation:\nAutomation: automate as many non-human tasks as possible."
  },
  {
    "objectID": "docs/website.html#design",
    "href": "docs/website.html#design",
    "title": "Configuring this Website",
    "section": "",
    "text": "One repo to rule them all.\n\n\n\nOrganization: files should be organized in a way that prioritizes thinking of it as a curriculum (instead of a website or a series of assignments). (subpoint: modularity)\nReproducibility:\nDocumentation:\nAutomation: automate as many non-human tasks as possible."
  },
  {
    "objectID": "docs/website.html#contributing",
    "href": "docs/website.html#contributing",
    "title": "Configuring this Website",
    "section": "Contributing",
    "text": "Contributing\n\nConfiguring your machine\n\nOn your machine, a new RStudio project from version control, linked directly to the repo at https://github.com/stat20/stat20.\nBe sure you’ve configured git on your machine (allowing you to make commits) and stored a Personal Access Token (PAT) (allowing your machine to push commits to your repo on GitHub).\n\n\n\nWorkflow\n\nIn RStudio on main, pull any new changes.\nCreate a new branch named with what you’re adding, prepended with your name (e.g. andrew-update-lab-6).\nEdit the files you wish to work on. As you go, the easiest way to see how your changes look is to run quarto preview --profile staff-site at the terminal. That will do a minimal render of the website to show you the document that you’re working on and will re-render that document every time you save. You can also run quarto preview if you want to see the student-facing site (with no staff guide).\nCommit your changes to your new branch and push them to GitHub.\nGo to your GitHub and make a pull request from your new branch into stat20/stat20, main branch. Once the PR is made, it will kick-off a test rendering of both the student site and the staff site (the staff site will take &gt; 10 min to fully render). Once they’re done, you can go to https://stat20-pr.netlify.app/ and https://stat20staff-pr.netlify.app/ to see how they look. Note: this action will kickoff anew for each commit that you add to the PR, so it can be good to make the PR when you have just one commit on the branch, then check back as you push more commits to see how it looks.\nOnce the PR is merged into main, it will kick-off an action that will render and publish the staff site to https://stat20staff.netlify.app/. https://stat20.org/ doesn’t get rendered and published when main is changed, but instead of a pre-programmed schedule. After merging the branch, that branch can be deleted.\n\n\n\nAdding a page\n\nCreate a new qmd file\nAdd your content like normal\n\n\n\nCourse Settings"
  },
  {
    "objectID": "docs/slides.html",
    "href": "docs/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Be sure your personal fork of the course-materials is up to date by syncing it to the one on github.com/stat20.\nPull the most recent changes from your fork to your local machine.\nMake changes to slides.qmd and save.\n\n\n\n\nQuarto Pub is a free service for publishing quarto documents, including slides, online. Start by setting up an account at https://quartopub.com/.\n\nOnce you are set up with an account, in the terminal, navigate to the directory that contains your slides, for example: cd summarizing-numerical-data.\nOpen slides.qmd and, ensure that under revealjs: that you add the option self-contained: true. (read more here)\nIn that directory run:\n\n\n\nTerminal\n\nquarto publish quarto-pub slides.qmd\n\n\nYou may be prompted to answer a few questions at the terminal. If it successfully published, it will provide a link to your published document\nAt this point, you’re all set!\n\nIf you’d like to learn more about publishing with Quarto Pub, see the official documentation.\n\n\n\nTest it out by working in a much simpler directory than course-materials, one called practice-repo. Since you won’t be pushing any commits back up to GitHub, you don’t need a create your own fork.\n\nCreate a new project in Rstudio from a version control repository and paste in https://github.com/stat20/practice-repo for the url.\nModify slides.qmd in the summarizing-numerical-data directory by, say, replacing \"Instructor\" with your name in the header.\nIn the terminal, navigate into the sub-directory with the slides: cd summarizing-numerical-data\nStill at the terminal, publish your slides:\n\n\nTerminal\n\nquarto publish quarto-pub slides.qmd\n\nClick through any prompts you might see at the terminal.\nIf it publishes successfully, the terminal will print a link to your published document. Visit that site and check the link at the top that it says that it has been “Published at …”. It should appear as &lt;user_name&gt;.quarto.pub/summarizing-categorical-data/."
  },
  {
    "objectID": "docs/slides.html#modifying-your-own-slides",
    "href": "docs/slides.html#modifying-your-own-slides",
    "title": "Slides",
    "section": "",
    "text": "Be sure your personal fork of the course-materials is up to date by syncing it to the one on github.com/stat20.\nPull the most recent changes from your fork to your local machine.\nMake changes to slides.qmd and save."
  },
  {
    "objectID": "docs/slides.html#publishing-your-own-slides",
    "href": "docs/slides.html#publishing-your-own-slides",
    "title": "Slides",
    "section": "",
    "text": "Quarto Pub is a free service for publishing quarto documents, including slides, online. Start by setting up an account at https://quartopub.com/.\n\nOnce you are set up with an account, in the terminal, navigate to the directory that contains your slides, for example: cd summarizing-numerical-data.\nOpen slides.qmd and, ensure that under revealjs: that you add the option self-contained: true. (read more here)\nIn that directory run:\n\n\n\nTerminal\n\nquarto publish quarto-pub slides.qmd\n\n\nYou may be prompted to answer a few questions at the terminal. If it successfully published, it will provide a link to your published document\nAt this point, you’re all set!\n\nIf you’d like to learn more about publishing with Quarto Pub, see the official documentation."
  },
  {
    "objectID": "docs/slides.html#test-it-out",
    "href": "docs/slides.html#test-it-out",
    "title": "Slides",
    "section": "",
    "text": "Test it out by working in a much simpler directory than course-materials, one called practice-repo. Since you won’t be pushing any commits back up to GitHub, you don’t need a create your own fork.\n\nCreate a new project in Rstudio from a version control repository and paste in https://github.com/stat20/practice-repo for the url.\nModify slides.qmd in the summarizing-numerical-data directory by, say, replacing \"Instructor\" with your name in the header.\nIn the terminal, navigate into the sub-directory with the slides: cd summarizing-numerical-data\nStill at the terminal, publish your slides:\n\n\nTerminal\n\nquarto publish quarto-pub slides.qmd\n\nClick through any prompts you might see at the terminal.\nIf it publishes successfully, the terminal will print a link to your published document. Visit that site and check the link at the top that it says that it has been “Published at …”. It should appear as &lt;user_name&gt;.quarto.pub/summarizing-categorical-data/."
  },
  {
    "objectID": "docs/assignments.html",
    "href": "docs/assignments.html",
    "title": "Adding and Removing Assignments",
    "section": "",
    "text": "Assignments are hosted on the Assignments page and include a list of Labs and a list of Problem Sets. These lists build automatically if the assignment files follow a particular naming scheme. Below we discuss how to add assignments to the list. They can be removed by renaming the files or removing them entirely from the repository.\nFor further reading on how these lists are made, see the Quarto documentation on document listings."
  },
  {
    "objectID": "docs/assignments.html#problem-sets",
    "href": "docs/assignments.html#problem-sets",
    "title": "Adding and Removing Assignments",
    "section": "Problem Sets",
    "text": "Problem Sets\nProblem sets are sets of drill-style problems that provide practice with the concepts and skills of a particular day’s lesson. They are stored within the subdirectory containing all of the materials for that day.\n\nPDF handouts\n\nCreate a file called ps.qmd alongside the notes for a given day’s materials. Your directory should look like this:\nintro-to-probability\n├── notes.qmd\n├── ps.qmd\nAt least two YAML options should be specified in the front-matter, title and the custom handout format:\n---\ntitle: Calculating Chances\nformat: stat20handout-pdf\n---\nUpon rendering the site, this assignment should appear on the assignments page. The format of the assignment list is controlled by the assignments template, which will automatically assign numbers to the problem sets based on their order in the directory.\n\nThe pdf notes use the stat20handout custom pdf format. See Custom Formats for more information on its use.\n\n\nQmd handouts\n\nJust like with pdfs, create a file called ps.qmd alongside the notes for a given day’s materials. Your directory should look like this:\nintro-to-probability\n├── notes.qmd\n├── ps.qmd\nAdd the following to the front-matter of ps.qmd.\n---\ntitle: Simulation\nformat:\n  html:\n    code-tools: \n      source: true\n      toggle: false\nsidebar: false\n---\nThe title should have the name of topic, usually the same as the title of the notes. This name will be used in creating the PS name in the listing. The remaining yaml options will provide a link at the top right of the problem set webpage that, when clicked, will produce a pop-up of the source of the qmd file. Students can copy and paste this into a blank qmd file in RStudio (and can remove the extra yaml options).\n\nIf you have multiple problem set files on a single day, you can differentiate them by adding a single letter or digit following ps and hyphen. ps.qmd, ps-2.qmd, and ps-b.qmd should all work. See the source for the assignment page for the rule that determines which files show up in the listing."
  },
  {
    "objectID": "docs/assignments.html#labs",
    "href": "docs/assignments.html#labs",
    "title": "Adding and Removing Assignments",
    "section": "Labs",
    "text": "Labs\n\n\n\n\n\n\nWarning\n\n\n\nUnder Construction"
  },
  {
    "objectID": "1-questions-and-data/labs/unvotes/part-2.html",
    "href": "1-questions-and-data/labs/unvotes/part-2.html",
    "title": "UN Votes",
    "section": "",
    "text": "Answer the following questions in the Quarto document template linked in the slides for the first day of class. Render the document, download the resulting pdf file, and submit it to Gradescope."
  },
  {
    "objectID": "1-questions-and-data/labs/unvotes/part-2.html#the-context-of-the-data",
    "href": "1-questions-and-data/labs/unvotes/part-2.html#the-context-of-the-data",
    "title": "UN Votes",
    "section": "The Context of the Data",
    "text": "The Context of the Data\nSince its founding in 1946, the United Nation has regularly put forth resolutions that aim to express the will of its members. Impactful historical resolutions have involved the Declaration of Human Rights in 1948, a prohibition on the use of nuclear and thermonuclear weapons in 1968, and the establishment of a no-fly zone in Libya in 2011. After each resolution is proposed, it is voted on in a roll-call vote of the member nations.\nThe data set at hand contains the voting records of every nation in the UN from 1946 - 2019 on three different issues: Colonialism, Human Rights, and Nuclear Weapons."
  },
  {
    "objectID": "1-questions-and-data/labs/unvotes/part-2.html#computing-on-the-data",
    "href": "1-questions-and-data/labs/unvotes/part-2.html#computing-on-the-data",
    "title": "UN Votes",
    "section": "Computing on the Data",
    "text": "Computing on the Data\n\nQuestion 1\nCreate a data visualization the shows the proportion of “yes” votes over time on three different issues: Colonialism, Human Rights, and Nuclear Weapons. Construct the plot to compare the voting patterns of the United States, the United Kingdom, and one other country of your choosing (not Turkey).\n\n\nQuestion 2\nFormulate two claims about voting behavior that is supported by this graphic. Categorize each claim as begin a summary, generalization, causal claim, or prediction.\n\n\nQuestion 3\nWhat else would you like to learn about this data set or the context of voting at the United Nations that would allow you to make more informed, accurate, thoughtful, and effective claims? List three specific items."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-as-data",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-as-data",
    "title": "Taxonomy of Data",
    "section": "Images as data",
    "text": "Images as data\n\n\n\nImages are composed of pixels (this image is 1520 by 1012)\nThe color in each pixel is in RGB\n\nEach band takes a value from 0-255\nThis image is data with 1520 x 1012 x 3 values."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#grayscale",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#grayscale",
    "title": "Taxonomy of Data",
    "section": "Grayscale",
    "text": "Grayscale\n\n\n\nGrayscale images have only one band\n0 is black, 255 is white\nThis image is data with 1520 x 1012 x 1 values."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#grayscale-1",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#grayscale-1",
    "title": "Taxonomy of Data",
    "section": "Grayscale",
    "text": "Grayscale\n\n\nTo simplify, assume our photos are 8 x 8 grayscale images."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-in-a-data-frame",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-in-a-data-frame",
    "title": "Taxonomy of Data",
    "section": "Images in a Data Frame",
    "text": "Images in a Data Frame\nConsider the following images which are our data:\n\n\n\n\n\n\n\n\n\nLet’s simplify them to 8 x 8 grayscale images"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-in-a-data-frame-1",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-in-a-data-frame-1",
    "title": "Taxonomy of Data",
    "section": "Images in a Data Frame",
    "text": "Images in a Data Frame\n\n\n\n\n\n\n\n\n\nIf you were to put the data from these (8 x 8 grayscale) images into a data frame, what would the dimensions of that data frame be in rows x columns? Answer at pollev.com.\n\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#a-note-on-variables",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#a-note-on-variables",
    "title": "Taxonomy of Data",
    "section": "A note on variables",
    "text": "A note on variables\nThere are three things that “variable” could be referring to:\n\n\na phenomenon\nhow the phenomenon is being recorded or measured into data\n\nwhat values can it take? (this is often an intent- or value-laden exercise!)\nfor numerical units, what unit should we express it in?\n\nHow the recorded data is being analyzed\n\nmight you bin/discretizing income data? what are the consequences of this?\n\n\n\n\n\nFor the following question, you may work under the second definition."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#what-type-of-variable-is-age",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#what-type-of-variable-is-age",
    "title": "Taxonomy of Data",
    "section": "What type of variable is age?",
    "text": "What type of variable is age?\nFor each of the following scenarios where age could be a variable, choose the most appropriate taxonomy according to the Taxonomy of Data.\n\n\nAges of television audiences/demographics\nAges of UC Berkeley students\nThe age of a rock\n\n\n\n\nAnswer at pollev.com.\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-1",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-1",
    "title": "Taxonomy of Data",
    "section": "Educated Guess 1",
    "text": "Educated Guess 1\n\nWhat will happen here?\n\n\nAnswer at pollev.com/&lt;name&gt;\n\n\n\n\n1 + \"one\"\n\n\n  \n    −\n    +\n \n 01:00\n \n\n\n“one” is a string with no link at all to the number 1\nwithout that link, without two objects that are recognized for their numerical value, + doesn’t work."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-2",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-2",
    "title": "Taxonomy of Data",
    "section": "Educated Guess 2",
    "text": "Educated Guess 2\n\nWhat will happen here?\n\n\nAnswer at pollev.com/&lt;name&gt;\n\n\n\n\na &lt;- c(1, 2, 3, 4)\nsqrt(log(a))\n\n\n  \n    −\n    +\n \n 01:00\n \n\nTalking points - a is a vector of length four - log and sqrt are functions that will return vectors of length four - they’re nested and will be evaluated from the inside out"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-3",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-3",
    "title": "Taxonomy of Data",
    "section": "Educated Guess 3",
    "text": "Educated Guess 3\n\nWhat will happen here?\n\n\nAnswer at pollev.com/&lt;name&gt;\n\n\n\n\na &lt;- 1 + 2\na + 1\n\n\n  \n    −\n    +\n \n 01:00\n \n\n\na is a not a string, it’s the name of an object that’s a number\nto overwrite a with a + 1 requires re-assigning it to a: a &lt;- a + 1 (in some languages, a + 1 would change the value of a)\na &lt;- a + 1 is a good time to mention that while a = a + 1 works in R and they might see it online, its convention to use &lt;- for many reasons including that mathematically the statement with = is confusing."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-4",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-4",
    "title": "Taxonomy of Data",
    "section": "Educated Guess 4",
    "text": "Educated Guess 4\n\nWhat will happen here?\n\n\nAnswer at pollev.com/&lt;name&gt;\n\n\n\n\na &lt;- c(1, 3.14, \"seven\")\nclass(a)\n\n\n  \n    −\n    +\n \n 01:00\n \n\n\nthe definition of a vector requires every element to be of the same type\nbased on their reading, there are three classes that they’re familiar with: numeric, factor, and character\nthere’s no way to translate “seven” into 7, so instead 1 and 3.14 must be translated into strings\nthey will likely encounter this when looking at a data set in R (or other languages) and finding that vectors with what looked like numbers are stored as strings. This usually happens because there’s a single errant character that the language doesn’t know how to parse as a number."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#functions-on-vectors",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#functions-on-vectors",
    "title": "Taxonomy of Data",
    "section": "Functions on vectors",
    "text": "Functions on vectors\nA vector is the simplest structure used in R to store data. It can be created using the function c().\n\nmy_vector &lt;- c(1, 3, 4)\nmy_vector\n\n[1] 1 3 4\n\n\n\nA function operates on an R object and produces output. R has many of the mathematical functions that you would expect.\n\nsum(my_vector)\n\n[1] 8"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#your-turn",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#your-turn",
    "title": "Taxonomy of Data",
    "section": "Your Turn",
    "text": "Your Turn\n\n\nCreate a vector named vec with the even integers between 1 and 10 as well as the number 99 (six elements total).\nFind the sum of that vector.\nFind the max of that vector.\nTake the mean of that vector and round it to the nearest integer.\n\n\nThese should all be solved with R code. If you don’t know the name of a function to use, with hazard a guess by looking for a help file (e.g. ?sum) or google it.\n\n  \n    −\n    +\n \n 05:00\n \n\nvec &lt;- c(2, 4, 6, 8, 10, 99) sum(vec) max(vec) ?round round(mean(vec))"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#working-in-a-qmd-file",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#working-in-a-qmd-file",
    "title": "Taxonomy of Data",
    "section": "Working in a qmd file",
    "text": "Working in a qmd file\nWorking in a new .qmd file allows you to save your code for later.\n\nDemo\n\nCreate a new qmd file from the RStudio menu, name it, and save it.\nInsert a new code cell.\nWrite your code into the cell.\nRender the document."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#building-a-data-frame",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#building-a-data-frame",
    "title": "Taxonomy of Data",
    "section": "Building a data frame",
    "text": "Building a data frame\nYou can combine vectors into a data frame using data.frame()1\n\nbill_depth_mm &lt;- c(15.0, 17.1, 18.7, 18.9)\nbill_length_mm &lt;- c(47.5, 40.2, 39.0, 35.3)\nspecies &lt;- c(\"Gentoo\", \"Adelie\", \"Adelie\", \"Adelie\")\n\n\n\n\n\npenguins_df &lt;- data.frame(bill_depth_mm, bill_length_mm, species)\npenguins_df\n\n  bill_depth_mm bill_length_mm species\n1          15.0           47.5  Gentoo\n2          17.1           40.2  Adelie\n3          18.7           39.0  Adelie\n4          18.9           35.3  Adelie\n\n\n\nYou can also use the tibble() function from the tidyverse package."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#your-turn-1",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#your-turn-1",
    "title": "Taxonomy of Data",
    "section": "Your Turn",
    "text": "Your Turn\n\n\nCreate a new .qmd file, name it, and save it.\nInsert a new code cell.\nCreate three vectors, name, hometown, and sibs_and_pets that contain observations on those variables from 6 people in this class.\nCombine them into a data frame called my_classmates.\n\n\n\n  \n    −\n    +\n \n 06:00"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html",
    "title": "The Taxonomy of Data",
    "section": "",
    "text": "In the beginning was data, and from that data was built an understanding of the world.\nIn the beginning was understanding, and from that understanding sprung questions that sought to be answered with data.\nSo, which is it?\nThis is a philosophical question and it is up for debate. What is clearer is that in the process of engaging in data science, you will inevitably find yourself at one of these beginnings, puzzling over how to make your way to the other one.\nThe defining element of data science is the centrality of data as the means of advancing our understanding of the world. The word “data” is used in many different ways, so let’s write down a definition to get everyone on the same page.\nThis broad definition permits a staggering diversity in the forms that data can take. When you conducted a chemistry experiment in high school and recorded your measurements in a table in a lab notebook, that was data. When you registered for this class and your name showed on CalCentral, that was data. When the James Webb Space Telescope took a photo of the distant reaches of our solar system, recording levels of light pixel-by-pixel, that was data.\nSuch diversity in data is more precisely described as diversity in the types of variables that are being measured in a data set.\nIn your chemistry notebook you may have recorded the temperature and pressure of a unit of gas, two variables that are of scientific interest. In the CalCentral data set, name is the variable that was recorded (on you!) but you can imagine other variables that the registrars office might have recorded: your year at Cal, your major, etc. Each of these are called variables because the value that is measured generally varies as you move from one object to the next. While your value of the name variable might be Penelope, if we record the same variable on another student we’ll likely come up with different value.",
    "crumbs": [
      "Notes",
      "Questions and Data",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#a-taxonomy-of-data",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#a-taxonomy-of-data",
    "title": "The Taxonomy of Data",
    "section": "A Taxonomy of Data",
    "text": "A Taxonomy of Data\nWhile the range of variables that we can conceive of is innumerable, there are recurring patterns in those variables that allow us to group them into persistent types that have shared properties. Such a practice of classification results in a taxonomy, which has been applied most notably in evolutionary biology to classify all forms of life.\nWithin the realm of data, an analogous taxonomy has emerged.\n\n\n\n\n\n\nFigure 2: the Taxonomy of Data.\n\n\n\n\nTypes of Variables\nThe principle quality of a variable is whether it is numerical or categorical.\n\n\nNumerical Variable\n\nA variable that take numbers as values and where the magnitude of the number has a quantitative meaning.\n\n\n\n\n\nCategorical Variable\n\nA variable that take categories as values. Each unique category is called a level.\n\n\n\nWhen most people think “data” they tend to think about numerical variables (like the temperature and pressure recorded in your lab notebook) but categorical variables (like the name recorded on CalCentral) are very common.\nAll numerical variables can be classified as either continuous or discrete.\n\n\nContinuous Numerical Variable\n\nA numerical variable that takes values on an interval of the real number line.\n\n\n\n\n\nDiscrete Numerical Variable\n\nA numerical variable that takes values that have jumps between them.\n\n\n\nA good example of a continuous numerical variable is temperature. If we are measuring outside air temperature on Earth in Fahrenheit, it is possible that we would record values anywhere from around -125 degrees F and +135 degrees F. While we might end up rounding our measurement to the nearest integer degree, we can imagine that the phenomenon of temperature itself varies smoothly and continuously across this range.\nA good example of a discrete numerical variable is household size. When the US Census goes door-to-door every year collecting data on every household, they record the number of people living in that household. A household can have 1 person, or 2 people, or 3 people, or 4 people, and so on, but it cannot have 2.83944 people. This makes it discrete.\nWhat unites both types of numerical variables is that the magnitude of the numbers have meaning and you can perform mathematical operations on them and the result also has meaning. It is possible and meaningful to talk about the average air temperature across three locations. It is also possible and meaningful to talk about the sum total number of people across ten households.\nThe ability to perform mathematical operations drops away when we move to ordinal variables. All categorical variables can be classified as either ordinal or nominal.\n\n\nOrdinal Categorical Variable\n\nA categorical variable with levels that have a natural ordering.\n\n\n\n\n\nNominal Categorical Variable\n\nA categorical variable with levels with no ordering.\n\n\n\nYou have likely come across ordinal categorical variables if you have taken an opinion survey. Consider the question:“Do you strongly agree, agree, feel neutral about, disagree, or strongly disagree with the following statement: Dogs are better than cats?” When you record answers to this question, you’re recording measurements on a categorical variable that takes values “strongly agree”, “agree”, “neutral”, “disagree”, “strongly disagree”. Those are the levels of the categorical variable and they have a natural ordering: “strongly agree” is closer to “agree” than it is to “strongly disagree”.\nYou can contrast this with a nominal categorical variable. Consider a second question that asks (as the registrar does): “What is your name?” There are many more possible levels in this case - “Penelope”, “David”, “Shobhana”, etc. - but those levels have no natural ordering. In fact this is very appropriate example of a nominal variable because the word itself derives from the Latin nomen, or “name”.\nLet’s take a look at a real data set to see if we can identify the variables and their types.\n\n\nExample: Palmer Penguins\nDr. Kristen Gorman is a fisheries and wildlife ecologist at the University of Alaska, Fairbanks whose work brought her to Palmer Station, a scientific research station run by the National Science Foundation in Antarctica. At Palmer Station, she took part in a long-term study to build an understanding of the breeding ecology and population structure of penguins.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Dr. Gorman recording measurements on penguins and Palmer Station, a research station in Antarctica.\n\n\n\nIn order to build her understanding of this community of penguins, she and fellow scientists spent time in the field recording measurements on a range of variables that capture important physical characteristics.\n\n\n\nTwo of the variables that were recorded were bill length and bill depth1. Each of these capture a dimension of the bill of a penguin recorded in millimeters These are identifiable as continuous numerical variables. They’re numerical because the values have quantitative meaning and they’re continuous because bill sizes don’t come in fixed, standard increments. They vary continuously.\nAnother variable that was recorded was the species of the penguin, either “Adelie”, “Gentoo”, or “Chinstrap”. Because these values are categories, this is a categorical variable. More specifically, it’s a nominal categorical because there is no obvious natural ordering between these three species.\n\n\n\nThese are just three of many variables that recorded in the penguins data set and published along their scientific findings in the paper, Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis)2. We will return throughout this course to this data set and this study. It is a prime example of how careful data collection and careful scientific reasoning can expand our understanding of a corner of our world about which we know very little.\n\n\nWhy Types Matter\nThe Taxonomy of Data is a useful tool of statistics and data science because it helps guide the manner in which data is recorded, visualized, and analyzed. Many confusing plots have been made by not thinking carefully about whether a categorical variable is ordinal or not or by mistaking a continuous numerical variable for a categorical variable. You will get plenty of practice using this taxonomy to guide your data visualization in the next unit.\nLike many tools built by scientists, though, this taxonomy isn’t perfect. There are many variables that don’t quite seem to fit into the taxonomy or that you can argue should fit into multiple types. That’s usually a sign that something interesting is afoot and is all the more reason to think carefully about the nature of the variables and the values it might take before diving into your analysis.",
    "crumbs": [
      "Notes",
      "Questions and Data",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#a-structure-for-data-the-data-frame",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#a-structure-for-data-the-data-frame",
    "title": "The Taxonomy of Data",
    "section": "A Structure for Data: The Data Frame",
    "text": "A Structure for Data: The Data Frame\nWhen we seek to grow our understanding of a phenomenon, sometimes we select a single variable that we go out and collect data on. More often, we’re dealing with more complex phenomenon that are characterized by a few, or a few dozen, or hundreds (or even millions!) of variables. CalCentral has far more than just your name on file. To capture all of the complexity of class registration at Cal, it is necessary to record dozens of variables.\nTo keep all of this data organized, we need a structure. While there are several different ways to structure a given data set, the format that has become most central to data science is the data frame.\n\n\nData Frame\n\nAn array that associates the observations (downs the rows) with the variables measured on each observation (across the columns). Each cell stores a value observed for a variable on an observation.\n\n\n\nWhile this definition might seem opaque, you are already familiar with a data frame. You are you just more accustomed to seeing it laid out this like this:\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nspecies\n\n\n\n\n43.5\n18.1\nChinstrap\n\n\n48.1\n15.1\nGentoo\n\n\n49.0\n19.5\nChinstrap\n\n\n45.4\n18.7\nChinstrap\n\n\n34.6\n21.1\nAdelie\n\n\n49.8\n17.3\nChinstrap\n\n\n40.9\n18.9\nAdelie\n\n\n45.3\n13.7\nGentoo\n\n\n\n\n\n\n\nYou might be accustomed to calling this a “spreadsheet” or a “table”, but the organizational norm of putting the variables down the columns and the observations across the rows make this a more specific structure.\nOne of the first questions that you should address when you first come across a data frame is to determine what the unit of observation is.\n\n\nUnit of Observation\n\nThe class of object on which the variables are observed.\n\n\n\nIn the case of data frame above, the unit of observation is a single penguin near Palmer Station. The first row captures the measurements on the first penguin, the second row captures the measurements of the second penguin, and so on. If I log into CalCentral to see the data frame that records information on the students enrolled in this class, the unit of observation is a single student enrolled in this class.\n\nNot a Data Frame\nBefore you leave thinking that “data frame” = “spreadsheet”, consider this data set:\n\n\n\n\n\nFor it to be a data frame, we would have to read across the columns and see the names of the variables. You can imagine recording whether or not an alien is green or blue, but those variables would take the values “yes” and “no”, not the counts that we see here. Furthermore, total is not a variable that we’ve recorded a single unit; this column captures aggregate properties of the whole data set.\nWhile this structure might well be called a “table” or possibly a “spreadsheet”, it doesn’t meet our definition for a data frame.",
    "crumbs": [
      "Notes",
      "Questions and Data",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#summary",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#summary",
    "title": "The Taxonomy of Data",
    "section": "Summary",
    "text": "Summary\nIn this lecture note we have focused on the nature of the data that will serve as the currency from which we’ll construct an improved understanding of the world. A first step is to identify the characteristics of the variables that are being measured and determine their type within the Taxonomy of Data. A second step is to organize them into a data frame to clearly associate the value that is measured for a variable with a particular observational unit.\nWith these ideas in hand, we learned how to bring data onto our computer, so that in our next class, we can begin the process of identifying its structure and communicating that structure numerically and visually.\nContinue on to the tutorial portion of the notes",
    "crumbs": [
      "Notes",
      "Questions and Data",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#footnotes",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#footnotes",
    "title": "The Taxonomy of Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPenguin artwork by @allison_horst.↩︎\nGorman KB, Williams TD, Fraser WR (2014). Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis). PLoS ONE 9(3):e90081. https://doi.org/10.1371/journal.pone.0090081↩︎",
    "crumbs": [
      "Notes",
      "Questions and Data",
      "The Taxonomy of Data"
    ]
  },
  {
    "objectID": "glossary-defs.html",
    "href": "glossary-defs.html",
    "title": "Definitions",
    "section": "",
    "text": "Questions and Data\n\n\nSummary\n\nA numerical, graphical, or verbal description of an aspect of data that is on hand.\n\n\n\n\n\nGeneralization\n\nA numerical, graphical, or verbal description of a broader set of units than those on which data was been recorded.\n\n\n\n\n\nCausal Claim\n\nA claim that changing the value of one variable will influence the value of another variable.\n\n\n\n\n\nPrediction\n\nA guess about the value of an unknown variable, based on other known variables.\n\n\n\n\n\nData\n\nAn item of (chiefly numerical) information, especially one obtained by scientific work, a number of which are typically collected together for reference, analysis, or calculation. From Latin datum: that which is given. Facts.\n\n\n\n\n\nVariable\n\nA characteristic of an object or observational unit that can be measured and recorded.\n\n\n\n\n\nNumerical Variable\n\nA variable that take numbers as values and where the magnitude of the number has a quantitative meaning.\n\n\n\n\n\nCategorical Variable\n\nA variable that take categories as values. Each unique category is called a level.\n\n\n\n\n\nContinuous Numerical Variable\n\nA numerical variable that takes values on an interval of the real number line.\n\n\n\n\n\nDiscrete Numerical Variable\n\nA numerical variable that takes values that have jumps between them.\n\n\n\n\n\nOrdinal Categorical Variable\n\nA categorical variable with levels that have a natural ordering.\n\n\n\n\n\nNominal Categorical Variable\n\nA categorical variable with levels with no ordering.\n\n\n\n\n\nData Frame\n\nAn array that associates the observations (downs the rows) with the variables measured on each observation (across the columns). Each cell stores a value observed for a variable on an observation.\n\n\n\n\n\nUnit of Observation\n\nThe class of object on which the variables are observed."
  },
  {
    "objectID": "assets/quiz-slides.html#individual-quiz",
    "href": "assets/quiz-slides.html#individual-quiz",
    "title": "Quiz",
    "section": "Individual Quiz",
    "text": "Individual Quiz\n\n\nBefore the quiz begins\n\n🧹 Clear your desk of everything except a pen/pencil and your cheat sheet.\n🧢 Remove hats, hoods, and sunglasses.\n📱 All electronic devices (laptop, phone, airbuds, smart watch) should be remain in your bag for the entire class period.\nSpread out as much as possible: 1 student to a triangle desk, 2 to a tall desk.\nWhen you get a quiz, cover it with your cheat sheet and do not begin.\n\n\nDuring the quiz\n\n👀 Keep your eyes on your own quiz.\nIf you need to use the bathroom, bring phone and quiz to the front of the class.\n⌛If you finish early, please raise your hand and we will pick it up.\n❗You must stop writing when time is called or risk a 0.\n\nGood luck! 🍀\n\n\n  \n    −\n    +\n \n 25:00"
  },
  {
    "objectID": "assets/quiz-slides.html#group-quiz",
    "href": "assets/quiz-slides.html#group-quiz",
    "title": "Quiz",
    "section": "Group Quiz",
    "text": "Group Quiz\n\n\nBefore the quiz begins\n\nForm groups of 2-3 people.\n🧹 Everything except a pen/pencil and your cheat sheet must remain off your desk.\n📱 All electronic devices (laptop, phone, airbuds, smart watch) should be remain in your bag for the entire class period.\nWhen you get a group quiz, write all group members names on it but do not begin.\n\n\nDuring the quiz\n\n🗣️Discuss each of the problems and decide as a group which answer is best.\n⌛If you finish early, please raise your hand and we will pick it up.\n❗You must stop writing when time is called or risk a 0.\n\nGood luck! 🍀\n\n\n  \n    −\n    +\n \n 15:00"
  },
  {
    "objectID": "5-prediction/labs/07-baseball/slides.html#baseball-rules",
    "href": "5-prediction/labs/07-baseball/slides.html#baseball-rules",
    "title": "Lab 5: Multiple Regression with Baseball",
    "section": "Baseball Rules",
    "text": "Baseball Rules\n\n\nKey terms to define here are Runs (R) and Wins (W). Here’s a helpful glossary from MLB: https://www.mlb.com/glossary"
  },
  {
    "objectID": "5-prediction/labs/07-baseball/slides.html#baseball-rules-1",
    "href": "5-prediction/labs/07-baseball/slides.html#baseball-rules-1",
    "title": "Lab 5: Multiple Regression with Baseball",
    "section": "Baseball Rules",
    "text": "Baseball Rules"
  },
  {
    "objectID": "5-prediction/labs/07-baseball/slides.html#sabermetrics",
    "href": "5-prediction/labs/07-baseball/slides.html#sabermetrics",
    "title": "Lab 5: Multiple Regression with Baseball",
    "section": "Sabermetrics",
    "text": "Sabermetrics\nCoined by Bill James in 1980, sabermetrics is\n\n“the search for objective knowledge about baseball.”"
  },
  {
    "objectID": "5-prediction/labs/07-baseball/slides.html#history-of-sabermetrics",
    "href": "5-prediction/labs/07-baseball/slides.html#history-of-sabermetrics",
    "title": "Lab 5: Multiple Regression with Baseball",
    "section": "History of Sabermetrics",
    "text": "History of Sabermetrics\n\nHenry Chadwick, a NY sportswriter, developed the box score in 1859\n“Percentage Baseball” by Earnshaw Cook in 1964\nThe Bill James Baseball Abstract, annual book beginning in 1977\nMoneyball"
  },
  {
    "objectID": "5-prediction/labs/08-cancer/lab.html",
    "href": "5-prediction/labs/08-cancer/lab.html",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "5-prediction/labs/08-cancer/lab.html#part-i-understanding-the-context-of-the-data",
    "href": "5-prediction/labs/08-cancer/lab.html#part-i-understanding-the-context-of-the-data",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Part I: Understanding the Context of the Data",
    "text": "Part I: Understanding the Context of the Data\n\nLab 6.1: Cancer Diagnosis"
  },
  {
    "objectID": "5-prediction/labs/08-cancer/lab.html#part-ii-computing-on-the-data",
    "href": "5-prediction/labs/08-cancer/lab.html#part-ii-computing-on-the-data",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Part II: Computing on the Data",
    "text": "Part II: Computing on the Data\nYou can load in the biopsies data frame using the code below:\n\nlibrary(tidyverse)\nbiopsies &lt;- \n  read_csv(\"https://www.dropbox.com/s/0rbzonyrzramdgl/cells.csv?dl=1\") |&gt;\n  mutate(diagnosis = factor(diagnosis, levels = c(\"B\", \"M\")))\n\nThe diagnosis is in the column named diagnosis; each other column should be used to predict the diagnosis.\n\nMake a single plot that examines the association between radius_mean and radius_sd separately for each diagnosis (hint: aes() should have three arguments).\nCalculate the correlation between these two variables for each diagnosis.\nGive at least a two-sentence interpretation of the results in the last two questions. In particular, comment on:\n\n\nIs the relationship between radius_mean and radius_sd different for benign biopsies vs. malignant biopsies?\nIf so, can you give an explanation for this difference?\n\n\nSplit the data set into a roughly 80-20 train-test set split.\nUsing the training data, fit a simple logistic regression model that predicts the diagnosis using the mean of the texture index.\nUsing a threshold of .5, What would your model predict for a biopsy with a mean texture of 15? What probability does it assign to that outcome?\nCalculate and report two misclassification rates for your simple model: first on the training data and then on the testing data.\nBuild a more complex model to predict the diagnosis using five predictors of your choosing.\nCalculate and report two misclassification rates for your complex model: first on the training data and then on the testing data.\nIs there any evidence that your model is overfitting? Explain in at least two sentences.\nMove back to your simple model for the next few questions.Report the total number of false negatives in the test data set.\nWhat can you change about your classification rule to lower the number of false negatives?\nMake the change you identified in the previous question and calculate the new number of false negatives.\nCalculate the testing misclassification rate using your new classification rule.\nDid your misclassification rule go up or down? Answer this question and explain why it went up or down in at least two sentences."
  },
  {
    "objectID": "5-prediction/01-method-of-least-squares/notes.html",
    "href": "5-prediction/01-method-of-least-squares/notes.html",
    "title": "The Method of Least Squares",
    "section": "",
    "text": "As we near the end of the semester, we revisit one last time the four different claims that might be made based on the line plot found in the news story1.\nIn this unit use, we aim to make claims like the final one, “The Consumer Price Index will likely rise throughout the summer”. This is a prediction, a claim that uses the structure of the data at hand to predict the value of observations about which we have only partial information. In midsummer, we know the date will be July 15th, that’s the x-coordinate. But what will the y-coordinate be, the Consumer Price Index?\nThe realm of prediction is the subject of intense research activity and commercial investment at the moment. Falling under the terms “machine learning” and “AI”, models for prediction have become very powerful in recent years; they can diagnose diseases, help drive autonomous vehicles, and compose text with eerily human sensibilities. At the core of these complicated models, though, are a few simple ideas that make it all possible."
  },
  {
    "objectID": "5-prediction/01-method-of-least-squares/notes.html#key-concepts-in-prediction",
    "href": "5-prediction/01-method-of-least-squares/notes.html#key-concepts-in-prediction",
    "title": "The Method of Least Squares",
    "section": "Key Concepts in Prediction",
    "text": "Key Concepts in Prediction\nIn making predictions, the most important question is “what do you want to predict?” The answer to that question is called the response variable.\n\nResponse Variable\n\nThe variable that is being predicted. Also called the dependent or outcome variable. Indicated by \\(y\\) or \\(Y\\) when treated as a random variable.\n\n\nThe related question to ask is is, “what data will you use to predict your response?” The answer to that question is…\n\nPredictor Variable(s)\n\nThe variable or variables that used to predict the response. Also called the independent variables or the features. Indicated by \\(x_1, x_2, \\ldots\\) etc.\n\n\nAn analyst working at the Federal Reserve to predict the Consumer Price Index in midsummer would use the CPI as their response variable. Their predictor could be time (the x-coordinate in the plot from the newspaper article) but could also include the federal interest rate and the unemployment rate.\nA second analyst working in another office at the Federal Reserve is tasked with predicting whether or not the economy will be recession within six months; that event (recession or no recession) would be the response variable. To predict this outcome, they might use those same predictors - the interest rate and the unemployment rate - but also predictors like the real GDP, industrial production, and retail sales.\nThe prediction problems being tackled by these two analysts diverge in an important way: the first is trying to predict a numerical variable and the second a categorical variable. This distinction in the Taxonomy of Data defines the two primary classes of models used for prediction.\n\nRegression Model\n\nA statistical model used to predict a numerical response variable.\n\nClassification Model\n\nA statistical model used to predict a categorical response variable.\n\n\nThe past two decades have been a golden age for predictive models, with hundreds of new model types being invented that can address regression tasks or classifications tasks or often both. To take a deep dive into this diverse landscape, take a course in statistical or machine learning. For the purpose of this course, we’ll just take a dip and focus on two models: Least Squares Linear Model for Regression and a Logistic Linear Model for Classification."
  },
  {
    "objectID": "5-prediction/01-method-of-least-squares/notes.html#linear-regression",
    "href": "5-prediction/01-method-of-least-squares/notes.html#linear-regression",
    "title": "The Method of Least Squares",
    "section": "Linear Regression",
    "text": "Linear Regression\nWe first introduced linear regression as a method of explaining a continuous numerical \\(y\\) variable in terms of a linear function of \\(p\\) explanatory terms, \\(x_i\\). \\[ \\hat{y} = b_0 + b_1x_1 + b_2x_2 + \\ldots +b_px_p \\]\nEach of the \\(b_i\\) are called coefficients.\nBefore, we saw linear regression in a very particular context: we used it for its ability to help us describe the structure of the data set at hand. It calculates not just one but two or more summary statistics - the regression coefficients, that tell us about the linear relationships between the variables.\nWe’re now going to revisit to this model for its ability to also make predictions on unseen data. Before we do so, though, you should be sure you’re familiar with these concepts: correlation coefficient, slope, intercept, fitted values, and residuals. Take a moment to skim those notes:\n\nSummarizing Numerical Associations\nMultiple Linear Regression\n\n\nThe Method of Least Squares\nWhen we presented the equations to calculate the slope and intercept of a least squares linear model in Unit 1, we did so without any explanation of where those equations came from. The remainder of these notes will cast some light on this mystery.\nThe least squares linear model is so-called because it defines a line that has a particular mathematical property: it is the line that has the lowest residual sum of squares of all possible lines.\n\nResidual Sum of Squares (RSS)\n\nFor observations of an explanatory variable \\(y_i\\), a response variable \\(y_i\\), predictions of its value (fitted values) \\(\\hat{y}_i\\), and a data set with \\(n\\) observations, the RSS is \\[ RSS = \\sum_{i = 1}^n \\left(y_i - \\hat{y}_i \\right)^2\\] where \\[ \\hat{y_i} = b_{0} + b_1x_{1,i} + b_2x_{2,i} + \\ldots +b_px_{p,i} \\]\n\n\nPlugging this expression into the first equation gives us\n\\[ RSS = \\sum_{i = 1}^n \\left(y_i - (b_{0} + b_1x_{1,i} + b_2x_{2,i} + \\ldots +b_px_{p,i}  )\\right)^2\\]\nSo how can we use this definition to decide precisely which slope \\(b_0\\) and intercept \\(b_1\\) to use? Please watch the following 19 minute video to find out.\n\n\n\nAlgorithmic Methods\nAs we saw in the video, the problem of least squares can be defined as one of optimization. We would like to make \\(RSS\\) as small as possible. The values of the coefficients that do this are said to optimize the \\(RSS\\) equation. Although a direct way to find these values involves taking the partial derivatives of the third equation with respect to each coefficient \\(b_0\\), \\(b_1\\), …, \\(b_p\\), setting the resulting equation equal to 0, and solving for the coefficient, you could think about using a different, perhaps more intuitive method.\nConsider picking a starting value for each of \\(b_0\\) through \\(b_p\\), plugging these into the third equation, and seeing what comes out. Then, try to pick another set of numbers that makes the result of the \\(RSS\\) equation smaller. Provided that you have a concerted method to pick the next set of numbers, you can repeat this process over and over until you find the set of numbers which yields the smallest value for \\(RSS\\). This process can be described as an algorithmic method to find the values of the coefficients \\(b_0\\) through \\(b_p\\).\nThere are many iterative algorithms that accomplish the same task, some better than others. Two examples:\n\nNelder-Mead: an older and more general (and generally not as reliable!) algorithm. This is the one we showed you in the video.\nGradient Descent: the most-used algorithm currently. Used to fit deep learning models."
  },
  {
    "objectID": "5-prediction/01-method-of-least-squares/notes.html#the-ideas-in-code",
    "href": "5-prediction/01-method-of-least-squares/notes.html#the-ideas-in-code",
    "title": "The Method of Least Squares",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\n\nWriting a function in R\nWe have been using functions that already exist in either the base installation of R or in another package, such as dplyr, ggplot2 or stat20data, but sometimes it can be useful to write our own functions.\nCustom functions in R can be created with the function function() (quite meta) and assigned to an object. The arguments that we would like the function to take go inside the parentheses of function(). The things that we would like the function to do go inside {}.\nHere is a representation of \\(f(x) = (x+.5)^2\\) in R, which we are saving into the object f. Once we run the following code, we’ll have access to f in our environment and can use it.\n\nf &lt;- function(x) {\n  (x + .5)^2\n}\n\n\nf(x = 1.5)\n\n[1] 4\n\n\n\n\nNelder-Mead on a simple function\nHere, we plot the function \\(f(x)\\) that we described above over the \\(x\\) values \\([-1,1]\\).\n\n\n\n\n\n\n\n\n\nWe can see that the minimum value of \\(f(x)\\) lies between \\(-1\\) and \\(1\\). Can we find the value of \\(x\\) that will give us this minimum using Nelder-Mead? We can try, using the optim() function!\n\noptim()\nThere are two main arguments to modify:\n\nThe function to optimize is passed to fn. In this case, the function we made is called f.\nYou provide a starting point for the algorithm with par.\n\nIn this situation, we are trying to find the value of \\(x\\) to minimize \\(f(x)\\), so we will enter a number. We’ll start with \\(x = .5\\), but you can tinker around yourself and try something different. Depending on what you start with, you may come up with something different as a final answer, so feel free to try out different numbers!\nIn a linear regression context, we are trying find the values of the coefficients \\(b_0\\) through \\(b_p\\) to minimize \\(\\hat{y}\\), so we need to input a vector of starting values, one for each coefficient. This can be done using c().\n\n\nNelder-Mead is a random algorithm; each time you run it you’ll get a (slightly) different answer. We therefore use set.seed() to make sure the same value is printed out for the purpose of publishing these notes.\n\nset.seed(5)\noptim(par = .5, fn = f)\n\n$par\n[1] -0.4\n\n$value\n[1] 0.01\n\n$counts\nfunction gradient \n      12       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nYou don’t need to understand the entirety of the output given by optim()– just focus on the number under $par. This is the value of \\(x\\) that optim() thinks will give the minimum value of \\(f(x)\\) after. In truth, the correct value is \\(-0.5\\), so we didn’t fare too badly!"
  },
  {
    "objectID": "5-prediction/01-method-of-least-squares/notes.html#summary",
    "href": "5-prediction/01-method-of-least-squares/notes.html#summary",
    "title": "The Method of Least Squares",
    "section": "Summary",
    "text": "Summary\nPrediction is the task of predicting the value of a response variable for an unseen observation using its values of other variables that are known, the predictors. The nature of the response variable determines the nature of the task (and corresponding model): regression concerns the prediction of numerical response variables, classification concerns the prediction of categorical response variables. One such regression model is the least squares linear model, which uses the line that minimizes the residual sum of squares. Finding the value of the coefficients is a task that can be solved directly using calculus (in simple settings like this one) and also with optimization algorithms (in more general settings). We gave an example of using such an algorithm, demonstrating how to write a function in R in the process."
  },
  {
    "objectID": "5-prediction/01-method-of-least-squares/notes.html#footnotes",
    "href": "5-prediction/01-method-of-least-squares/notes.html#footnotes",
    "title": "The Method of Least Squares",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSmialek, Jeanna (2022, May 11). Consumer Prices are Still Climbing Rapidly. The New York Times. https://www.nytimes.com/2022/05/11/business/economy/april-2022-cpi.html↩︎"
  },
  {
    "objectID": "5-prediction/03-overfitting/slides.html#agenda",
    "href": "5-prediction/03-overfitting/slides.html#agenda",
    "title": "Overfitting",
    "section": "Agenda",
    "text": "Agenda\n\nAnnouncements\nConcept Questions\nProblem Set: Overfitting\nLab: Cancer Diagnosis"
  },
  {
    "objectID": "5-prediction/03-overfitting/slides.html#announcements",
    "href": "5-prediction/03-overfitting/slides.html#announcements",
    "title": "Overfitting",
    "section": "Announcements",
    "text": "Announcements\n\nProblem Sets:\n\nPS 18: Overfitting releases Tuesday and due next Tuesday at 9am\nExtra Practice: Logistic Regression releases Thursday (non-turn in)\n\n\n\n\nLab 6:\n\nLab 6.1 releases Tuesday and due next Tuesday at 9am\nLab 6.2 releases Thursday and due next Tuesday at 9am\n\n\n\n\n\nQuiz 4:\n\nnext Monday in-class.\ncovers Wrong By Design through Logistic Regression (Thu/Fri)"
  },
  {
    "objectID": "5-prediction/03-overfitting/slides.html#rq-1",
    "href": "5-prediction/03-overfitting/slides.html#rq-1",
    "title": "Overfitting",
    "section": "RQ 1",
    "text": "RQ 1\n\n\n\n−+\n01:00\n\n\n\n\nWhich one of these (open pollev.com) is not an example of overfitting (either in real life or in statistics)?"
  },
  {
    "objectID": "5-prediction/03-overfitting/slides.html#where-is-overfitting-worse",
    "href": "5-prediction/03-overfitting/slides.html#where-is-overfitting-worse",
    "title": "Overfitting",
    "section": "Where is overfitting worse?",
    "text": "Where is overfitting worse?\n\n\n\n−+\n01:00\n\n\n\n\nSuppose I overfit my model to the training data. In which scenario (for which training data) would I expect the test set performance to be significantly worse? Assume that the testing sets A and B look like their corresponding training sets.\n\n\n\nIf you overfit the data to training set A, you will likely have a complex curve (maybe a polynomial). This curve will fit poorly a testing set that looks similar to A. If you overfit to B, since the data has low variance you will have a positively sloping line. The testing set performance will be very similar to the training set performance (it will not be markedly worse).\nAnswer: Training set A"
  },
  {
    "objectID": "5-prediction/02-improving-predictions/notes.html",
    "href": "5-prediction/02-improving-predictions/notes.html",
    "title": "Evaluating and Improving Predictions",
    "section": "",
    "text": "In the last lecture we built our first prediction machine: a line drawn through a scatter plot that that minimizes the sum of squared residuals. In these lecture notes we focus on two questions: How can we evaluate the quality of our predictions? and How can we improve them?"
  },
  {
    "objectID": "5-prediction/02-improving-predictions/notes.html#evaluating-the-fit-to-your-data",
    "href": "5-prediction/02-improving-predictions/notes.html#evaluating-the-fit-to-your-data",
    "title": "Evaluating and Improving Predictions",
    "section": "Evaluating the fit to your data",
    "text": "Evaluating the fit to your data\nOnce you have fit a linear model to a scatter plot, you are able to answer questions such as:\n\nWhat graduation rate would you expect for a state with a poverty rate of 15%?\n\nGraphically, this can be done by drawing a vertical line from where the poverty rate is 15% and finding where that line intersects your linear model. If you trace from that intersection point horizontally to the y-axis, you’ll find the predicted graduation rate.\n\n\n\n\n\n\n\n\n\nFrom the plot above, we can tell that the model yields a prediction around roughly 82.5%. To be more precise, we could plug the x-value into our equation for the line and solve.\n\\[ \\hat{y} = 96.2 + -0.89 \\cdot 15 = 82.85 \\]\nSo how good of a prediction is 82.85%? Until we observe a state with a poverty rate of 15%, we’ll never know! What we can know, however, is how well our model explains the structure found in the data that we have observed. For those observations, we have both the predicted (or fitted) values \\(\\hat{y}_i\\) as well as their actual y-values \\(y_i\\). These can be used to calculate a statistic that measures the explanatory power of our model.\n\nMeasuring explanatory power: \\(R^2\\)\n\\(R^2\\) is a statistic that captures how good the predictions from your linear model are (\\(\\hat{y}\\)) by comparing them another even simpler model: \\(\\bar{y}\\). To understand how this statistic is constructed please watch this short video found in the Media Gallery on bCourses (14 minutes).\n\n\n\n\n\n\nR-squared (\\(R^2\\))\n\nA statistic that measures the proportion of the total variability in the y-variable (total sum of squares, TSS) that is explained away using our model involving x (sum of squares due to regression, SSR).\n\\[R^2 = \\frac{SSR}{TSS} = \\frac{\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\nBecause the total variablity is composed of the explained and the unexplain variability, \\(R^2\\) can be equivalent formulated as 1 minus the proportion of total variability that is unexplained by the model, which uses the more familiar residual sum of squares (RSS).\n\\[R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\n\\(R^2\\) has the following properties:\n\nAlways takes values between 0 and 1.\n\\(R^2\\) near 1 means predictions were more accurate.\n\\(R^2\\) near 0 means predictions were less accurate.\n\n\n\n\n\n\n\n\n\n\n\nExample: Poverty and Graduation\nTo fit the least squares linear regression model to predict graduation rate using the poverty rate, we turn to the familiar lm() function.\nFor this particular model, \\(R^2 = .56\\). This means that poverty rate is able to explain about 56% of the variability found in graduation rates. That’s a good start!"
  },
  {
    "objectID": "5-prediction/02-improving-predictions/notes.html#improving-predictions",
    "href": "5-prediction/02-improving-predictions/notes.html#improving-predictions",
    "title": "Evaluating and Improving Predictions",
    "section": "Improving predictions",
    "text": "Improving predictions\n\\(R^2\\) allows us to quantify how well the model explains the structure found in the data set. From a model-building standpoint, it gives us a goal: to find a model with the highest possible \\(R^2\\). Here we outline three different methods for pursuing this goal - adding predictors, transformations, and polynomials - and we’d look at a different data set for each one.\n\nAdding Predictors\nLet’s return to the data set that that we studying when we first learned about multiple linear regression: ratings of Italian restaurants from the ZAGAT guide. For each of the 168 restaurants in the data set, we have observations on the average price of a meal, the food quality, the quality of the decor, the quality of the service, and whether it is east or west of Fifth Avenue.\n\n\n# A tibble: 168 × 6\n   restaurant          price  food decor service geo  \n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1 Daniella Ristorante    43    22    18      20 west \n 2 Tello's Ristorante     32    20    19      19 west \n 3 Biricchino             34    21    13      18 west \n 4 Bottino                41    20    20      17 west \n 5 Da Umberto             54    24    19      21 west \n 6 Le Madri               52    22    22      21 west \n 7 Le Zie                 34    22    16      21 west \n 8 Pasticcio              34    20    18      21 east \n 9 Belluno                39    22    19      22 east \n10 Cinque Terre           44    21    17      19 east \n# ℹ 158 more rows\n\n\nMaybe we want a model that will tell us how much we will have to spend at a new restaurant that is not upfront about its pricing; or maybe we just opened a new restaurant and want to know how much customers expect to spend. So price will serve as our response variable, leaving us four possible predictor variables. Let’s fit four different regression models, each one incorporating more more information by adding a predictor.\nUnfortunately we can’t visualize these four linear models as four lines on a scatterplot because only the first model describes a line. The second describes two parallel lines; the third describes two parallel planes in 3D; the fourth describes two parallel hyperplanes in 4D (🤯).\nWe can, however, compare these four models in an arena where they’re all on the same playing field: how well they predict price. To quantify that, we can calculate the \\(R^2\\) value for each.\n\n\n  model R_squared\n1    m1 0.3931835\n2    m2 0.3987720\n3    m3 0.6278808\n4    m4 0.6278809\n\n\nObserve that the more information we provide the model - by adding predictors - the greater the \\(R^2\\) becomes! This is not a particular characteristic of the ZAGAT data set but of \\(R^2\\) in general. Adding new predictors will never lower the \\(R^2\\) of a model fit using least squares.\n\n\nNon-linear transformation\nThe world is not always linear. We can create non-linear prediction models by building off the above linear model machinery. To demonstrate how to use this approach to increase the predictive power of our model, we’ll turn to a non-linear trend that should look familiar…\n\nA single non-linear term\nTake a question from flights lab as an example where we plot the average airspeed vs. flight distance. First let’s try fitting a linear model.\n\n\n\n\n\n\n\n\n\nA linear model does not seem appropriate to model average speed from distance. There does appear to be a monotonically increasing trend, but it starts out steeper then flattens out1. This trend is reminiscent of functions like log or square root.\nLets try transforming our predictor (distance) with the log function to create a new variable called log_dist.\nWe can then fit a linear model using this new log_dist variable as the predictor.\nLooking at the data below, we see there does seem to be a linear relationship between avg_speed and our new variable log_dist! Notice the x-axis in the below plot is log_dist whereas it was distance in the above plot.\n\n\n\n\n\n\n\n\n\nThe linear model with log_dist (\\(R^2=0.843\\)) predicts avg_speed better than the linear model with distance (\\(R^2=0.72\\))\nWe can now think of our predictive model as\n\\[\n\\widehat{y} = b_0 + b_1 \\cdot \\log(x)\n\\]\nIn other words, our model is non-linear since \\(x\\) appears inside of a logarithm. We can plot the non-linear prediction function in the original predictor distance and we see the prediction function is curved!\n\n\n\n\n\n\n\n\n\nSo is this a linear model or a non-linear model? It’s both. We created a new variable log_dist by transforming the original variable; the prediction function is a linear function of this new variable. But we can also think of this as a function of the original variable distance; the prediction function is a non-linear function of this original variable.\n\n\n\nPolynomials\nSometimes we need an more complex transformation than just a simple function (e.g. \\(\\sqrt{x}, \\log(x),  x^2,...\\)). Take the following example where there is a strong association between x and y, but it’s not linear (this data, admitted, was simulated in R).\n\n\n\n\n\n\n\n\n\nSo how should we model this? Polynomials to the rescue!\nA polynomial is a function like\n\\[\nf(x) = -20 + 34 x - 16 x^2 + 2 x^3\n\\]\nMore generally a polynomial is a function like\n\\[\nf(x) = c_0 + c_1 \\cdot x + c_2 \\cdot x^2 + \\dots + c_d \\cdot x^d\n\\]\nwhere the \\(d+1\\) coefficients \\(c_0, c_1, \\dots, c_d\\) are constants The number \\(d\\) is called the degree of the polynomial – this is the largest exponent that appears.\nPolynomials are flexible functions that can be quite useful for modeling. We can fit a polynomial model by adding new transformed variables to the data frame then fitting a linear model with these new transformed variables. This is just like how we fit a logarithmic function before by adding a new log transformed variable to the data frame then fit a linear model.\n\nThe prediction function here is a polynomial given by\n\\[\n\\widehat{y} = -20.086 + 34.669 \\cdot x -16.352 \\cdot x^2 + 2.042 \\cdot x^3\n\\]"
  },
  {
    "objectID": "5-prediction/02-improving-predictions/notes.html#the-ideas-in-code",
    "href": "5-prediction/02-improving-predictions/notes.html#the-ideas-in-code",
    "title": "Evaluating and Improving Predictions",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\n\nInspect model output with the broom library\nConsider the code we ran earlier to fit a linear model which can predict graduation rate using the poverty rate.\nWhen you run this code, you’ll see a new object appear in your environment: m1. This new object, though, is not a vector or a data frame. It’s a much richer object called a list that stores all sorts of information about your linear model. You can click through the different part of m1 in your environment pane, or your can use functions from the broom package to extract the important components using code.\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.558         0.549  2.50      61.8 3.11e-10     1  -118.  242.  248.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe glance() function returns a series of different metrics used to evaluate the quality of your model. First among those is r-squared. Because the output of glance() is just another data frame, we can extract just the r-squared column using select().\n\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.558\n\n\nHere’s the \\(R^2\\) we got earlier!\n\n\nFitting polynomials in R with poly()\nIn R, we can fit polynomials using the poly() function. Here is the code that was used to fit the polynomial earlier in the notes.\nYou do not need to worry about the meaning behind the raw = TRUE argument. The simulated data frame mentioned earlier is called df, and has two variables in it: predictor and response.\n\n\n\nCall:\nlm(formula = response ~ poly(x = predictor, degree = 3, raw = TRUE), \n    data = df)\n\nCoefficients:\n                                 (Intercept)  \n                                     -20.086  \npoly(x = predictor, degree = 3, raw = TRUE)1  \n                                      34.669  \npoly(x = predictor, degree = 3, raw = TRUE)2  \n                                     -16.352  \npoly(x = predictor, degree = 3, raw = TRUE)3  \n                                       2.042  \n\n\n\n\nMaking predictions on a new observation with predict()\nWe have spending a lot of time talking about how to fit a model meant for predicting, but have not actually done any predicting! The predict() function can help us do this. It takes in two main arguments:\n\nobject: This is the linear model object which contains the coefficients \\(b_0\\), …, \\(b_p\\). In the graduate and poverty example, this object was m1. We had m1 through m4 in the ZAGAT example.\nnewdata: This is a data frame containing the new observation(s). This data frame must at least contain each of the predictor variables used in the column, with a value of these variables for each observation.\n\n\nExample: ZAGAT food rating\nHere, we will use m2 from the ZAGAT example. This model used \\(food\\) and \\(geo\\) in an attempt to predict price at a restaurant.\nFirst, let’s make a new data frame with a couple of new, made-up observations.\nOne of these restaurants is located in east Manhattan and has a food score of 25/30, while the other one is in west Manhattan and has a food score of 17/30.\nNow, we can use this data frame alongside our m2 model object to make predictions for the prices.\n\n\n       1        2 \n55.89738 31.44043 \n\n\nWe are predicted to have to pay roughly \\(\\$56\\) at the first restaurant and roughly \\(\\$31\\) at the second."
  },
  {
    "objectID": "5-prediction/02-improving-predictions/notes.html#summary",
    "href": "5-prediction/02-improving-predictions/notes.html#summary",
    "title": "Evaluating and Improving Predictions",
    "section": "Summary",
    "text": "Summary\nIn this lecture we learned how to evaluate and improve out predictions. While there are many metrics to measure the explanatory power of a model, one of the most commonly used is \\(R^2\\), the proportion of the variability of the \\(y\\) that is explained by the model.\nTo improve our predictions - and increase the \\(R^2\\) - we saw three different strategies. If you have additional predictors in your data frame, its easy as pie to add them to your regression model and you are guaranteed to increase your \\(R^2\\).\nA second strategy is capture non-linear structure by creating new variables that are simple transformations of the existing variable. The third approach, also targeting non-linear structure, is to replace a single predictor with a polynomial."
  },
  {
    "objectID": "5-prediction/02-improving-predictions/notes.html#footnotes",
    "href": "5-prediction/02-improving-predictions/notes.html#footnotes",
    "title": "Evaluating and Improving Predictions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe call this concave or sometimes diminishing marginal returns.↩︎"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#agenda",
    "href": "5-prediction/05-logistic-regression/slides.html#agenda",
    "title": "Logistic Regression",
    "section": "Agenda",
    "text": "Agenda\n\nAnnouncements\nConcept Questions\nBreak\nLab 6.2\nAppendix: Logistic Regression with the penguins dataset"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#announcements",
    "href": "5-prediction/05-logistic-regression/slides.html#announcements",
    "title": "Logistic Regression",
    "section": "Announcements",
    "text": "Announcements\n\nQuiz 4:\n\nMonday in class.\nWrong By Design through Logistic Regression\n\n\n\n\nProblem Sets:\n\nPS 18 (Overfitting) due next Tuesday at 9am\nExtra Practice (Logistic Regression)\n\n\n\n\n\nLab 6:\n\nboth parts due Tuesday at 9am"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#section",
    "href": "5-prediction/05-logistic-regression/slides.html#section",
    "title": "Logistic Regression",
    "section": "",
    "text": "A logistic regression model was fit in an attempt to predict the sex of a penguin \"male\" or \"female\" based on its body mass (grams).\n\nAssuming that no change to the penguins dataset was made, will the model be predicting the probability of the penguin being male or the probability of the penguin being female?\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#section-1",
    "href": "5-prediction/05-logistic-regression/slides.html#section-1",
    "title": "Logistic Regression",
    "section": "",
    "text": "m1 &lt;- glm(sex ~ body_mass_g, data = penguins, family = \"binomial\")\n\n\n\n (Intercept)  body_mass_g \n-5.162541644  0.001239819 \n\n\n\nWhich of the expressions given in the poll (math or code) will correctly calculate the predicted probability that a penguin that weighs 4000 g is a female? Select all that apply\n\n\n\n\n−+\n01:00\n\n\n\n\nTo answer this one correctly, they’ll need to know (or remember from the RQ) that the reference level is female, so this model is predicting the probability male.\nYou can show that they can either use R as a calculator or use the predict function.\n1/(1 + exp(-(-5.15 + .00124 * 4000))) predict(m1, newdata = data.frame(body_mass_g = 4000), type = “response”)\nThe latter is more precise, which is useful here since p-hat (prob of male) is .449 therefore prob of female is .55.\nAs a bonus, try sketching this function on a scatterplot!\nWhen sketching this, the positive slope means the s-curve goes up and to the right. The negative intercept shifts the whole curve a tiny bit to the left, reflecting the base rate of a couple more males than females (168 vs 165)."
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#section-2",
    "href": "5-prediction/05-logistic-regression/slides.html#section-2",
    "title": "Logistic Regression",
    "section": "",
    "text": "What is the misclassification rate of this model?\n\n\n\n# A tibble: 4 × 3\n# Groups:   sex [2]\n  sex    y_hat      n\n  &lt;fct&gt;  &lt;chr&gt;  &lt;int&gt;\n1 female female   109\n2 female male      56\n3 male   female    74\n4 male   male      94\n\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#section-3",
    "href": "5-prediction/05-logistic-regression/slides.html#section-3",
    "title": "Logistic Regression",
    "section": "",
    "text": "m2 &lt;- glm(sex ~ body_mass_g + bill_length_mm, \n          data = penguins, family = \"binomial\")\n\n\n\n   (Intercept)    body_mass_g bill_length_mm \n   -6.91208086     0.00101530     0.06112808 \n\n\n\nOpen up RStudio and fit the model here in the slides. What are the predicted sexes of these two penguins?\n\nbody mass = 3900 g, bill length = 50\nbody mass = 4100 g, bill length = 35\n\n\n\n\n\n−+\n01:00\n\n\n\n\nYou can have students work on their laptops for this question.\nThis one extends the task of the previous one by having them include another covariate and also to do the thresholding procedure to go from p-hat to y-hat. This doesn’t specify the threshold, so using .5 is probably a good default.\npredict(m2, newdata = data.frame(body_mass_g = 3900, bill_length_mm = 50), type = “response”) # male predict(m2, newdata = data.frame(body_mass_g = 4100, bill_length_mm = 35), type = “response”) # female"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#building-a-predictive-model",
    "href": "5-prediction/05-logistic-regression/slides.html#building-a-predictive-model",
    "title": "Logistic Regression",
    "section": "Building a predictive model",
    "text": "Building a predictive model\n\nDecide on the mathematical form of the model: logistic linear regression\n\n\n\nSelect a metric that defines the “best” fit: the coefficients in logistic regression are the ones that minimize not the RSS function but a function called log-loss (which we don’t have time to cover)\n\n\n\n\nEstimating the coefficients of the model that are best using the training data: we know how to do this: test + train + glm()!\n\n\n\n\nEvaluating predictive accuracy using a test data set:\\(R^2\\) isn’t relevant here. We need a new metric!"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#example-penguins",
    "href": "5-prediction/05-logistic-regression/slides.html#example-penguins",
    "title": "Logistic Regression",
    "section": "Example: penguins",
    "text": "Example: penguins\n\nset.seed(132)\n\n# randomly sample train/test set split\nset_type &lt;- sample(x = c('train', 'test'), \n                   size = nrow(penguins), \n                   replace = TRUE, \n                   prob = c(0.8, 0.2))"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#example-penguins-1",
    "href": "5-prediction/05-logistic-regression/slides.html#example-penguins-1",
    "title": "Logistic Regression",
    "section": "Example: penguins",
    "text": "Example: penguins\n\nset.seed(132)\n\n# randomly sample train/test set split\nset_type &lt;- sample(x = c('train', 'test'), \n                   size = nrow(penguins), \n                   replace = TRUE, \n                   prob = c(0.8, 0.2))\n\ntrain &lt;- penguins |&gt;\n  filter(set_type == \"train\")\n\ntest &lt;- penguins |&gt;\n  filter(set_type == \"test\")"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#predicting-into-test-set",
    "href": "5-prediction/05-logistic-regression/slides.html#predicting-into-test-set",
    "title": "Logistic Regression",
    "section": "Predicting into test set",
    "text": "Predicting into test set\n\nm2 &lt;- glm(sex ~ body_mass_g + bill_length_mm,\n          data = train, family = \"binomial\")\np_hat &lt;- predict(m2, test, type = \"response\")"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#predicting-into-test-set-1",
    "href": "5-prediction/05-logistic-regression/slides.html#predicting-into-test-set-1",
    "title": "Logistic Regression",
    "section": "Predicting into test set",
    "text": "Predicting into test set\n\nm2 &lt;- glm(sex ~ body_mass_g + bill_length_mm,\n          data = train, family = \"binomial\")\np_hat &lt;- predict(m2, test, type = \"response\")\n\ntest |&gt;\n  select(sex)\n\n# A tibble: 70 × 1\n   sex   \n   &lt;fct&gt; \n 1 female\n 2 male  \n 3 female\n 4 male  \n 5 male  \n 6 female\n 7 male  \n 8 female\n 9 male  \n10 male  \n# ℹ 60 more rows"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#predicting-into-test-set-2",
    "href": "5-prediction/05-logistic-regression/slides.html#predicting-into-test-set-2",
    "title": "Logistic Regression",
    "section": "Predicting into test set",
    "text": "Predicting into test set\n\nm2 &lt;- glm(sex ~ body_mass_g + bill_length_mm,\n          data = train, family = \"binomial\")\np_hat &lt;- predict(m2, test, type = \"response\")\n\ntest |&gt;\n  select(sex) |&gt;\n  mutate(p_hat = p_hat)\n\n# A tibble: 70 × 2\n   sex    p_hat\n   &lt;fct&gt;  &lt;dbl&gt;\n 1 female 0.345\n 2 male   0.566\n 3 female 0.259\n 4 male   0.280\n 5 male   0.365\n 6 female 0.196\n 7 male   0.428\n 8 female 0.220\n 9 male   0.559\n10 male   0.279\n# ℹ 60 more rows"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#predicting-into-test-set-3",
    "href": "5-prediction/05-logistic-regression/slides.html#predicting-into-test-set-3",
    "title": "Logistic Regression",
    "section": "Predicting into test set",
    "text": "Predicting into test set\n\nm2 &lt;- glm(sex ~ body_mass_g + bill_length_mm,\n          data = train, family = \"binomial\")\n\ntest |&gt;\n  select(sex) |&gt;\n  mutate(p_hat = predict(m2, test, type = \"response\"),\n         y_hat = ifelse(p_hat &gt; .5, \"male\", \"female\"))\n\n# A tibble: 70 × 3\n   sex    p_hat y_hat \n   &lt;fct&gt;  &lt;dbl&gt; &lt;chr&gt; \n 1 female 0.345 female\n 2 male   0.566 male  \n 3 female 0.259 female\n 4 male   0.280 female\n 5 male   0.365 female\n 6 female 0.196 female\n 7 male   0.428 female\n 8 female 0.220 female\n 9 male   0.559 male  \n10 male   0.279 female\n# ℹ 60 more rows"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#classification-errors",
    "href": "5-prediction/05-logistic-regression/slides.html#classification-errors",
    "title": "Logistic Regression",
    "section": "Classification errors",
    "text": "Classification errors\n\ntest |&gt;\n  select(sex) |&gt;\n  mutate(p_hat = p_hat,\n         y_hat = ifelse(p_hat &gt; .5, \"male\", \"female\"),\n         FP = sex == \"female\" & y_hat == \"male\",\n         FN = sex == \"male\" & y_hat == \"female\")\n\n# A tibble: 70 × 5\n   sex    p_hat y_hat  FP    FN   \n   &lt;fct&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;lgl&gt; &lt;lgl&gt;\n 1 female 0.345 female FALSE FALSE\n 2 male   0.566 male   FALSE FALSE\n 3 female 0.259 female FALSE FALSE\n 4 male   0.280 female FALSE TRUE \n 5 male   0.365 female FALSE TRUE \n 6 female 0.196 female FALSE FALSE\n 7 male   0.428 female FALSE TRUE \n 8 female 0.220 female FALSE FALSE\n 9 male   0.559 male   FALSE FALSE\n10 male   0.279 female FALSE TRUE \n# ℹ 60 more rows"
  },
  {
    "objectID": "5-prediction/05-logistic-regression/slides.html#misclassification-rate",
    "href": "5-prediction/05-logistic-regression/slides.html#misclassification-rate",
    "title": "Logistic Regression",
    "section": "Misclassification Rate",
    "text": "Misclassification Rate\n\ntest |&gt;\n  select(sex) |&gt;\n  mutate(p_hat = p_hat,\n         y_hat = ifelse(p_hat &gt; .5, \"male\", \"female\")) |&gt;\n  summarize(MCR = mean(sex != y_hat))\n\n# A tibble: 1 × 1\n    MCR\n  &lt;dbl&gt;\n1 0.371"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "R Tutorials",
    "section": "",
    "text": "Tutorials serve to introduce you to the fundamentals of the R language.",
    "crumbs": [
      "Tutorials",
      "R Tutorials"
    ]
  },
  {
    "objectID": "3-generalization/labs/elections/learning-objectives.html",
    "href": "3-generalization/labs/elections/learning-objectives.html",
    "title": "Stat 20",
    "section": "",
    "text": "Write down a probability model for vote counts.\nReview\n\n\nUnit of observation\n\n\n\nlibrary(rvest)\n# Reading in the table from Wikipedia\ntab &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_tallest_buildings\") %&gt;% \n  html_node(\".wikitable\") %&gt;%\n  html_table(fill = TRUE)\n\nReferences:\nOriginal iran analysis:\nhttps://www.researchgate.net/publication/45856921_A_first-digit_anomaly_in_the_2009_Iranian_presidential_election\nhttps://arxiv.org/pdf/0906.2789.pdf\nFollow up by walter mebane:\nhttp://websites.umich.edu/~wmebane/note18jun2009.pdf\nGelman’s take:"
  },
  {
    "objectID": "3-generalization/notes.html",
    "href": "3-generalization/notes.html",
    "title": "Generalization",
    "section": "",
    "text": "In an enormously entertaining paper written about a decade ago, the economist Peter Backus estimated his chance of finding a girlfriend on any given night in London at about 1 in 285,000 or 0.0000034%. As he writes, this is either depressing or cheering news for a person, depending on what you had estimated your chance to be before reading the paper and doing a similar computation for yourself.1 The interesting point in the paper was using a probabilistic argument (originally developed by the astronomer and astrophysicist Frank Drake to estimate the probability of extra-terrestrial civilizations) to think about his dating problems. Anyone can follow the arguments put forward by Backus, including his statements that use probability.\nWe all have some notion of chance or probability, and can ask questions like: - What is the chance you will get an A in Stat 20? (About 32%, based on last fall.)2 - What is the chance the 49ers will win the Super Bowl this year? (They are the favorites, with an implied probability of about 54.5%, .)3 - What is the chance you will roll a double on your next turn to get out of jail while playing Monopoly? (One in six.) - What is the chance that Donald Trump will win the Presidential election? (About 47%.) 4\nThe second of our four types of claim we will investigate is a generalization.\nSo far, we have examined data sets and summarized them, both numerically and visually. We have looked at data distributions, and associations between variables. Can we extend the conclusions that we make about the data sets to larger populations? If we notice that bill length and flipper length have a strong linear relationship for the penguins in our data, can we say this is true about all penguins? How do we draw valid conclusions about the population our data was drawn from? These are the kinds of questions we we will study using tools from probability theory.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Generalization"
    ]
  },
  {
    "objectID": "3-generalization/notes.html#footnotes",
    "href": "3-generalization/notes.html#footnotes",
    "title": "Generalization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPaper is at https://www.astro.sunysb.edu/fwalter/AST248/why_i_dont_have_a_girlfriend.pdf and a talk by Backus at https://www.youtube.com/watch?v=ClPPSry8bBw↩︎\nhttps://berkeleytime.com/grades/0-7077-all-all&1-7077-fall-2022-all↩︎\nhttps://www.freep.com/betting/sports/nfl-49ers-vs-chiefs-odds-moneylines-spreads-totals-best-nfl-odds-this-week↩︎\nhttps://www.thelines.com/odds/election/↩︎",
    "crumbs": [
      "Notes",
      "Generalization",
      "Generalization"
    ]
  },
  {
    "objectID": "3-generalization/01-prob-foundations/slides.html#probability-of-two-events",
    "href": "3-generalization/01-prob-foundations/slides.html#probability-of-two-events",
    "title": "Probability Foundations",
    "section": "Probability of two events…",
    "text": "Probability of two events…\n\nxkcd comic showing two people discussing what it means to have a 50-50 chancehttps://imgs.xkcd.com/comics/prediction.png\n\nMaybe a bit of discussion here about situations in which two things can happen, but what do they think about the chance of each eg. coin toss, even/odd number on a die roll, winning the lottery vs not winning, getting an A in stat 20 vs not etc."
  },
  {
    "objectID": "3-generalization/01-prob-foundations/slides.html#rules-of-probability",
    "href": "3-generalization/01-prob-foundations/slides.html#rules-of-probability",
    "title": "Probability Foundations",
    "section": "Rules of probability",
    "text": "Rules of probability\nLet \\(\\Omega\\) be the outcome space, and let \\(P(A)\\) denote the probability of the event \\(A\\). Then we have:\n\n\n\\(P(A) \\ge 0\\)\n\n\n\n\n\\(P(\\Omega) = 1\\)\n\n\n\n\nIf \\(A\\) and \\(B\\) are mutually exclusive (\\(A \\cap B = \\{\\}\\)), then \\(P(A \\cup B) = P(A) + P(B)\\)\n\n\nConcepts to review: (KEEP THIS BRIEF, OR INCORPORATE INTO CQ’s) - The first two rules of probability - unions and intersections - mutually exclusive events and the addition rule - good idea to draw Venn diagrams here - Use rule 3 to write down the complement rule, and show what A^C means"
  },
  {
    "objectID": "3-generalization/01-prob-foundations/slides.html#the-linda-problem",
    "href": "3-generalization/01-prob-foundations/slides.html#the-linda-problem",
    "title": "Probability Foundations",
    "section": "The Linda Problem",
    "text": "The Linda Problem\n\n  \n    −\n    +\n \n 01:00\n \nThe Linda problem is from a very famous experiment conducted by Daniel Kahneman and Amos Tversky in 1983 (The version below is from the book Thinking, Fast and Slow by Kahneman, page 156):\nLinda is thirty-one years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in antinuclear demonstrations.\n\nWhich alternative is more probable?\n\n\nLinda is a bank teller.\nLinda is a bank teller and is active in the feminist movement.\n\n\nCorrect answer: (a) Depending on the response, you can discuss how even though (b) is clearly contained in (a) and therefore has lower probability, an overwhelming majority of their respondents ranked (b) as more likely. “About 85% to 90% of undergraduates at several major universities chose the second option, contrary to logic”, and talk about why this is so. Probability can be tricky and counter-intuitive. If they do well, congratulate them and say that they are among the rare people who understand that \\(P(A \\textbf{ and } B)\\) must be lower than \\(P(A)\\).\n\nKahneman, Daniel. Thinking, Fast and Slow (p. 158). Farrar, Straus and Giroux."
  },
  {
    "objectID": "3-generalization/01-prob-foundations/slides.html#coin-tosses-10-times",
    "href": "3-generalization/01-prob-foundations/slides.html#coin-tosses-10-times",
    "title": "Probability Foundations",
    "section": "Coin tosses: 10 times",
    "text": "Coin tosses: 10 times\n\nWe can simulate coin tosses and see if the simulations justify our intuitive understanding of what happens when we toss a fair coin. Go over code and review set.seed() and sample(). Maybe change arguments and see what happens.\n\n\nset.seed(12345)\n\ncoin &lt;- c(\"Heads\", \"Tails\")\ntosses &lt;- sample(coin, 10, replace = TRUE)\ndata.frame(tosses) |&gt;\n  group_by(tosses) |&gt; \n  summarise(n = n())\n\n# A tibble: 2 × 2\n  tosses     n\n  &lt;chr&gt;  &lt;int&gt;\n1 Heads      3\n2 Tails      7"
  },
  {
    "objectID": "3-generalization/01-prob-foundations/slides.html#coin-tosses-50-times",
    "href": "3-generalization/01-prob-foundations/slides.html#coin-tosses-50-times",
    "title": "Probability Foundations",
    "section": "Coin tosses: 50 times",
    "text": "Coin tosses: 50 times\n\nComment on the fact that it is not a 50-50 split.\n\n\nset.seed(12345)\n\ntosses &lt;- sample(coin, 50, replace = TRUE)\ndata.frame(tosses) |&gt;\n  group_by(tosses) |&gt; \n  summarise(n = n())\n\n# A tibble: 2 × 2\n  tosses     n\n  &lt;chr&gt;  &lt;int&gt;\n1 Heads     15\n2 Tails     35"
  },
  {
    "objectID": "3-generalization/01-prob-foundations/slides.html#coin-tosses-500-times",
    "href": "3-generalization/01-prob-foundations/slides.html#coin-tosses-500-times",
    "title": "Probability Foundations",
    "section": "Coin tosses: 500 times",
    "text": "Coin tosses: 500 times\n\nMaybe tossing five hundred times will improve the split:\n\n\nset.seed(12345)\n\ntosses &lt;- sample(coin, 500, replace = TRUE)\ndata.frame(tosses) |&gt;\n  group_by(tosses) |&gt; \n  summarise(n = n())\n\n# A tibble: 2 × 2\n  tosses     n\n  &lt;chr&gt;  &lt;int&gt;\n1 Heads    251\n2 Tails    249\n\n\nWe see that as the number of tosses increases, the split of heads and tails begins to look closer to 50-50."
  },
  {
    "objectID": "3-generalization/01-prob-foundations/slides.html#looking-at-proportions",
    "href": "3-generalization/01-prob-foundations/slides.html#looking-at-proportions",
    "title": "Probability Foundations",
    "section": "Looking at proportions:",
    "text": "Looking at proportions:\nHere is a plot of the proportion of tosses that land heads when we toss a coin \\(n\\) times, where \\(n\\) varies from \\(1\\) to \\(1000\\).\n\nBe sure to explain what they are looking at. We see that at the beginning, for a small number of tosses, the proportion of times that the coin lands heads is all over the place, but it eventually settles down to be around 0.5. This verifies our intuition that if we toss a fair coin, the proportion of times that the coin lands heads should be about 0.5. This idea of the probability of a particular outcome as a long run proportion of times that we see that outcome is called the frequentist theory of probability, and we will be using this theory in our class. (A different theory of probability uses a subjective notion of probability, but we won’t get into that at this time.) We will think about the probability of heads as the long-run relative frequency, or the proportion of times the coin will land heads if we toss it many, many times. This fits with our intuition that if we have a fair coin, that means that each of the two possible outcomes will occur roughly the same number of times when we toss it over and over again. This is the justification for calling the two outcomes equally likely and allowing us to define the probability of heads to be 1/2."
  },
  {
    "objectID": "3-generalization/01-prob-foundations/slides.html#section",
    "href": "3-generalization/01-prob-foundations/slides.html#section",
    "title": "Probability Foundations",
    "section": "",
    "text": "−\n    +\n \n 01:00\n \nSuppose Ali and Bettina are playing a game, in which Ali tosses a fair coin \\(n\\) times, and Bettina wins one dollar from Ali if the proportion of heads is less than 0.4. Ali lets Bettina decide if \\(n\\) is 10 or 100.\n\nWhich \\(n\\) should Bettina choose?\n\n\nThis ties into the plot of proportions of heads as the number of coin tosses increases. Hopefully Bettina realizes she has a better chance of getting the number of heads far away from 0.5 with fewer tosses."
  },
  {
    "objectID": "3-generalization/01-prob-foundations/slides.html#section-1",
    "href": "3-generalization/01-prob-foundations/slides.html#section-1",
    "title": "Probability Foundations",
    "section": "",
    "text": "Part 1: Suppose we roll a die 4 times. The chance that we see six (the face with six spots) at least once is given by \\(\\displaystyle \\frac{1}{6} +  \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} =  \\frac{4}{6} = \\frac{2}{3}\\)\n\nTrue or false?\n\n\n\nPart 2: Suppose we roll a pair of dice 24 times. The chance that we see a pair of sixes at least once is given by \\(\\displaystyle 24 \\times \\frac{1}{36} =  \\frac{24}{36} = \\frac{2}{3}\\)\n\nTrue or false?\n\n\n  \n    −\n    +\n \n 01:00\n \n\nBoth parts are false since we are using the addition rule on events that are not mutually exclusive. It is important that you do NOT talk about what the actual probability is since that uses the multiplication rule and we have not discussed that in the notes. This is more an exercise in when not to use the addition rule."
  },
  {
    "objectID": "3-generalization/01-prob-foundations/slides.html#section-2",
    "href": "3-generalization/01-prob-foundations/slides.html#section-2",
    "title": "Probability Foundations",
    "section": "",
    "text": "−\n    +\n \n 01:00\n \nConsider the Venn diagram below, which has 20 possible outcomes in \\(\\Omega\\), depicted by the purple dots. Suppose the dots represent equally likely outcomes. What is the probability of \\(A\\) or \\(B\\) or \\(C\\)? That is, what is \\(P(A \\cup B \\cup C)\\)?\n\n\nAsk them what is the quickest way to do this. Maybe even give them only 15 or 30 seconds."
  },
  {
    "objectID": "3-generalization/01-prob-foundations/notes.html",
    "href": "3-generalization/01-prob-foundations/notes.html",
    "title": "Probability Foundations",
    "section": "",
    "text": "In order to be taken seriously, we need to be careful about how we collect data, and then how we generalize our findings. For example, you may have observed that some polling companies are more successful than others in their estimates and predictions, and consequently people pay more attention to them. Below is a snapshot of rankings of polling organizations from the well-known website FiveThirtyEight1, and one can imagine that not many take heed of the polling done by the firms with C or worse grades. According to the website, the rankings are based on the polling organization’s ``historical accuracy and methodology”.\nIn order to make estimates as these polling organizations are doing, or understand the results of a clinical trial, or other such questions in which we generalize from our data sample to a larger group, we have to understand the variations in data introduced by randomness in our sampling methods. Each time we poll a different group of voters, for example, we will get a different estimate of the proportion of voters that will vote for Joe Biden in the next election. To understand variation, we first have to understand how probability was used to collect the data.\nSince classical probability came out of gambling games played with dice and coins, we can begin our study by thinking about those.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Foundations"
    ]
  },
  {
    "objectID": "3-generalization/01-prob-foundations/notes.html#de-mérés-paradox",
    "href": "3-generalization/01-prob-foundations/notes.html#de-mérés-paradox",
    "title": "Probability Foundations",
    "section": "De Méré’s Paradox",
    "text": "De Méré’s Paradox\n\n\n\n\n\nIn 17th century France, gamblers would bet on anything. In particular, they would bet on a fair six-sided die landing 6 at least once in four rolls. Antoine Gombaud, aka the Chevalier de Méré, was a gambler who also considered himself something of a mathematician. He computed the chance of a getting at least one six in four rolls as 2/3 \\((4 \\times (1/6) = 4/6)\\). He won quite often by betting on this event, and was convinced his computation was correct. Was it?\nThe next popular dice game was betting on at least one double six in twenty-four rolls of a pair of dice. De Méré knew that there were 36 possible outcomes when rolling a pair of dice, and therefore the chance of a double six was 1/36. Using this he concluded that the chance of at least one double six in 24 rolls was the same as that of at least one six in four rolls, that is, 2/3 (\\(24 \\times 1/36\\)). He happily bet on this event (at least one double six in 24 rolls) but to his shock, lost more often than he won! What was going on?\nWe will see later how to compute this probability, but for now we can estimate the value by simulating the game many times (1000 times each) and looking at the proportion of times we see at least one six in 4 rolls of a fair die, and do the same with at least one double six in 24 rolls.\n\n\nNumber of simulations = 1000\n\n\n  prop_wins_game_1\n1            0.514\n\n\n  prop_wins_game_2\n1            0.487\n\n\nYou can see here that the poor Chevalier wasn’t as good a mathematician as he imagined himself to be, and didn’t compute the chances correctly. The simulated probabilities are nowhere close to 4/6 and 2/3, the probabilities that he computed for the first and second game, respectively. 🧐\nBy the end of this unit, you’ll be able to conduct simulations like these yourself in R! For today, we are going to begin by introducing the conceptual building blocks behind probability.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Foundations"
    ]
  },
  {
    "objectID": "3-generalization/01-prob-foundations/notes.html#basics",
    "href": "3-generalization/01-prob-foundations/notes.html#basics",
    "title": "Probability Foundations",
    "section": "Basics",
    "text": "Basics\nFirst, let’s establish some terminology:\n\n\nExperiment\n\nAn action, involving chance, that can result in a finite number of possible outcomes (results of the experiment). For example, a coin toss is an experiment, and the possible outcomes are the coin landing heads or tails.\n\n\n\n\n\nOutcome space\n\nThis is just a set. It is the collection of all the possible outcomes of an experiment is called an outcome space or sample space, and we denote it by the upper case Greek letter \\(\\Omega\\) (``Omega”). For example, if we toss a coin, then the corresponding outcome space is \\(\\Omega = \\{\\text{Heads, Tails}\\}\\). If we roll a die, then the corresponding outcome space \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\). We will denote a set by enclosing the elements of the set in braces: \\(\\{ \\}\\).\n\n\n\n\n\nEvent\n\nA collection of outcomes as a result of the experiment being performed, perhaps more than once. For example, we could toss a coin twice, and consider the event of both tosses landing heads. We usually denote events by upper case letters from the beginning of the alphabet: \\(A, B, C, \\ldots\\). An event is a subset of the outcome space, and we denote this by writing \\(A \\subset \\Omega\\).\n\n\n\n\n\nP(A)\n\nFor any event \\(A\\), we write the probability of \\(A\\) as \\(P(A)\\).\n\n\n\n\n\nEqually likely outcomes\n\nWhen all the possible outcomes in a finite outcome space of size \\(n\\) happen with the same probability, which is \\(\\displaystyle \\frac{1}{n}\\).\n\n\nLet’s say that there are \\(n\\) possible outcomes in the outcome space \\(\\Omega\\), and an event \\(A\\) has \\(k\\) possible outcomes out of those \\(n\\). If all the outcomes are equally likely to happen (as in a die roll or coin toss), then we say that the probability of \\(A\\) occuring is \\(\\displaystyle \\frac{k}{n}\\).\n\\[\nP(A) = \\frac{k}{n}\n\\]\n\n\nExample: Tossing a fair coin\n\n\n\n\n\nSuppose we toss a fair coin, and I ask you what is the chance of the coin landing heads. Like most people, you reply 50%. Why? Well… (you reply) there are two possible things that can happen, and if the coin is fair, then they are both equally likely, so the probability of heads is 1/2 or 50%.\nHere, we have thought about an event (the coin landing heads), seen that there is one outcome in that event, and two outcomes in the outcome space, so we say the probability of the event, \\(P(\\text{Heads})\\), is 1/2.\n\n\nExample: Tossing a fair six-sided die2\nConsider rolling a fair six-sided die: six outcomes are possible so \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\). Since the die is fair, each outcome is equally likely, with probability \\(= \\displaystyle \\frac{1}{6}\\). We can list the outcomes and their probabilities in a table.\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\\(4\\)\n\\(5\\)\n\\(6\\)\n\n\n\n\nProbability\n\\(\\displaystyle \\frac{1}{6}\\)\n\\(\\displaystyle \\frac{1}{6}\\)\n\\(\\displaystyle \\frac{1}{6}\\)\n\\(\\displaystyle \\frac{1}{6}\\)\n\\(\\displaystyle \\frac{1}{6}\\)\n\\(\\displaystyle \\frac{1}{6}\\)\n\n\n\nLet \\(A\\) be the event that an even number is rolled. Then the set \\(A\\) can be written \\(\\{2,4,6\\}\\). Since all of these outcomes are equally likely:\n\\[P(A) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{3}{6}\\]",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Foundations"
    ]
  },
  {
    "objectID": "3-generalization/01-prob-foundations/notes.html#axioms-of-probability",
    "href": "3-generalization/01-prob-foundations/notes.html#axioms-of-probability",
    "title": "Probability Foundations",
    "section": "Axioms of probability",
    "text": "Axioms of probability\nIn order to compute the probabilities of events, we need to set some basic mathematical rules called axioms (which are intuitively clear if you think of the probability of an event as the proportion of the outcomes that are in it). There are three basic rules that will help us compute probabilities:\n\nAxiom 1\n\nThe chance of any event is at least \\(0\\): \\(P(A) \\ge 0\\) for any event \\(A\\).\n\nAxiom 2\n\nThe chance of an outcome being in \\(\\Omega\\) is \\(1\\): \\(P(\\Omega) = 1\\). This is true because we can consider that the probability of \\(\\Omega\\) is the number of outcomes in \\(\\Omega\\) divided by \\(n\\), which is \\(n/n = 1\\).\n\n\nBefore we write the third rule, we need some more definitions and notation:\n\n\nImpossible event\n\nAn event with no outcomes in it. Denoted by either empty braces \\(\\{\\}\\) or the symbol for the empty set \\(\\emptyset\\). The probability of the impossible event is \\(0\\).\n\n\n\n\n\nUnion of events\n\nGiven events \\(A\\), \\(B\\), we can define a new event called \\(A\\) or \\(B\\), which consists of all the outcomes that are either in \\(A\\) or in \\(B\\) or in both. This is also written as \\(A \\cup B\\), read as ``\\(A\\) union \\(B\\)’’.\n\n\n\n\n\nIntersection of events\n\nGiven events \\(A\\), \\(B\\), we can define a new event called \\(A\\) and \\(B\\), which consists of all the outcomes that are both in \\(A\\) and in \\(B\\). This is also written as \\(A \\cap B\\), read as ``\\(A\\) intersect \\(B\\)’’.\n\n\n\nNow we consider events that don’t intersect or overlap at all, that is, they are disjoint from each other, or mutually exclusive:\n\n\nMutually exclusive events\n\nIf two events \\(A\\) and \\(B\\) do not overlap, that is, they have no outcomes in common, we say that the events are mutually exclusive.\n\n\nIf \\(A\\) and \\(B\\) are mutually exclusive, then we know that if one of them happens, the other one cannot. We denote this by writing \\(A \\cap B = \\emptyset\\) and read this as \\(A\\) intersect \\(B\\) is empty. Therefore, we have that\n\\[P(A \\cap B) = P(\\emptyset) = 0\\].\n\nFor example, if we are playing De Méré’s second game, the event \\(A\\) that we roll a pair of sixes and the event \\(B\\) that we roll a pair of twos cannot happen on the same roll. These events \\(A\\) and \\(B\\) are mutually exclusive.\n\nHowever, if we roll a die, the event \\(C\\) that we roll an even number and the event \\(D\\) that we roll a prime number are not mutually exclusive, since the number 2 is both even and prime.\n\nHere’s another example that might interest soccer fans: The event that Manchester City wins the English Premier League (EPL) in 2024, and the event that Liverpool wins the EPL in 2024 are mutually exclusive, but the events that Manchester City are EPL champions in 2024 and Manchester City are EPL champions in 2023 are not mutually exclusive.\nNow for the third axiom:\n\nAxiom 3\n\nIf \\(A\\) and \\(B\\) are mutually exclusive (\\(A \\cap B = \\emptyset\\)), then\n\n\n\\[P(A \\cup B) = P(A) + P(B)\\] That is, for two mutually exclusive events, the probability that either of the two events might occur is the sum of their probabilities. This is called the addition rule.\nFor example, consider rolling a fair six-sided die, and the two events \\(A\\) and \\(B\\), where \\(A\\) is the event of rolling a multiple of \\(5\\), and \\(B\\) is the event that we roll a multiple of \\(2\\).\nThe only outcome in \\(A\\) is \\(\\{5\\}\\), while \\(B\\) consists of \\(\\{2, 4, 6\\}\\). \\(P(A) = 1/6\\), and \\(P(B) = 3/6\\). Since \\(A \\cap B =\\emptyset\\), that is, \\(A\\) and \\(B\\) have no outcomes in common, we have that\n\\[P(A \\cup B) = P(A) + P(B) = \\frac{1}{6} + \\frac{3}{6} = \\frac{4}{6}\\]\n\nThe complement rule\nHere is an important consequence of axiom 3. Let \\(A\\) be an event in \\(\\Omega\\). The complement of \\(A\\), written as \\(A^C\\), consists of all those outcomes in \\(\\Omega\\) that are not in \\(A\\). Then we have the following rule:\n\\[P(A) + P(A^C) =  1\\]\nThis is because \\(A  \\cup A^C = \\Omega\\), and \\(A  \\cap A^C \\emptyset\\)).\n\n\nAn example with the axioms: penguins\nConsider the penguins dataset, which has 344 observations, of which 152 are Adelie penguins and 68 are Chinstrap penguins. Suppose we pick a penguin at random, what is the probability that we would pick an Adelie penguin? What about a Gentoo penguin?\n\n\nCheck your answer\n\nLet \\(A\\) be the event of picking an Adelie penguin, \\(C\\) be the event of picking a Chinstrap penguin, and \\(G\\) be the event of picking a Gentoo penguin.\nAssuming that all the penguins are equally likely to be picked, we see that then \\(P(A) = 152/344\\), and \\(P(C) = 68/344\\).\nSince only one penguin is picked, we see that \\(A, C\\), and \\(G\\) are mutually exclusive. This means that \\(P(A)+P(C)+P(G) = 1\\), since \\(A, C\\), and \\(G\\) together make up all of \\(\\Omega\\).\nTherefore the complement of \\(G\\), \\(G^C\\), which is a penguin that is not Gentoo, consists of Adelie and Chinstrop penguins, and by the addition rule,\n\\[P(G^C) = P(A \\cup C) = P(A) + P(C) = (152+68)/344 = 220/344\\]\nFinally, the complement rule tells us that\n\\[P(G) = 1 - P(G^C) = 1 - 220/344 = 124/344\\].\n\n\n\n\n\n\n\nWARNING!!\n\n\n\nWe use \\(A\\) to denote an event or a set, while P(A) is a number - you can think of \\(P(A)\\) as representing the relative size of \\(A\\). This means that the following types of statements don’t make sense as we haven’t defined what it means to add sets or union numbers etc.:\n\n\\(P(A) \\cup P(B)\\) or \\(P(A) \\cap P(B)\\)\n\\(A + B\\), or \\(A - B\\), or \\(A \\times B\\) etc",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Foundations"
    ]
  },
  {
    "objectID": "3-generalization/01-prob-foundations/notes.html#venn-diagrams",
    "href": "3-generalization/01-prob-foundations/notes.html#venn-diagrams",
    "title": "Probability Foundations",
    "section": "Venn Diagrams",
    "text": "Venn Diagrams\nWe often represent events using Venn diagrams. The outcome space \\(\\Omega\\) is usually represented as a rectangle, and events are represented as circles inside \\(\\Omega\\). Here is a Venn diagram showing two events \\(A\\) and \\(B\\), their intersection, and their union:\n\n\n\n\n\nHere is a Venn diagram showing two mutually exclusive events (no overlap):",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Foundations"
    ]
  },
  {
    "objectID": "3-generalization/01-prob-foundations/notes.html#further-examples",
    "href": "3-generalization/01-prob-foundations/notes.html#further-examples",
    "title": "Probability Foundations",
    "section": "Further examples",
    "text": "Further examples\n\n1. Tossing a fair coin\nSuppose we toss a coin twice and record the equally likely outcomes. What is \\(\\Omega\\)? What is the chance of at least one head?\nSolution: \\(\\Omega = \\{HH, HT, TH, TT\\}\\), where \\(H\\) represents the coin landing heads, and \\(T\\) represents the coin landing tails. Note that since we can get exactly one head and one tail in two ways, we have to write out both ways so that all the outcomes are equally likely.\nNow, let \\(A\\) be the event of getting at least one head in two tosses. We can do this by listing the outcomes in \\(A\\): \\(A = \\{HH, HT, TH\\}\\) and so \\(P(A) = 3/4\\).\nAlternatively, we can consider \\(A^C\\) which is the event of no heads, so \\(A^C = \\{TT\\}\\) and \\(P(A^C) = 1/4\\).\nIn this case, \\(P(A) = 1- P(A^C) = 1-1/4 = 3/4\\).\nNow you try: Let \\(\\Omega\\) be the outcome space of tossing a coin three times. What is the probability of at least one head? What about exactly one head?\n\n\nCheck your answer\n\n\\(\\Omega = \\{HHH, HHT, HTH, THH, HTT, THT, TTH, TTT \\}\\).\nLet \\(A\\) be the event of at least one head. Then \\(A^C\\) is the event of no heads, so \\(A^C = \\{TTT\\}\\), and \\(P(A^C) = 1/8\\). Therefore \\(P(A) = 1-1/8 = 7/8\\). Note that this is much quicker than listing and counting the outcomes in \\(A\\).\nIf \\(B\\) is the event of exactly one head, then \\(B = \\{HTT, THT, TTH\\}\\) and \\(P(B) = 3/8\\).\n\n\n\n2. A box of tickets\n\n\n\n\n\nConsider the box above which has five almost identical tickets. The only difference is the value written on them. Imagine that we shake the box to mix the tickets up, and then draw one ticket without looking so that all the tickets are equally likely to be drawn3.\nWhat is the chance of drawing an even number?\n\n\nCheck your answer\n\nSolution:\nLet \\(A\\) be the event of drawing an even number, then \\(A = \\{2, 2, 4\\}\\): we list 2 twice because there are two tickets marked 2, making it twice as likely as any other number. \\(P(A) = 3/5\\)\n\n\n\n\n\n\n3. Tossing a biased coin\nSuppose I have a coin that is twice as likely to land heads as it is to land tails. This means that I cannot represent \\(\\Omega\\) as \\(\\{H, T\\}\\) since heads and tails are not equally likely. How should I write \\(\\Omega\\) so that the outcomes are equally likely?\n\n\nCheck your answer\n\nSolution:\nIn this case, we want to represent equally likely outcomes, and want \\(H\\) to be twice as likely as \\(T\\). We can therefore represent \\(\\Omega\\) as \\(\\{H, H, T \\}\\). Now the chance of the coin landing \\(H\\) can be written as \\(P(A)\\) where \\(A\\) is the event the coin lands \\(H\\) is given by 2/3.\nSuppose we toss the coin twice. How would we list the outcomes so that they are equally likely? Now we have to be careful, and think about all the things that can happen on the second toss if we have \\(H\\) on the first toss.\nThis is much easier to imagine if we imagine drawing twice from a box of tickets, but putting the first ticket back before drawing the second (to represent the fact that the probabilities of landing \\(H\\) or \\(T\\) stay the same on the second toss.)\nNow, imagine the box of tickets that represents \\(\\Omega\\) to be \\(\\fbox{H, H, T}\\). We draw one ticket at first, which could be one of three tickets (there are two tickets that could be \\(H\\), and one \\(T\\)). We can represent it using the following picture:\n\n\n\n\n\nFrom this picture, where we use color to distinguish the two different outcomes of heads and one outcome of tails, we can see that there are 9 possible outcomes that are equally likely, and we get the following probabilities (where \\(HT\\), for example, represents the event that the first toss is heads, followed by the second toss being tails.)\n\\(P(HH) = 4/9,\\; P(HT) = P(TH) = 2/9, P(TT) = 1/9\\) (Check that the probabilities sum to 1!)\n\n\n\n\n\n\nAsk yourself\n\n\n\nWhat box would we use if the coin is not a fair coin, but lands heads \\(5\\) out of \\(6\\) times?\n\n\n\n\n\n4. Betting on red in roulette\n\n\n\n\n\nAn American roulette wheel has 38 pockets4, of which 18 are red, 18 black, and 2 are green. The wheel is spun, and a small ball is thrown on the wheel so that it is equally likely to land in any of the 38 pockets. Players bet on which colored or numbered pocket the ball will come to rest in. If you bet one dollar that the ball will land on red, and it does, you get your dollar back, and you win one more dollar, so your net gain is $1. If it doesn’t, and lands on a black or green number, you lose your dollar, and your net “gain” is -$1.\nWhat is the chance that we will win one dollar on a single spin of the wheel?\nHint Write out the chance of the ball landing in a red pocket, and not landing in a red pocket.",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Foundations"
    ]
  },
  {
    "objectID": "3-generalization/01-prob-foundations/notes.html#summary",
    "href": "3-generalization/01-prob-foundations/notes.html#summary",
    "title": "Probability Foundations",
    "section": "Summary",
    "text": "Summary\n\nIn this lecture, we introduced equally likely outcomes,and defined the outcome space of an experiment.\nThen, using equally likely outcomes, we defined the probability of an event as the ratio of the number of outcomes in the event to the number of total outcomes in the outcome space.\nWe wrote down the axioms (fundamental rules) of probability, after defining unions, intersections, and mutually exclusive events and Venn diagrams.\nIn the “Ideas in Code” section, explored how to simulate probabilities using sample() and replicate(), and learned another useful function seq()",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Foundations"
    ]
  },
  {
    "objectID": "3-generalization/01-prob-foundations/notes.html#footnotes",
    "href": "3-generalization/01-prob-foundations/notes.html#footnotes",
    "title": "Probability Foundations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis website was begun as poll aggregation site, by the statistician Nate Silver.↩︎\nThe singular is die and the plural is dice. If we use the word “die” without any qualifiers, we will mean a fair, six-sided die.↩︎\nWe call the tickets equally likely when each ticket has the same chance of being drawn. That is, if there are \\(n\\) tickets in the box, each has a chance of \\(1/n\\) to be drawn. We also refer to this as drawing a ticket uniformly at random, because the chance of drawing the tickets are the same, or uniform.↩︎\nPhoto via unsplash.com↩︎",
    "crumbs": [
      "Notes",
      "Generalization",
      "Probability Foundations"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.\nThe source materials for the curriculum for this course and the software that builds this website are available under at:\nhttps://github.com/stat20/stat20"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Lab assignments are an opportunity to put the concepts from the notes into practice to answer questions about a real data set. Your lab report should be a pdf file generated from a fully reproducible qmd file. For a helpful R reference, see base R, data visualization (ggplot2), and data wrangling (dplyr).\n\n\n\n\n\n\n\n\nLab 1: UN Votes\n\n  \n    \n      Part II\n    \n  \n\n\n\n\n\n\n\n\n\nLab 2: Class Survey\n\n  \n    \n      Slides\n    \n  \n\n  \n    \n      Part I\n    \n  \n\n  \n    \n      Part II\n    \n  \n\n\n\n\n\n\n\n\n\nLab 3: Flights\n\n  \n    \n      Slides\n    \n  \n\n  \n    \n      Part I\n    \n  \n\n  \n    \n      Part II\n    \n  \n\n  \n    \n      Part II\n    \n  \n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assignments.html#labs",
    "href": "assignments.html#labs",
    "title": "Assignments",
    "section": "",
    "text": "Lab assignments are an opportunity to put the concepts from the notes into practice to answer questions about a real data set. Your lab report should be a pdf file generated from a fully reproducible qmd file. For a helpful R reference, see base R, data visualization (ggplot2), and data wrangling (dplyr).\n\n\n\n\n\n\n\n\nLab 1: UN Votes\n\n  \n    \n      Part II\n    \n  \n\n\n\n\n\n\n\n\n\nLab 2: Class Survey\n\n  \n    \n      Slides\n    \n  \n\n  \n    \n      Part I\n    \n  \n\n  \n    \n      Part II\n    \n  \n\n\n\n\n\n\n\n\n\nLab 3: Flights\n\n  \n    \n      Slides\n    \n  \n\n  \n    \n      Part I\n    \n  \n\n  \n    \n      Part II\n    \n  \n\n  \n    \n      Part II\n    \n  \n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assignments.html#worksheets",
    "href": "assignments.html#worksheets",
    "title": "Assignments",
    "section": "Worksheets",
    "text": "Worksheets\nWorksheets are pen-and-paper practice problems that we work on in class. The goal of the worksheets is simply practice: they help you drill the techniques needed to complete the labs.\n\n\n\nWS: Taxonomy of Data\n\n\nWS: Summarizing Categorical Data\n\n\nWS: Summarizing Numerical Data\n\n\nWS: A Grammar of Graphics\n\n\nWS: Conditioning\n\n\nWS: Summarizing Numerical Associations\n\n\nWS: Summarizing Numerical Associations\n\n\nWS: Probability Foundations\n\n\nWS: Computing Probabilities\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Navigate the notes using the menu in the left sidebar or the search in the upper right corner of the page.",
    "crumbs": [
      "Notes"
    ]
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-1",
    "href": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-1",
    "title": "Multiple Linear Regression",
    "section": "Question 1",
    "text": "Question 1\n\nm1 &lt;- lm(bill_depth_mm ~ bill_length_mm, data = penguins)\n\n\n\nm2 &lt;- lm(bill_depth_mm ~ bill_length_mm + body_mass_g + species, \n         data = penguins)\n\n\nHow many more coefficients does the second model have than the first?\n\n\nRemind students that they need to remember whether species and body_mass_g are numerical or categorical. Students should know how many species there are (three).\nOne addl. coefficient for body mass; two addl. for species (one less than the number of species, which is three). This gives a total of three more coefficients."
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-2",
    "href": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-2",
    "title": "Multiple Linear Regression",
    "section": "Question 2",
    "text": "Question 2\n\n\n\n−+\n01:00\n\n\n\n\n\n\nm2\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + body_mass_g + species, \n    data = penguins)\n\nCoefficients:\n     (Intercept)    bill_length_mm       body_mass_g  speciesChinstrap  \n        10.33083           0.09484           0.00117          -0.90748  \n   speciesGentoo  \n        -5.80117  \n\n\n\nWhich is the correct interpretation of the coefficient in front of bill length? Select all that apply.\n\n\nThis one assesses their ability to use a conditional interpretation of a regression coefficient (controlling for the other variables in the model…).\nThe correct interpretation controls for both other variables (body mass and species), and has the x variable and the y variable in the correct places."
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-3",
    "href": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-3",
    "title": "Multiple Linear Regression",
    "section": "Question 3",
    "text": "Question 3\n\n\n\n−+\n01:00\n\n\n\n\n\n\nm2\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + body_mass_g + species, \n    data = penguins)\n\nCoefficients:\n     (Intercept)    bill_length_mm       body_mass_g  speciesChinstrap  \n        10.33083           0.09484           0.00117          -0.90748  \n   speciesGentoo  \n        -5.80117  \n\n\n\nWhich is the correct interpretation of the coefficient in front of Gentoo?\n\n\nThis one assesses their ability to use a conditional interpretation of a regression coefficient (controlling for the other variables in the model…)\nThe correct interpretation needs to involve a comparison to the reference level, Adelie and should not involve anything about bill length or body mass, (other than to say they should be fixed)."
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-4",
    "href": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-4",
    "title": "Multiple Linear Regression",
    "section": "Question 4",
    "text": "Question 4\n\n\n\n−+\n01:00\n\n\n\n\n\n\nm2\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + body_mass_g + species, \n    data = penguins)\n\nCoefficients:\n     (Intercept)    bill_length_mm       body_mass_g  speciesChinstrap  \n        10.33083           0.09484           0.00117          -0.90748  \n   speciesGentoo  \n        -5.80117  \n\n\n\nHow would this linear model best be visualized?\n\n\nThree numerical variables means we will have planes in a 3D space. The coefficients for the dummy variables will shift the planes up and down. There are three species, so three parallel planes is what we are looking for."
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-5",
    "href": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-5",
    "title": "Multiple Linear Regression",
    "section": "Question 5",
    "text": "Question 5\nConsider the following linear regression output where the variable school is categorical and the variable hours_studied is numerical.\n\n\n\nCoefficients\nEstimate\n\n\n\n\n(Intercept)\n2.5\n\n\nhours_studied\n.2\n\n\nschoolCal\n1\n\n\nschoolStanford\n-1"
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-5-cont.",
    "href": "2-summarizing-data/06-multiple-linear-regression/slides.html#question-5-cont.",
    "title": "Multiple Linear Regression",
    "section": "Question 5 (cont.)",
    "text": "Question 5 (cont.)\n\nSay I wanted to create a data frame from the original edu dataframe which contains the minimum, median, and IQR for hours_studied among each school. In order to do this, I make use of group_by() followed by summarize(). I save this data frame into an object called GPA_summary.\n\n\n\nWhat are the dimensions of GPA_summary?\n\n\n\n\n\n\n−+\n01:00\n\n\n\n\n\n\nThe correct answer in the poll should be 3x4. The three rows are for the three levels in the school category (there is an additional level beyond \"Cal\" and \"Stanford\", regardless if it is not stated). One column is for the school name, the other three are for each of the statistics calculated; the three rows are for the three levels in the school category."
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/notes.html",
    "href": "2-summarizing-data/06-multiple-linear-regression/notes.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "In the last lecture we built our first linear model: an equation of a line drawn through the scatter plot.\n\\[ \\hat{y} = 96.2 + -0.89 x \\]\nWhile the idea is simple enough, there is a sea of terminology that floats around this method. A linear model is any model that explains the \\(y\\), often called the response variable or dependent variable, as a linear function of the \\(x\\), often called the explanatory variable or independent variable. There are many different methods that can be used to decide which line to draw through a scatter plot. The most commonly-used approach is called the method of least squares, a method we’ll look at closely when we turn to prediction. If we think more generally, a linear model fit by least squares is one example of a regression model, which refers to any model (linear or non-linear) used to explain a numerical response variable.\nThe reason for all of this jargon isn’t purely to infuriate students of statistics. Linear models are one of the most widely used statistical tools; you can find them in use in diverse fields like biology, business, and political science. Each field tends to adapt the tool and the language around them to their specific needs.\nA reality of practicing statistics in these field, however, is that most data sets are more complex than the example that we saw in the last notes, where there were only two variables. Most phenomena have many different variables that relate to one another in complex ways. We need a more more powerful tool to help guide us into these higher dimensions. A good starting point is to expand simple linear regression to include more than one explanatory variable!",
    "crumbs": [
      "Notes",
      "Summarization",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/notes.html#multiple-linear-regression",
    "href": "2-summarizing-data/06-multiple-linear-regression/notes.html#multiple-linear-regression",
    "title": "Multiple Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nMultiple Linear Regression\n\nA method of explaining a continuous numerical \\(y\\) variable in terms of a linear function of \\(p\\) explanatory terms, \\(x_i\\). \\[ \\hat{y} = b_0 + b_1x_1 + b_2x_2 + \\ldots +b_px_p \\] Each of the \\(b_i\\) are called coefficients.\n\n\nTo fit a multiple linear regression model using least squares in R, you can use the lm() function, with each additional explanatory variable separated by a +.\nMultiple linear regression is powerful because it has no limit to the number of variables that we can include in the model. While Hans Rosling was able to fit 5 variables into a single graphic, what if we had 10 variables? Multiple linear regression allows us to understand high dimensional linear relationships beyond whats possible using our visual system.\nIn today’s notes, we’ll discuss two specific examples where a multiple linear regression model might be applicable\n\nA scenario involving two numerical variables and one categorical variable\nA scenario involving three numerical variables.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/notes.html#two-numerical-one-categorical",
    "href": "2-summarizing-data/06-multiple-linear-regression/notes.html#two-numerical-one-categorical",
    "title": "Multiple Linear Regression",
    "section": "Two numerical, one categorical",
    "text": "Two numerical, one categorical\nThe Zagat Guide was for many years the authoritative source of restaurant reviews. Their approach was very different from Yelp!. Zagat’s review of a restaurant was compiled by a professional restaurant reviewer who would visit a restaurant and rate it on a 30 point scale across three categories: food, decor, and service. They would also note the average price of a meal and write up a narrative review.\nHere’s an example of a review from an Italian restaurant called Marea in New York City.\n\n\n\n\n\nIn addition to learning about the food scores (27), and getting some helpful tips (“bring your bank manager”), we see they’ve also recorded a few more variables on this restaurant: the phone number and website, their opening hours, and the neighborhood (Midtown).\nYou might ask:\n\nWhat is the relationship between the food quality and the price of a meal at Italian restaurant? Are these two variables positively correlated or is the best Italian meal in New York a simple and inexpensive slice of pizza?\n\nTo answer these questions, we need more data. The data frame below contains Zagat reviews from 168 Italian restaurants in Manhattan.\n\n\n# A tibble: 168 × 6\n   restaurant          price  food decor service geo  \n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1 Daniella Ristorante    43    22    18      20 west \n 2 Tello's Ristorante     32    20    19      19 west \n 3 Biricchino             34    21    13      18 west \n 4 Bottino                41    20    20      17 west \n 5 Da Umberto             54    24    19      21 west \n 6 Le Madri               52    22    22      21 west \n 7 Le Zie                 34    22    16      21 west \n 8 Pasticcio              34    20    18      21 east \n 9 Belluno                39    22    19      22 east \n10 Cinque Terre           44    21    17      19 east \n# ℹ 158 more rows\n\n\nApplying the taxonomy of data, we see that for each restaurant we have recorded the price of an average meal, the food, decor, and service scores (all numerical variables) as well as a note regarding geography (a categorical nominal variable). geo captures whether the restaurant is located on the east side or the west side of Manhattan1.\nLet’s summarize the relationship between food quality, price, and one categorical variable - geography - using a colored scatter plot.\n\n\n\n\n\n\n\n\n\nIt looks like if you want a very tasty meal, you’ll have to pay for it. There is a moderately strong, positive, and linear relationship between food quality and price. This plot, however, has a third variable in it: geography. The restaurants from the east and west sides are fairly well mixed, but to my eye the points on the west side might be a tad bit lower on price than the points from the east side. I could numerically summarize the relationship between these three variables by hand-drawing two lines, one for each neighborhood.\n\n\n\n\n\nFor a more systematic approach for drawing lines through the center of scatter plots, we need to return to the method of least squares, which is done in R using lm(). In this linear model, we wish to explain the \\(y\\) variable as a function of two explanatory variables, food and geo, both found in the zagat data frame. We can express that relationship using the formula notation.\n\n\n\nCall:\nlm(formula = price ~ food + geo, data = zagat)\n\nCoefficients:\n(Intercept)         food      geowest  \n    -15.970        2.875       -1.459  \n\n\nIt worked . . . or did it? If extend our reasoning from the last notes, we should write this model as\n\\[\\widehat{price} = -15.97 + 2.87 \\times food - 1.45 \\times geo\\]\nWhat does it mean to put a categorical variable, geo, into a linear model? And how do three three numbers translate into the two lines shown above?\n\nIndicator variables\nWhen working with linear models like the one above, the value of the explanatory variable, \\(geowest\\), is multiplied by a slope, 1.45. According to the Taxonomy of Data, arithmetic functions like multiplication are only defined for numerical variables. While that would seem to rule out categorical variables for use as explanatory variables, statisticians have come up with a clever work-around: the dummy variable.\n\nDummy Variable\n\nA variable that is 1 if an observation takes a particular level of a categorical variable and 0 otherwise. A categorical variable with \\(k\\) levels can be encoded using \\(k-1\\) dummuy variables.\n\n\nThe categorical variable geo can be converted into an dummy variable by shifting the question from “Which side of Manhattan are you on?” to “Are you on the west side of Manhattan?” This is a mutate step.\n\n\n# A tibble: 168 × 4\n    food price geo   geowest\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;  \n 1    22    43 west  TRUE   \n 2    20    32 west  TRUE   \n 3    21    34 west  TRUE   \n 4    20    41 west  TRUE   \n 5    24    54 west  TRUE   \n 6    22    52 west  TRUE   \n 7    22    34 west  TRUE   \n 8    20    34 east  FALSE  \n 9    22    39 east  FALSE  \n10    21    44 east  FALSE  \n# ℹ 158 more rows\n\n\nThe new dummy variable geowest is a logical variable, so it has a dual representation as TRUE/FALSE as well as 1/0. Previously, this allowed us to do Boolean algebra. Here, it allows us to include a dummy variable in a linear model.\nWhile you can create dummy variables by hand using mutate, in practice, you will not need to do this. That’s because they are created automatically whenever you put a categorical variable into lm(). Let’s revisit the linear model that we fit above with geowest in the place of geo.\n\\[\\widehat{price} = -15.97 + 2.87 \\times food - 1.45 \\times geowest\\]\nTo understand the geometry of this model, let’s focus on what the fitted values will be for any restaurant that is on the west side. For those restaurants, the geowest dummy variable will take a value of 1, so if we plug that in and rearrange,\n\\[\\begin{eqnarray}\n\\widehat{price} &= -15.97 + 2.87 \\times food - 1.45 \\times 1 \\\\\n&= (-15.97 - 1.45) + 2.87 \\times food \\\\\n&= -17.42 + 2.87 \\times food\n\\end{eqnarray}\\]\nThat is a familiar sight: that is an equation for a line.\nLet’s repeat this process for the restaurants on the east side, where the geowest dummy variable will now take a value of 0.\n\\[\\begin{eqnarray}\n\\widehat{price} &= -15.97 + 2.87 \\times food - 1.45 \\times 0 \\\\\n&= -15.97 + 2.87 \\times food\n\\end{eqnarray}\\]\nThat is also the equation for line.\nIf you look back and forth between these two equations, you’ll notice that they share the same slope and have different y-intercepts. Geometrically, this means that the output of lm() was describing the equation of two parallel lines:\n\none where geowest is 1 (for restaurants on the west side of town)\none where geowest is 0 (for restaurants on the east side of town).\n\nThat means we can use the output of lm() to replace my hand-drawn lines with ones that arise from the method of least squares.\n\n\n\n\n\n\n\n\n\n\n\nReference levels\nOne question you still might have: Why did R include the dummy variable for the west side of town as opposed to the one for the east side?. The answer lies in the type of variable that geo is recorded as in the zagat dataframe. If you look closely at the initial output, you will see that geo is currently designated chr, which is short for character. geo is indeed a categorical variable with two levels: east and west.\nLike in previous settings, R will determine the “order” of levels in a categorical variable registered as a character by way of the alphabet. This means that east will be tagged first and chosen as the reference level: the level of a categorical variable which does not have a dummy variable in the model. If you would like west to be the reference level, then you would need to reorder the levels using factor() inside of a mutate() so that west comes first. This would change the equation that results from then fitting a linear model with lm(), as you can see below!\n\n\n\nCall:\nlm(formula = price ~ food + geo, data = mutate(zagat, geo = factor(geo, \n    levels = c(\"west\", \"east\"))))\n\nCoefficients:\n(Intercept)         food      geoeast  \n    -17.430        2.875        1.459  \n\n\nNow our equation looks a little bit different!\n\\[\\widehat{price} = -17.43 + 2.87 \\times food + 1.46 \\times geoeast\\]\nIn general, if you include a categorical variable with \\(k\\) levels in a regression model, there will be \\(k-1\\) dummy variables (and thus, coefficients) associated with it in the model: one for each level of the variable except the reference level2. Knowing the reference level also helps us interpret dummy variables that are part of the regression equation; we will see this in a moment. For now, let’s move to our second scenario.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/notes.html#three-numerical",
    "href": "2-summarizing-data/06-multiple-linear-regression/notes.html#three-numerical",
    "title": "Multiple Linear Regression",
    "section": "Three numerical",
    "text": "Three numerical\nWhile the standard scatter plot allows us to understand the association between two numerical variables like price and food, to understand the relationship between three numerical variables, we will need to build this scatterplot in 3D3.\n\n\n\n\n\n\nTake a moment to explore this scatter plot4. Can you find the name of the restaurant with very bad decor but pretty good food and a price to match? (It’s Gennaro.) What about the restaurant that equally bad decor but has rock bottom prices that’s surprising given it’s food quality isn’t actually somewhat respectable? (It’s Lamarca.)\nInstead of depicting the relationship between these three variables graphically, let’s do it numerically by fitting a linear model.\n\n\n\nCall:\nlm(formula = price ~ food + decor, data = zagat)\n\nCoefficients:\n(Intercept)         food        decor  \n    -24.500        1.646        1.882  \n\n\nWe can write the corresponding equation of the model as\n\\[ \\widehat{price} = -24.5 + 1.64 \\times food + 1.88 \\times decor \\]\nTo understand the geometry of this model, we can’t use the trick that we did with dummy variables. decor is a numerical variable just like food, so it takes more values than just 0 and 1.\nIndeed this linear model is describing a plane.\n\n\n\n\n\n\nIf you inspect this plane carefully you’re realize that the tilt of the plane is not quite the same in every dimension. The tilt in the decor dimension is just a little bit steeper than that in the food dimension, a geometric expression of the fact that the coefficient in front of decor, 1.88, is just a bit higher than the coefficient in front of food, 1.64.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/notes.html#interpreting-coefficients",
    "href": "2-summarizing-data/06-multiple-linear-regression/notes.html#interpreting-coefficients",
    "title": "Multiple Linear Regression",
    "section": "Interpreting coefficients",
    "text": "Interpreting coefficients\nWhen moving from simple linear regression, with one explanatory variable, to the multiple linear regression, with many, the interpretation of the coefficients becomes trickier but also more insightful.\n\nThree numerical\nMathematically, the coefficient in front of \\(food\\), 1.64, can be interpreted a few different ways:\n\nIt is the difference that we would expect to see in the response variable, \\(price\\), when two Italian restaurants are separated by a food rating of one and they have the same decor rating.\nControlling for \\(decor\\), a one point increase in the food rating is associated with a $1.64 increase in the \\(price\\).\n\nSimilarly for interpreting \\(decor\\): controlling for the quality of the food, a one-point increase in \\(decor\\) is associated with a $1.88 increase in the \\(price\\).\n\n\nTwo numerical, one categorical\nThis conditional interpretation of the coefficients extends to the first setting we looked at, when one variable is numerical and the other is a dummy variable. Here is that model:\n\\[\\widehat{price} = -15.97 + 2.87 \\times food - 1.45 \\times geowest\\]\nOne might interpret \\(food\\) like this:\n\nFor two restaurants both on the same side of Manhattan, a one point increase in food score is associated with a $2.87 increase in the price of a meal.\n\nAs for \\(geowest\\):\n\nFor two restaurants with the exact same quality of food, the restaurant on the west side is expected to be $1.45 cheaper than the restaurant on the east side.\n\nWe make the comparison to the the east side since this level is the reference level according to the linear model shown. This is a useful bit of insight - it gives a sense of what the premium is of being on the eastside.\nIt is also visible in the geometry of the model. When we’re looking at restaurants with the same food quality, we’re looking at a vertical slice of the scatter plot. Here the vertical gray line is indicating restaurants where the food quality gets a score of 18. The difference in expected price of meals on the east side and west side is the vertical distance between the red line and the blue line, which is exactly 1.45. We could draw this vertical line anywhere on the graph and the distance between the red line and the blue will still be exactly 1.45.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/notes.html#summary",
    "href": "2-summarizing-data/06-multiple-linear-regression/notes.html#summary",
    "title": "Multiple Linear Regression",
    "section": "Summary",
    "text": "Summary\nWe began this unit on Summarizing Data with graphical and numerical summaries of just a single variable: histograms and bar charts, means and standard deviations. In the last set of notes we introduced our first bivariate numerical summaries: the correlation coefficient, and the linear model. In these notes, we introduced multiple linear regression, a method that can numerically describe the linear relationships between an unlimited number of variables. The types of variables that can be included in these models is similarly vast. Numerical variables can be included directly, generalizing the geometry of a line into a plane in a higher dimension. Categorical variables can be included using the trick of creating dummy variables: logical variables that take a value of 1 where a particular condition is true. The interpretation of all of the coefficients that result from a multiple regression is challenging but rewarding: it allows us to answer questions about the relationship between two variables after controlling for the values of other variables.\nIf this felt like a deep dive into a multiple linear regression, don’t worry. Linear models are one of the most commonly used statistical tools, so we’ll be revisiting them throughout the course: investigating their use in making generalizations, causal claims, and predictions.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2-summarizing-data/06-multiple-linear-regression/notes.html#footnotes",
    "href": "2-summarizing-data/06-multiple-linear-regression/notes.html#footnotes",
    "title": "Multiple Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFifth Avenue is the wide north-south street that divides Manhattan into an east side and a west side.↩︎\nThis is the case for a model including an intercept term; these models will be our focus this semester and are the most rcommonly used.↩︎\nWhile ggplot2 is the best package for static statistical graphics, it does not have any interactive functionality. This plot was made using a system called plotly, which can be used both in R and Python. Read more about how it works at https://plotly.com/r/.↩︎\nThis is a screenshot from an interactive 3D scatter plot. We’ll see the interactive plot in class tomorrow.↩︎",
    "crumbs": [
      "Notes",
      "Summarization",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#estimate-the-correlation",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#estimate-the-correlation",
    "title": "Summarizing Numerical Associations",
    "section": "Estimate the correlation",
    "text": "Estimate the correlation\n\n\nWhat is the (Pearson) correlation coefficient between these two variables?\n\n\nThis plot has a very strong negative linear association; the correlation coefficient is -.98.\nThe goal of this exercise is threefold: 1. Start calibrating the ability to associate the correlation with the structure in a scatter plot. 2. Make it clear that this is still just an informal eye-balling procedure (therefore multiple answers can be reasonable, but some are unreasonable). 3. Set some of them up for a challenge on the next one; some will conflate correlation with association.\nIf you’d like to train their eye a bit more, you can open up https://www.guessthecorrelation.com/ in a browser and play the game a few times.\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#section",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#section",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "Which four plots exhibit the strongest association?\n\n\n  \n    −\n    +\n \n 01:00\n \n\nStudent will likely identify B, D, E, and F. F only appears to be highly associated because of the scaling of the y-axis. This is a good opportunity to review definition of association. The following slides demonstrate the effects of that scaling."
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\n\n\ndf_f"
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-1",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-1",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\n\n\ndf_f\n\n        x            y\n1   -8.00 -1.307007369\n2   -7.95 -0.466886973\n3   -7.90 -1.317638373\n4   -7.85  1.008059015\n5   -7.80  1.153714017\n6   -7.75  0.136000314\n7   -7.70  1.081268517\n8   -7.65  0.027234440\n9   -7.60 -0.755489564\n10  -7.55  0.643581028\n11  -7.50 -2.221816425\n12  -7.45 -0.352711462\n13  -7.40  0.788325702\n14  -7.35  0.348209977\n15  -7.30  0.888132692\n16  -7.25  1.494640228\n17  -7.20  0.539665177\n18  -7.15  1.534245783\n19  -7.10 -0.636288267\n20  -7.05  1.400750746\n21  -7.00  0.857266821\n22  -6.95  0.689240640\n23  -6.90 -0.578689332\n24  -6.85 -0.247484190\n25  -6.80 -1.616711187\n26  -6.75 -0.211921880\n27  -6.70 -1.389650022\n28  -6.65 -0.667030427\n29  -6.60  0.265923179\n30  -6.55  0.368512913\n31  -6.50  0.159370185\n32  -6.45 -0.803029578\n33  -6.40 -1.181853575\n34  -6.35  0.799141786\n35  -6.30 -0.119741600\n36  -6.25  1.055099362\n37  -6.20  0.260610303\n38  -6.15  1.215058556\n39  -6.10 -2.093776227\n40  -6.05 -0.340475843\n41  -6.00  1.098181964\n42  -5.95 -1.022920292\n43  -5.90  0.997308309\n44  -5.85  0.442282185\n45  -5.80  2.024841893\n46  -5.75 -0.206236931\n47  -5.70 -0.366921396\n48  -5.65 -0.540982671\n49  -5.60  0.326545855\n50  -5.55  1.185875818\n51  -5.50  0.163270582\n52  -5.45 -0.252746010\n53  -5.40 -0.538783647\n54  -5.35  0.239830058\n55  -5.30  0.227238979\n56  -5.25  0.146973095\n57  -5.20  1.206565164\n58  -5.15  0.274237725\n59  -5.10  0.291223416\n60  -5.05 -0.048435420\n61  -5.00 -0.160503335\n62  -4.95  0.982662625\n63  -4.90 -0.009812706\n64  -4.85 -1.236449694\n65  -4.80 -0.220086594\n66  -4.75  0.916450245\n67  -4.70  0.312158589\n68  -4.65 -0.271222289\n69  -4.60 -0.362834150\n70  -4.55  0.690127669\n71  -4.50  0.954579372\n72  -4.45 -1.002334741\n73  -4.40 -0.827510811\n74  -4.35 -0.481050964\n75  -4.30  0.213284850\n76  -4.25  0.694018655\n77  -4.20  0.318705891\n78  -4.15  1.359291786\n79  -4.10  0.299499214\n80  -4.05 -0.136853535\n81  -4.00 -0.907524278\n82  -3.95  0.580039124\n83  -3.90 -0.451761432\n84  -3.85 -0.637651429\n85  -3.80 -0.377287558\n86  -3.75 -0.108975033\n87  -3.70  0.266174878\n88  -3.65  1.950537958\n89  -3.60  2.481797153\n90  -3.55  0.024020307\n91  -3.50 -0.271974465\n92  -3.45 -0.324420253\n93  -3.40  0.914990966\n94  -3.35 -0.177070403\n95  -3.30  1.668125591\n96  -3.25 -0.196626395\n97  -3.20  0.909735944\n98  -3.15  0.379802619\n99  -3.10 -0.717674077\n100 -3.05 -0.137978296\n101 -3.00 -0.032596143\n102 -2.95 -1.151078285\n103 -2.90 -0.035147120\n104 -2.85  1.289653594\n105 -2.80  1.301664628\n106 -2.75 -1.314453018\n107 -2.70  0.279983464\n108 -2.65 -0.134496134\n109 -2.60 -0.993968666\n110 -2.55 -1.125493956\n111 -2.50 -0.585075814\n112 -2.45 -0.360440229\n113 -2.40 -0.128946258\n114 -2.35 -1.763949074\n115 -2.30  0.249921212\n116 -2.25  1.356315758\n117 -2.20 -0.221241869\n118 -2.15 -0.255945298\n119 -2.10  0.764032808\n120 -2.05 -0.429023148\n121 -2.00 -0.506416626"
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-2",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-2",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\n\n\ndf_f |&gt;\n    ggplot(aes(x = x,\n               y = y)) +\n    geom_point() +\n    ylim(-9, 9) +\n    theme_bw()"
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-3",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-3",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\n\n\ndf_f |&gt;\n    ggplot(aes(x = x,\n               y = y)) +\n    geom_point() +\n    ylim(-9, 9) +\n    theme_bw()"
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-4",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-4",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\n\n\ndf_f |&gt;\n    ggplot(aes(x = x,\n               y = y)) +\n    geom_point() +\n    theme_bw()"
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-5",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-5",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\n\n\ndf_f |&gt;\n    ggplot(aes(x = x,\n               y = y)) +\n    geom_point() +\n    theme_bw()"
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-6",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-6",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\ndf_f |&gt;\n    summarize(pearson = cor(x, y))"
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-7",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#pearson-correlation-and-scale-7",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\ndf_f |&gt;\n    summarize(pearson = cor(x, y))\n\n      pearson\n1 -0.03106456"
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#describing-associations",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#describing-associations",
    "title": "Summarizing Numerical Associations",
    "section": "Describing Associations",
    "text": "Describing Associations\nWhen considering the structure in a scatter plot, pay attention to:\n\nThe strength of the association\n\nHow much does the conditional distribution of y change when you move along the x?\n\n\n\n\nThe shape of the association\n\nLinear? Quadratic? Cubic or beyond?\n\n\n\n\n\nThe direction of the (linear) association\n\nPositive or negative?"
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#section-1",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#section-1",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "Guess: Which state has the highest poverty rate? The lowest?\n\n\nThis can be asked without poll-everywhere, without group discussion, just asking students to shout out which states they think will have highest and lowest.\nAs they answer, ask where they’re thinking those states points are; how they’re using this plot to think about the highest povery rate and the lowest poverty rate. Point out that these two questions don’t even take advantage of the scatter plot, which allows you to see the relationship between two variables. Each question just deals with the marginal distribution of poverty or graduation rate, which could just as well be two separate histograms, as shown on the next slide."
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#section-2",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#section-2",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "This is an equally-useful set of plots to answer the previous question.\nIf we want to ask a question that requires a scatter plot (and a linear model) to answer, we need to ask a conditional question."
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/slides.html#section-3",
    "href": "2-summarizing-data/05-summarizing-associations/slides.html#section-3",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "Which state has the highest graduation rate given its poverty rate?\n\n\n  \n    −\n    +\n \n 01:00\n \n\nThis assesses whether or not students have an informal sense of what a residual is."
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/notes.html",
    "href": "2-summarizing-data/05-summarizing-associations/notes.html",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "Which of the following plots do you think depicts the relationship between the high school graduation rate and the poverty rate among the 50 US states?\nIf you guessed the plot on the left, you are correct 🎉.\nStates with higher poverty rates tend to have lower graduation rates. This is a prime example of two variables that are associated. In a previous set of notes we defined association between two categorical variables, but lets replace that with a more general definition that can apply here.\nYou can detect associations in scatter plots by scanning from left to right along the x-axis and determining whether or not the conditional distribution of the y-variable is changing or not. In the figure to the left below, when you look first to the states with low poverty rates (in the blue box), you find that the conditional distribution of the graduation rate (represented by the blue density curve along the right side of the scatter plot) is high: most of those states have graduation rates between 85% and 90%. When you scan to the right in that scatter plot, and condition on having a high poverty rate (the states in the red box), the conditional distribution shifts downwards. Those states have graduations rates in the low 80%s.\nThe plot on the right, by contrast, exhibits no association between poverty rate and graduation rate. When we compare the low poverty states with the high poverty states, their conditional distributions of Graduation rate are essentially the same.\nSo we can use the simple scatter plot to determine whether or not two numerical variables are associated, but sometimes a graphic isn’t enough. In these notes we’ll move from graphical summaries to numerical summaries and construct two different approaches to capturing these associations in numbers: the correlation coefficient and the simple linear model.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Numerical Associations"
    ]
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/notes.html#the-correlation-coefficient",
    "href": "2-summarizing-data/05-summarizing-associations/notes.html#the-correlation-coefficient",
    "title": "Summarizing Numerical Associations",
    "section": "The Correlation Coefficient",
    "text": "The Correlation Coefficient\nLet’s set out to engineer our first numerical summary in the same manner that we have previously, by laying out the properties that we’d like our summary to have.\nPlease watch the following 12 minute video.\n\n\nCorrelation coefficient, \\(r\\)\n\nThe correlation coefficient, \\(r\\), between two variables \\(x\\) and \\(y\\) is \\[r = \\frac{1}{n-1}\\sum_{i=1}^n \\left( \\frac{x_i - \\bar{x}}{s_x} \\right) \\left( \\frac{y_i - \\bar{y}}{s_y} \\right)\\]\n\n\n\n\nSeveral different statistics have been proposed for measuring association. This is the most common and is more specifically called the Pearson correlation.\n\nExample: Poverty and Graduation rate\nThe data frame used to create the scatter plot above on the left looks like this.\n\nselect(poverty, Graduates, Poverty)\n\n# A tibble: 51 × 2\n   Graduates Poverty\n       &lt;dbl&gt;   &lt;dbl&gt;\n 1      79.9    14.6\n 2      90.6     8.3\n 3      83.8    13.3\n 4      80.9    18  \n 5      81.1    12.8\n 6      88.7     9.4\n 7      87.5     7.8\n 8      88.7     8.1\n 9      86      16.8\n10      84.7    12.1\n# ℹ 41 more rows\n\n\nSince it is a data frame, we can use the summarize() function to calculate our summary statistic.\n\npoverty |&gt;\n  summarize(r = cor(Poverty, Graduates))\n\n# A tibble: 1 × 1\n       r\n   &lt;dbl&gt;\n1 -0.747\n\n\nThe value of -0.747 tells us that the linear association between these variables is negative and reasonably strong. This is our first example of a bivariate summary statistic: there are two variables that we put inside the cor() function to compute our statistic.\nLet’s repeat this calculation for the data frame that created the shapeless scatter plot with no association, poverty_shuffled.\n\npoverty_shuffled |&gt;\n  summarize(r = cor(Poverty, Graduates))\n\n# A tibble: 1 × 1\n        r\n    &lt;dbl&gt;\n1 -0.0546\n\n\nAs expected, that scatter plot yields a correlation coefficient very close to zero because the points are scattered across all four quadrants of the plot.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Numerical Associations"
    ]
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/notes.html#the-simple-linear-model",
    "href": "2-summarizing-data/05-summarizing-associations/notes.html#the-simple-linear-model",
    "title": "Summarizing Numerical Associations",
    "section": "The Simple Linear Model",
    "text": "The Simple Linear Model\nAnother approach to summarizing the linear association is to just … draw a line.\n\n\n\n\n\nThis line serves both as a graphical summary and also as a numerical summary. After all, every line that you draw on a scatter plot is defined by two numbers: the slope and the y-intercept. This line is called a simple linear model.\n\nSimple Linear Model\n\nAn expression for a possible value of the \\(y\\) variable, \\(\\hat{y}\\), as a linear function of the \\(x\\) variable with slope \\(b_1\\) and y-intercept \\(b_0\\). \\[\\hat{y} = b_0 + b_1x\\]\n\n\nTherefore, a simple linear model captures the linear relationship of two variables in not one but two summary statistics, \\(b_0\\) and \\(b_1\\).\nFor the line above, we can do our best to eye-ball these. The line appears to rise -2 percentage points for every 2.5 that it runs, so I’d estimate the slope to be about \\(-2/2.5 = -0.8\\). If I were to draw the line all the way to the left until it crossed the y-axis at a poverty rate of 0, its y-intercept would be around 95. So I could express the line that is drawn above as:\n\\[\\hat{y} = 95 - 0.8 x\\]\n\nThe Least Squares Line\nIf that felt a little shifty to you - drawing a line by hand and then eyeballing its slope and intercept - we can be more precise by using a more precisely-defined type of linear model: the least squares line. This is a method that we’ll study in depth when we get to the unit on prediction, but for now, we’ll use it because it makes calculation very easy. You can find the slope and intercept of the least squares line using statistics that we’re already familiar with: \\(\\bar{x}, \\bar{y}, s_x, x_y\\), and \\(r\\).\n\nLeast Squares Slope\n\n\\[ b_1 = r \\frac{s_y}{s_x} \\]\n\nLeast Squares Intercept\n\n\\[ b_0 = \\bar{y} - b_1 \\bar{x}\\]\n\n\n\nSo how does this line look compared to the hand-drawn line? Let’s calculate the slope and intercept and add the resulting line to our scatter plot.\n\n\n\n\n\n\n\n\n\nThat works remarkably well!\nThe function that was used to calculate the least squares slope and intercept is lm().\n\nlm(Graduates ~ Poverty, data = poverty)\n\n\nCall:\nlm(formula = Graduates ~ Poverty, data = poverty)\n\nCoefficients:\n(Intercept)      Poverty  \n    96.2022      -0.8979  \n\n\nThe syntax for lm() uses what’s called “formula notation” in R. The first argument is a formula of the form y ~ x and can be read as, “Explain the y as a function of the x”. In the second argument, you specify which data frame contains the variables used in the formula.\nSo if the correlation coefficient measures the strength of the linear relationship between two variables, what exactly are the slope and intercept measuring? The slope captures the expected change in the \\(y\\) associated with the \\(x\\) changing by 1 unit. In this example, states that are separated by 1 percentage point in their poverty rate tend to be separated by about -.89 in their graduation rate. This is distinct from what the correlation tells us because while \\(r\\) will stay the same regardless of the units in which the data is measured, \\(b_1\\) is expressly designed to tell us how those units of measurement relate to one another.\nWhat about the intercept? It tells us the value that we’d expect the \\(y\\) to take when the \\(x\\) takes a value of zero. Sometimes that’s an informative statistic, sometime it is not. In this setting, do you really expect the graduation rate to be around 96% when their poverty rate is zero? What would it even look like for a state to have a poverty rate of zero? The abstraction of the linear model allows us to ponder such a world, but the reality of economics in the US is that we would never actually observe poverty rates of zero.\nSo what good is the intercept? Well, it’s useful in helping us calculate a residual.\n\n\nResiduals\nOne of the benefits of explaining the association between two variables with a line instead of just the correlation coefficient is that it allows us to calculate what we would expect an observation’s y-value to be based on its x value, so that we can see how far our expectation is from reality. That gap between expectation and reality is called a residual.\n\nResidual (\\(\\hat{e}_i\\))\n\nThe difference between the observed value of a data point, \\(y_i\\), and the value that we would expect according to a linear model, \\(\\hat{y}_i\\). \\[ \\hat{e}_i = y_i - \\hat{y}_i \\]\n\n\n\n\n\\(\\hat{y}_i\\) is said “y hat sub i” and is also called the “fitted value”.\nLet’s calculate the residual for California. Here is that row in the data set.\n\npoverty |&gt;\n  filter(State == \"California\") |&gt;\n  select(State, Graduates, Poverty)\n\n# A tibble: 1 × 3\n  State      Graduates Poverty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 California      81.1    12.8\n\n\nThis shows us that for California, \\(y = 81.1\\), so the next step is to find where the line passes through California’s x-value, \\(x = 12.8\\). There are several ways to do that calculation, including using R like a calculator and simply plugging that value into the equation for the line show above.\n\ny_hat &lt;- 96.2022 - 0.8979 * 12.8\ny_hat\n\n[1] 84.70908\n\n\nWith that in hand, we can calculate California’s residual.\n\n81.1 - y_hat\n\n[1] -3.60908\n\n\nThis residual tells us that California is actually a bit of an underachiever. Among states with a poverty rate around 12.8, we would expect their graduate rate to be around 84.7. California’s rate, however, is 81.1, a decrease of 3.6.\nThe calculation of the residual can be seen in the plot below.\n\n\n\n\n\n\n\n\n\nThe horizontal dashed line represents \\(\\hat{y} = 84.7\\), the y-value of the least squares line when it passes through \\(x = 12.8\\). The vertical red dashed line is the residual: the distance between the line and the observation in the y direction.\nResiduals open up a new avenue for numerical statistics. While the slope and intercept are two statistics that tell us about the overall linear relationship between the two variables, each residual is a statistic that tells us whether an individual observation’s y-value is higher or lower than we’d expect based on its x-value.\n\n\nIf you have \\(n\\) data points, you can calculate \\(n\\) residuals.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Numerical Associations"
    ]
  },
  {
    "objectID": "2-summarizing-data/05-summarizing-associations/notes.html#summary",
    "href": "2-summarizing-data/05-summarizing-associations/notes.html#summary",
    "title": "Summarizing Numerical Associations",
    "section": "Summary",
    "text": "Summary\nIn these notes we considered the question of how to capture the association between two variables with visualizations and numerical summary statistics. The correlation coefficient is one of the most common: it captures the strength and direction of the linear trend. This statistic can be used, along with other simple summary statistics, to calculate the slope and intercept of the least squares line. The least squares line is an alternative approach to summarizing the linear relationship between two numerical variables. It has the advantage of providing an expectation for the y-value of every observation, which allows us to calculate residuals which are expressions of whether each observation is higher or lower than we’d expect.\nWe’ll spend time practicing calculating these statistics - and looking at lots of scatter plots - in class. We’ve also prepared a tutorial to help you become adept at working with linear models in R.\nContinue on to the tutorial portion of the notes",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Numerical Associations"
    ]
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#cq-1",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#cq-1",
    "title": "Summarizing Categorical Data",
    "section": "CQ 1",
    "text": "CQ 1\n\nThe next four subquestions are based on the same table."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "The table below displays data from a survey on a class of students.\n\n\nWhat proportion of the class was in the marching band?\n\n\n  \n    −\n    +\n \n 00:30\n \n\nAn example of a marginal proportion."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-1",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-1",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "What proportion of those in the marching band where juniors?\n\n\n  \n    −\n    +\n \n 00:30\n \n\nAn example of a conditional proportion."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-2",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-2",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "What proportion were sophomores not in the marching band?\n\n\n  \n    −\n    +\n \n 00:30\n \n\nAn example of a joint proportion."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-3",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-3",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "What were the dimensions of the raw data from which this table was constructed?\n\n\n  \n    −\n    +\n \n 00:30"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-4",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-4",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "How would you characterize the association between these two variables?\n\n\n  \n    −\n    +\n \n 00:30"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#cq-2",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#cq-2",
    "title": "Summarizing Categorical Data",
    "section": "CQ 2",
    "text": "CQ 2\nPolitical affiliation and college degree status of 500 survey participants.\n\n\nWhich group is the largest?\n\n\nThe General Social Survey is a widely used sources of data on the attitudes, behaviors, and attributes of Americans. This plot shows the relationship between the political affiliation and college degree status of 500 participants.\nCannot tell which group is the largest since this plot as been normalized so that the proportions within each party sum to 1. The unnormalized plot on the following slide is one that allows us to answer this questions.\nThe unnormalized bar chart of counts preserves original counts and thus is good at comparing joint proportions. The normalized bar count shows condition proportions and thus is good for showing associations between variables.\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-5",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-5",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "What does this plot show?\n\n\nImportant note: it looks like a leading “1” was cropped from the numbers along the y axis.\nThis is a confusing stacked bar chart! (unfortunately I wasn’t able to track down the source.)\nThe height of each total bar appears to be the energy supply from renewables worldwide in each of these years. The proportion on top, however, appears to be that number divided by the global total energy supply, which changes every year. The fact that those proportions are decreasing left to right as the bar heights are increasing suggests that renewables are increasing every year (the numerator) but the total energy supply (the denominator) is increasing at a faster rate.\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/notes.html",
    "href": "2-summarizing-data/01-summarizing-categorical-data/notes.html",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "The tools for calculating numerical summaries and graphical summaries can be cleanly divided between tools developed for categorical data and tools for numerical data. We’ll discuss each in turn, starting with categorical data.\nWhen Dr. Gorman collected data near Palmer Station, Antarctica, she recorded a total of eight variables on 333 penguins, 10 rows of which are presented here in a data frame.\nThe first two of these you’ll recognize as nominal categorical variables. The species of each penguin can take one of three levels: Adelie, Chinstrap, or Gentoo; and the island on which they were found can also take three levels: Biscoe, Dream, or Torgersen.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Categorical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/notes.html#summary",
    "href": "2-summarizing-data/01-summarizing-categorical-data/notes.html#summary",
    "title": "Summarizing Categorical Data",
    "section": "Summary",
    "text": "Summary\nA wise statistician once said, “In statistics, most of what we do is add things up. Sometimes we divide. The challenging part is deciding what to add and divide.” This captures the deceptive simplicity of summarizing categorical data. Categorical summaries involve counts of categories or proportions. Those proportions can be joint proportions, marginal proportions, or conditional proportions. Those counts and proportions are commonly displayed in contingency tables or in bar charts. Subtle choices of which proportion to present results in the telling of dramatically different stories.\nContinue on to the tutorial portion of the notes",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Categorical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/labs/flights/part-2.html",
    "href": "2-summarizing-data/labs/flights/part-2.html",
    "title": "Flights",
    "section": "",
    "text": "Slides\nlibrary(tidyverse)\nlibrary(stat20data)\ndata(flights)"
  },
  {
    "objectID": "2-summarizing-data/labs/flights/part-2.html#part-i-understanding-the-context-of-the-data",
    "href": "2-summarizing-data/labs/flights/part-2.html#part-i-understanding-the-context-of-the-data",
    "title": "Flights",
    "section": "Part I: Understanding the Context of the Data",
    "text": "Part I: Understanding the Context of the Data\nFor the questions on the handout, consult the image of the data frame found in the slides linked above.\n\nPart 1: Understanding the Context of the Data"
  },
  {
    "objectID": "2-summarizing-data/labs/flights/part-2.html#part-ii-computing-on-the-data",
    "href": "2-summarizing-data/labs/flights/part-2.html#part-ii-computing-on-the-data",
    "title": "Flights",
    "section": "Part II: Computing on the Data",
    "text": "Part II: Computing on the Data\nThe data for this lab can be found in the flights data frame in the stat20data package. Run ?flights at the console to learn more about the columns. Where applicable, answer each question with one pipeline, which may include dplyr code, ggplot2 code or both.\n\nQuestion 1\nHow many flights in the dataset left in the springtime and were destined for Portland, Oregon?\n\n\nQuestion 2\nCreate a new variable called avg_speed that is the average speed of the plane during the flight, measured in miles per hour. Save it back into the data frame; you’ll use it later on.\n\n\nQuestion 3\nArrange the data frame to figure out: what is the destination and delay time (in hrs) for the flight that was most delayed?\n\n\nQuestion 4\nArrange the data frame to figure out: what is the destination and delay time (in hrs) for the flight that was least delayed, i.e. that left the most ahead of schedule?\n\n\nQuestion 5\nWhat proportion of all of the flights left on or ahead of schedule? For Oakland and SFO separately, what proportion of flights left on or ahead of schedule?\n\n\nQuestion 6\nHow many flights left SFO during March 2020?\n\n\nQuestion 7\nHow many flights left SFO during April 2020?\n\n\nQuestion 8\nCreate a bar chart that shows the distribution by month of all the flights leaving the Bay Area (SFO and OAK). Do you any sign of an effect of the pandemic?\n\n\nQuestion 9\nCreate a histogram showing the distribution of departure delays for all flights. Be sure to\n\nset the limits of the x-axis to focus on where most of the data lie,\nadd a text annotation that explains the meaning of a negative departure delay,\nand below the plot write 1-2 sentences that describe the shape and modality of the distribution.\n\n\n\nQuestion 10\nCreate a data frame that contains the median and interquartile range for departure delays, grouped by carrier. Which carrier has the lowest typical departure delay? Which one has the least variable departure delays?\n\n\nQuestion 11\nCreate a plot that captures the relationship of average speed vs. distance and describe the shape and structure that you see. What phenomena related to taking flights from the Bay Area might explain this structure?\n\n\nQuestion 12\nFor flights leaving SFO, which month has the highest mean departure delay? What about the highest median departure delay? Which of these measures is more useful to know when deciding which month(s) to avoid flying if you particularly dislike flights that are severely delayed?\n\n\nQuestion 13\nEach individual airplane can be uniquely identified by its tailnumber in the same way that US citizens can be by their social security numbers. Which airplane flew the farthest in total during this year for which we have data? How many times around the planet does that translate to?\n\n\nQuestion 14\nWhat is the tailnumber of the fastest plane in the data set? What type of plane is it (google it!)? Be sure to be clear how you’re defining fastest.\n\n\nQuestion 15\nThe plot below shows the relationship between the number of flights going out of SFO and the average departure delay. It illustrates the hypothesis that more flights on a given day would lead to a more congested airport which would lead to greater delays on average. Each point represents single day in 2020; there are 366 of them on the plot. Please form a single pipeline that will create this plot, starting with the raw data set.\n\n\n\n\n\n\n\nQuestion 16\nCreate a plot to illustrate the association between departure delay and arrival delay. Summarize the linear relationship by calculating the correlation coefficient and by fitting a linear model and showing the value of those statistics (this requires two multiple pipelines). For an optional challenge, use geom_smooth() to superimpose the linear model on the scatter plot.\n\n\nQuestion 17\nWhich flight has the longest arrival delay given its departure delay?\n\n\nQuestion 18\nFit a multiple linear regression model that explains arrival delay using departure delay and the distance of the flight and print out the coefficients (the intercept and two slopes). Speculate as to why the sign (positive or negative) of the distance coefficient is what it is.\n\n\nQuestion 19\nOn average, which carrier’s flights had the smallest arrival delay given their departure delay and distance?\n\n\nQuestion 20\nCan we compare the regression coefficients for departure delay and distance to understand which has the stronger relationship? Why or why not?\n\n\nLast Question\nWill you ensure that your submission to Gradescope…\n\nis of a pdf generated from a qmd file,\nhas all of your code visible to readers,\nand assigns each of the questions to all pages that show your work for that question?\n\n(This one is easy! Just answer “yes” or “no”)"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#section",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#section",
    "title": "A Grammar of Graphics",
    "section": "",
    "text": "What are the aesthetics and geometry of this plot?\n\n\n\n  \n    −\n    +\n \n 01:00\n \n\nCorrect answer: see code above.\nThis is a warm up to get them thinking about the fundamental concept of aesthetic mappings and geometries on a plot style and data set they’ll be familiar with. This should be quite easy."
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#section-1",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#section-1",
    "title": "A Grammar of Graphics",
    "section": "",
    "text": "What code was used to make this plot?\n\n\n  \n    −\n    +\n \n 01:00\n \n\nCorrect answer: see above.\nThis demonstrates a practice called “double-mapping”, when the same column is mapped to two different aesthetic attributes. Most consider this a bad practice.\nIn a setting like this, usually the author is really just trying to get more color onto the plot. That can be accomplished by setting the color of the bars using something like:"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#section-2",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#section-2",
    "title": "A Grammar of Graphics",
    "section": "",
    "text": "Global infectious disease prevalence in 1989\n\n\n\nWhat are the aesthetics and geometry of this plot?\n\n\n\n  \n    −\n    +\n \n 01:00\n \n\nCorrect answer: Color is mapped to taxonomic family, size is mapped to prevalence, the diseases are represented using the point geometry.\nGood follow up questions and boardwork include:\n\nWhat does the data frame look like that made this plot?\nWhat determines where the bubbles are placed in the chart? Answer: bubble charts have different methods of doing this. This one appears to use a packing algorithm so get them close together. Important, none of those methods refer to a column in the data frame therefore are not aesthetic mappings.\nIf you really want to go far afield, you could show an example of a beeswarm chart, which is a bubble chart that uses one of the x or the y to map a numerical variable and then uses a packing algorithm to determine placement on the other axis.\nOne of the lessons of this bubble chart is that our “named” charts correspond to particular combos of aesthetic mappings and geometriess. A “scatterplot” uses the point geometry and maps numerical data to the x and y axes. A “bubble chart” uses the point geometry too, but maps size to a numerical variable (and often uses color too)."
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#concept-activity-1",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#concept-activity-1",
    "title": "A Grammar of Graphics",
    "section": "Concept Activity",
    "text": "Concept Activity\nYou will be watching a 2.5 minute video of a presentation by a scientist, Hans Rosling, who studied global public health. He presents data visualizations depicting the change in life expectancy and family size over several decades in the 20th century.\n\nOn a piece of note paper:\n\nSketch out the data frame used to create the graphic and add the names of the variables.\nList the aesthetic attributes used to encode the data in the graphic.\nIdentify the geometry used in the plot."
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#section-4",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#section-4",
    "title": "A Grammar of Graphics",
    "section": "",
    "text": "Please turn to your neighbors and…\n\nDiscuss what you came up with in terms of . . .\n\nthe variables present in the data frame\nthe aesthetic attributes used to encode that data in the plot\nthe geometry\n\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-were-the-variables-and-aesthetic-attributes",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-were-the-variables-and-aesthetic-attributes",
    "title": "A Grammar of Graphics",
    "section": "What were the variables and aesthetic attributes?",
    "text": "What were the variables and aesthetic attributes?\n\n\n\nVisual Cues / Aesthetics\n\nLocation along the x-axis\nLocation along the y-axis\nSize of point\nColor of point\nAnimation\n\n\nVariables\n\nFertility rate\nLife expectancy\nPopulation\nRegion\nYear"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-did-the-data-frame-look-like",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-did-the-data-frame-look-like",
    "title": "A Grammar of Graphics",
    "section": "What did the data frame look like?",
    "text": "What did the data frame look like?\nWhat was the unit of observation? What were the variables? What were their type?\n\n\n\nUnit of observation\n\nA country in a given year\n\n\nVariables\n\nFertility rate (continuous)\nLife expectancy (continuous)\nPopulation (continuous)\nRegion (nominal)\nYear (discrete)"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-geometry-is-used-to-represent-the-observations",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-geometry-is-used-to-represent-the-observations",
    "title": "A Grammar of Graphics",
    "section": "What geometry is used to represent the observations?",
    "text": "What geometry is used to represent the observations?\n\n\nPoints"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-claim-was-made",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-claim-was-made",
    "title": "A Grammar of Graphics",
    "section": "What type of claim was made?",
    "text": "What type of claim was made?\n\nThe group Morning Consult conducted a poll of a representative sample of registered 1,256 Republican voters on August 24, 2023, asking them who they planned to vote for in the primary eleection. 58% of respondents replied “Trump” and 14% replied “DeSantis”. A major news outlet ran a headline, “Trump leads DeSantis by 44 points among registered Republican voters.” . . .\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-variable-is-listeners",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-variable-is-listeners",
    "title": "A Grammar of Graphics",
    "section": "What type of variable is listeners?",
    "text": "What type of variable is listeners?\n{fig-alt=“A dataframe with a column labelled ‘Listeners (in million)’ with values 40, 66, 60, 73, 57, 75, and 13’}\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-proportion-is-used",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-proportion-is-used",
    "title": "A Grammar of Graphics",
    "section": "What type of proportion is used?",
    "text": "What type of proportion is used?\n\n\n\nRoughly 68 percent of those passengers who were in the first class survived the wreckage of the Titanic.\n\n\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#which-measure-of-centerspread-is-least-appropriate",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#which-measure-of-centerspread-is-least-appropriate",
    "title": "A Grammar of Graphics",
    "section": "Which measure of center/spread is least appropriate?",
    "text": "Which measure of center/spread is least appropriate?\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-are-the-aesthetics-and-geometry-of-this-plot",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-are-the-aesthetics-and-geometry-of-this-plot",
    "title": "A Grammar of Graphics",
    "section": "What are the aesthetics and geometry of this plot?",
    "text": "What are the aesthetics and geometry of this plot?\n\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-has-not-changed-when-moving-from-left-to-right",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-has-not-changed-when-moving-from-left-to-right",
    "title": "A Grammar of Graphics",
    "section": "What has not changed when moving from left to right?",
    "text": "What has not changed when moving from left to right?"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/notes.html",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/notes.html",
    "title": "A Grammar of Graphics",
    "section": "",
    "text": "In the last set of notes, we saw that there is a side to the discipline of Statistics that looks like engineering. Summary statistics - medians, standard deviations, etc. - are carefully crafted tools that capture different characteristics of a data set for use in very specific situations. There is another practice in statistics that looks more like a science; that is, a field that seeks to take many different phenomena and explain them using a systematic overarching theory. That practice is data visualization.\nAt this point in the course, you’ve seen several several examples of data visualizations.\nThe diversity of shapes and structures used in these plots suggest that each one is a thing-unto-itself, specially devised to provide one particular style of visualization. But what elements do they share?\nFocus on the nature of the data being used. Exactly half of the plots above are illustrating the distribution of a single variable; the other half illustrate the relation between two variables. Can you tell which is which?\nConsider the manner in which variability in the data is being conveyed used different visual cues. How many of the plots above utilize an x-axis? A y-axis? Color?\nFinally, how are the observations finding their way onto the plot? Three of the plots above share the same data variable, utilize the same visual cues, and differ only in the shape used to encode the observations.\nBy asking these questions, we begin to find recurring structures in a wide range of plot types. These recurring structures have been compiled into a widely-used framework called the Grammar of Graphics.",
    "crumbs": [
      "Notes",
      "Summarization",
      "A Grammar of Graphics"
    ]
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#the-grammar-of-graphics",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#the-grammar-of-graphics",
    "title": "A Grammar of Graphics",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nIn 1999, a statistician named Leland Wilkinson published the first edition of what has been the most influential work in data visualization, The Grammar of Graphics1. The title is fitting. In the same way that a grammar defines the regular structures and composition of a language, his book outlines a framework to structure statistical graphics.\n\nNearly every current software tool used to build plots has been informed by this book2. Its influence can be found in Tableau, Plotly, and the Python libraries bokeh, altair, seaborn, and plotnine. The most complete implementation of the grammar is found in an R package called ggplot2 by Hadley Wickham3.\nIn Wickham’s adaptation of the grammar of graphics, a plot can be decomposed into three primary elements:\n\nthe data,\nthe aesthetic mapping of the variables in the data to visual cues, and\nthe geometry used to encode the observations on the plot.\n\nLet’s go through each of these components one-by-one to understand the role that they play in a plot like this, which we’ll refer to as the “penguin plot”.\n\n\n\n\n\n\n\n\n\nThe above plot is an example new type of plot which involves two numerical variables: a scatter plot. The points may also be colored by a third variable, generally a categorical one.\n\nData\nWhat variables are needed to construct the penguin plot above?\nWe see bill_length_mm and bill_depth_mm; those are labeled clearly on the x and y axes. We must also know the species of each of these penguins in order to know which color to label each point. In other words, there are three columns of a data frame that we need to have on hand.\n\n\n# A tibble: 333 × 3\n   bill_length_mm bill_depth_mm species\n            &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;  \n 1           39.1          18.7 Adelie \n 2           39.5          17.4 Adelie \n 3           40.3          18   Adelie \n 4           36.7          19.3 Adelie \n 5           39.3          20.6 Adelie \n 6           38.9          17.8 Adelie \n 7           39.2          19.6 Adelie \n 8           41.1          17.6 Adelie \n 9           38.6          21.2 Adelie \n10           34.6          21.1 Adelie \n# ℹ 323 more rows\n\n\nThrough working on your assignments, you may have also seen the following plot, which is a variant of the “penguin plot”:\n\n\n\n\n\n\n\n\n\nIn this course we’ve talked plenty about the structure of a data frame, so this part of the grammar of graphics is straight-forward. Be sure that every variable that you wish to include in your plot is present in the same data frame.\n\nMore fundamentally, be sure the data you’re using is well-suited to the message you aim to convey with your plot. Many plots go wrong right here at the outset, so be sure you’re on firm footing.\n\n\nAesthetics\nThe most impactful decision that you’ll make when constructing a plot using the grammar of graphics is deciding how to encode variables in a data frame into visual variation in your plot.\nThe penguin plot relies upon three forms of visual variation. The first is the location along the x-axis. Penguins with longer bills are placed on the right side of the plot and those with shorter bills are placed on the left. Variation in bill depth is captured by variation in the location along the y-axis, which is the second form. The third form is color: each of the three species is designated by one of three colors.\n\n\n\n\n\nWe can summarize this encoding, or “aesthetic mapping”, as:\n\nbill_length_mm is mapped to the x-axis\nbill_depth_mm is mapped to the y-axis\nspecies is mapped to color\n\nThese are three of many different techniques for visually encoding variability. Here is a list of the aesthetic attributes that are most commonly used:\n\nx: location along the x-axis\ny: location along the y-axis\ncolor: hue of the mark that represents the observation\nalpha: the level of transparency of the color\nsize: the size of the mark representing the observation\nshape: the shape of the mark representing the observation\nfill: the color of the inside of the representation of an observation\n\n\n\nGeometries\nWith the data set in place and the aesthetic mappings selected, the final choice in making our plot is to decide how to graphically express the observations themselves. For the penguin plot above, each observation in represented by a point, so it is said to use a “point” geometry. That is just one of many options. Other options are listed below.\n\npoint\nbar\nline\nhistogram\ndensity\nviolin\ndotplot\nboxplot\n\nWhen we speak about whether a plot is a scatter plot, a bar chart, a histogram, etc, we are discussing the geometry of a plot. The impact of this choice can be seen in the following two plots.\n\n\n\n\n\n\n\n\n\nBoth plots share the same data (penguins) and the same aesthetic mappings(bill_length to the x-axis and species to the y-axis). Where they differ is the geometry: the plot on the left uses the violin while the one on the right uses the boxplot.\n\n\nExample: Births over Time\nThe following plot displays the total number of births over time recorded in London, England during the 17th century.\n\n\n\n\n\n\n\n\n\nIt was constructed from a data frame with 82 rows, the first six of which are shown below.\n\n\n# A tibble: 6 × 2\n   year total\n  &lt;int&gt; &lt;int&gt;\n1  1629  9901\n2  1630  9315\n3  1631  8524\n4  1632  9584\n5  1633  9997\n6  1634  9855\n\n\nWe can decompose this graphic using the grammar of graphics.\n\ndata: the data frame displayed above\naesthetic mappings: year is mapped to the x-axis, total is mapped to the y-axis\ngeometry: the observations are expressed as a line\n\nThis process makes clear what decisions were made in constructing the plot and suggests ways in which we might consider changing the graphic. What if we changed the geometry so that the observations are expressed as points?\n\n\n\n\n\n\n\n\n\nIs this a better graphic? That depends on the message you aim to convey. The line geometry emphasizes the general trend over time. The point geometry makes clearer the total births in each particular year.\nWhat if you wanted to convey both of those messages? If two geometries are compatible, they can be combined.\n\n\n\n\n\n\n\n\n\nIs this a better graphic? Again, that depends. It makes it possible to see both the trend over time and the individual observations. But this plot is more complicated and therefore articulates each of those individual messages less clearly. Sometimes simplicity and a single message is best.\nHere’s another example of a plot that combines two geometries:\n\n\n\n\n\nHere, a density geometry is overlaid on top of a histogram geometry. It is unclear whether this was a good design decision. What do you think? What is the advantage of a stair-step histogram versus the smooth line of a density curve?",
    "crumbs": [
      "Notes",
      "Summarization",
      "A Grammar of Graphics"
    ]
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#summary",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#summary",
    "title": "A Grammar of Graphics",
    "section": "Summary",
    "text": "Summary\nThe Grammar of Graphics is a framework to express a great variety of statistical graphics in terms of their shared elements. In this framework, the core features of the plot are the data, the aesthetic mapping between aesthetic attributions and variables in the data frame, and the geometry that is used to express the observation. There are a wide range of geometries and aesthetic attributes that can be drawn from and recombined in powerful ways. What we have done so far is cover just the fundamentals of the framework, so if you are unsatisfied with the resulting plots, that’s good. Now, we will polish up these plots to make thoughtful graphics that focus on effectively conveying a single message.\nContinue on to the tutorial portion of the notes",
    "crumbs": [
      "Notes",
      "Summarization",
      "A Grammar of Graphics"
    ]
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#footnotes",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#footnotes",
    "title": "A Grammar of Graphics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWilkinson, Leland. The grammar of graphics. Springer Science & Business Media, 2005.↩︎\nFor more context around The Grammar of Graphics and the development of modern visualization tools, read the brief Three Waves of Data Visualization by Elijah Meeks, Senior Data Visualization Engineer at Netflix: https://www.tableau.com/about/blog/2019/2/three-waves-data-visualization-brief-history-and-predictions-future-100830.↩︎\nThe ggplot2 package is described in the manuscript, A layered grammar of graphics, by Hadley Wickham in the Journal of Computational and Graphical Statistics in 2010.↩︎",
    "crumbs": [
      "Notes",
      "Summarization",
      "A Grammar of Graphics"
    ]
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#describing-shape",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#describing-shape",
    "title": "Summarizing Numerical Data",
    "section": "Describing Shape",
    "text": "Describing Shape\n\nWhich of these variables do you expect to be uniformly distributed?\n\nbill length of Gentoo penguins\nsalaries of a random sample of people from California\nhouse sale prices in San Francisco\nbirthdays of classmates (day of the month)\n\nPlease vote at pollev.com.\n\n\n  \n    −\n    +\n \n 01:00\n \n\nCorrect answer: 4.T\nThis lends itself very well to boardwork: you can ask a student to tell you how to draw the distribution birthdays. Topics to bring up:\n\nBarchart shows more information than a histgram (which would aggregate)\nFor discrete numerical, barchart is fine as long as the number of categories isn’t too great.\nBirthdays isn’t quite uniform: you would expect 29, 30, 31 to have fewer since not all months have those dayes.\n\nYou can continue the same approach to drawing the remaining three distribution: 1) bimodal (separated out by species) and symmetric (as demonstrated on the next slide) 2) unimodal and heavily right skewed 3) unimodal and right skewed."
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#mean-median-mode-which-is-best",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#mean-median-mode-which-is-best",
    "title": "Summarizing Numerical Data",
    "section": "Mean, median, mode: which is best?",
    "text": "Mean, median, mode: which is best?\n\n\nIt depends on your desiderata: the nature of your data and what you seek to capture in your summary.\n\n\n\n\nGet out a piece of paper. You’ll be watching a 3 minute video that discusses characteristics of a typical human. Note which numerical summaries are used and what for."
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-1",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-1",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "After the video, you can write on the board each of mean, median, mode and have students list measures that were reported. There are many modes because lots of the data that they collect is categorical."
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#general-advice",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#general-advice",
    "title": "Summarizing Numerical Data",
    "section": "General Advice",
    "text": "General Advice\n\n\nMeans are often a good default for symmetric data.\n\n\n\n\nMeans are sensitive to very large and small values, so can be deceptive on skewed data. &gt; Use a median\n\n\n\n\nModes are often the only option for categorical data.\n\n\n\nBut there are other notions of typical… what about a maximum?"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-2",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-2",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "This is an interesting example of when we don’t use a measure of central tendency to characterize the distribution. The answer to, “What will the temperature be tomorrow?” is generally answered not with an average but with a max. This is a good question to put to students: why do we characterize daily temperatures by their max?"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-4",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-4",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "This is a nice but not necessary example of an excellent visualization of summary statistics. Every dot is a max temp on a day of the year over a 40 year span. The annotations of the PWN heat dome are very effective in showing by how much they are the max of a distribution of maxes."
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-5",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-5",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Why are measures of spread so important? Consider the following question.\n\n\n\nThere are two new food delivery services that open in Berkeley: Oski Eats and Cal Cravings. A friend of yours that took Stat 20 collected data on each and noted that Oski Eats has a mean delivery time of 29 minutes and Cal Cravings a mean delivery time of 27 minutes. Which would would you rather order from?\n\n\nThis starts off as a softball, so you probably don’t want to give too much time or bother with a poll. Simply ask students why they voted for Cal Cravings and then give them a bit more information…"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#one-possible-reality",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#one-possible-reality",
    "title": "Summarizing Numerical Data",
    "section": "One possible reality",
    "text": "One possible reality"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-6",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-6",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Which would would you rather order from?\n\n\n  \n    −\n    +\n \n 01:00\n \n\nNow you can ask students to make a good argument for both options. Cal Cravings is shorter on average but is very variable; its good for the gamblers. For students who value predictability, Oski Eats is better because of its smaller spread."
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Man feigns madness, contemplates life and death, and seeks revenge.\nSon avenges his father, and it only takes four hours.\nA tragedy written by the English playwright around 1600.\n29,551 words on a page.\nYou may recognize each of these as summaries of the play, “Hamlet”. None of these are wrong, per se, but they do focus on very different aspects of the work. Summarizing something as rich and complex as Hamlet invariably involves a large degree of omission; we’re reducing a document of 29,551 words down to a single sentence or phrase, after all. But summarization also involves important choices around what to include.\nThe same considerations of omission and inclusion come into play when developing a numerical or graphical summary of a data set. Some guidance to bear in mind:\nWhat should I include?\nWhat should I omit?\nIn these notes we’ll keep this guidance in mind as we discuss how to summarize numerical data with graphics, in words, and with statistics. Specifically, we will learn how to:\nSummarizing two numerical variables is a task which deserves its own set of notes; that set of notes will come later on!",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Numerical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html#constructing-graphical-summaries",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html#constructing-graphical-summaries",
    "title": "Summarizing Numerical Data",
    "section": "Constructing Graphical Summaries",
    "text": "Constructing Graphical Summaries\nLet’s turn to an example admittedly less complex than Hamlet: the Palmer penguins. One of the numerical variables Dr. Gorman recorded was the length of the bill in millimeters. The values of the first 16 penguins are:\n\n\n\n\n\n\n\n\nbill_length_mm\n\n\n\n\n39.1\n\n\n39.5\n\n\n40.3\n\n\n36.7\n\n\n39.3\n\n\n38.9\n\n\n39.2\n\n\n41.1\n\n\n38.6\n\n\n34.6\n\n\n36.6\n\n\n38.7\n\n\n42.5\n\n\n34.4\n\n\n46.0\n\n\n37.8\n\n\n\n\n\nWe have many options for different plot types that we could use to summarize this data graphically. To understand the differences, it’s helpful to lay out the criterion that we hold for a summary to be a success. Let’s call those criteria the desiderata, a word meaning “that which is desired or needed”.\nFor our first graphic, let’s set a high bar.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nAll information must be preserved.\n\n\n\nThe most commonly used graphic that fulfills this criterion is the dot plot.\n\n\n\n\n\n\n\n\n\nThe dot plot is, in effect, a one-dimensional scatter plot. Each observation shows up as a dot and its value corresponds to its location along the x-axis. Importantly, it fulfills our desiderata: given this graphic, one can recreate the original data perfectly. There was no information loss.\nAs the number of observations grow, however, this sort of graphical summary becomes unwieldy. Instead of focusing on the value of each observation, it becomes more practical to focus on the general shape of the distribution. Let’s consider a broader goal for our graphic.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nBalance depiction of the general characteristics of the distribution with a fidelity to the individual observations.\n\n\n\nThere are several types of graphics that meet this criterion: the histogram, the density plot, and the violin plot.\n\nHistograms\n\n\n\n\n\n\n\n\n\nAt first glance, a histogram looks like deceptively like a bar chart. There are bars arranged along the x-axis according to their values with heights that correspond to the count of each value found in the data set. So how is this not a bar chart?\nA histogram involves aggregation. The first step in creating a histogram is to divide the range of the variable into bins of equal size. The second step is to count up the number of observations that occur in each bin. In this way, some observations will have their own bar (every bar with a count of one) but other observations will be aggregated into the same bar: the tallest bar, with a count of 3, corresponds to all observations from 39.09 to 39.30: 39.1, 39.2, and 39.3.\nThe degree of aggregation performed by the histogram is determined by the binwidth. Most software will automatically select the binwidth1, but it can be useful to tinker with different values to see the distribution at different levels of aggregation. Here are four histograms of the same data that use four different binwidths.\n\n\n\n\n\n\n\n\n\nIf you are interested in only the coarsest structure in the distribution, best to use the larger binwidths. If you want to see more detailed structure, a smaller binwidth is better.\nThere is a saying that warns about times when you, “can’t see the forest for the trees”, being overwhelmed by small details (the trees) and unable to see the bigger picture (the forest). The histogram, as a graphical tool for summarizing the distribution of a numerical variable, offers a way out. Through your choice of binwidth, you can determine how much to focus on the forest (large bindwidth) or the trees (small binwidth).\n\n\nDensity plots\nImagine that you build a histogram and place a cooked piece of spaghetti over the top of it. The curve created by the pasta is a form of a density plot.\n\n\n\n\n\n\n\n\n\nBesides the shift from bars to a smooth line, the density plot also changes the y-axis to feature a quantity called “density”. We will return to define this term later in the course, but it’s sufficient to know that the values on the y-axis of a density plot are rarely useful. The important information is relative: an area of the curve with twice the density as another area has roughly twice the number of observations.\nThe density plot, like the histogram, offers the ability to balance fidelity to the individual observations against a more general shape of the distribution. You can tip the balance to feature what you find most interesting by adjusting the bandwidth of the density plot.\n\n\n\n\n\n\n\n\n\nA density curve tends to convey the overall shape of a distribution more quickly than does a histogram, but be sure to experiment with different bandwidths. Strange but important features of a distribution can be hidden behind a density curve that is too smooth.\n\n\nViolin plots\nOften we’re interested not in the distribution of a single variable, but in the way the distribution of that variable changes from one group of observational units to another. Let’s add this item to our list of criteria for a statistical graphic.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nBalance depiction of the general characteristics of the distribution with a fidelity to the individual observations.\nAllow for easy comparisons between groups.\n\n\n\nThere are several different ways to compare the distribution of a variable across two or more groups, but one of the most useful is the violin plot. Here is a violin plot of the distribution of bill length across the three species of penguins.\n\n\n\n\n\n\n\n\n\nThe distribution of bill length in each species is represented by a shape that often looks like a violin but is in fact a simple density curve reflected about its x-axis. This means that you can tinker with a violin plot the same as a density plot, but changing the bandwidth.\nIf this plot type looks familiar, you may have seen its cousin, the box plot.\n\n\n\n\n\n\n\n\n\nThe box plot conveys a similar story to the violin plot: Adelies have shorter bills than Chinstraps and Gentoos. Box plots have the advantage of requiring very little computation to construct2, but in a world of powerful computers, that is no longer remarkable. What they lack is a “smootness-knob” that you can turn to perform more or less smoothing. For this reason, violin plots are a more flexible alternative to box plots.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Numerical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html#describing-distributions",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html#describing-distributions",
    "title": "Summarizing Numerical Data",
    "section": "Describing Distributions",
    "text": "Describing Distributions\nThe desideratum that we used to construct the histogram and the violin plot include the ability to “depict general characteristics of the distribution”. The most important characteristics of a distribution are its shape, center, and spread.\nWhen describing the shape of a distribution in words, pay attention to its modality and skew. The modality of a distribution captures the number of distinct peaks (or modes) that are present.\n\n\n\n\n\nA good example of a distribution that would be described as unimodal is the original density plot of bill lengths of 16 Adelie penguins (below left). There is one distinct peak around 39. Although there is another peak around 34, it is not prominent enough to be considered a distinct mode. The distribution of the bill lengths of all 344 penguins (below right), however, can be described as bimodal.\n\n\n\n\n\n\n\n\n\nMultiple modes are often a hint that there is something more going on. In the plot to the right above, Chinstraps and Gentoo penguins, which are larger, are clumped under the right mode while the smaller Adelie penguins are dominant in the left mode.\nThe other important characteristic of the shape of a distribution is its skew.\n\n\n\n\n\nThe skew of a distribution describes the behavior of its tails: whether the right tail stretches out (right skew), the left tail stretches out (left skew), or if both tails are of similar length (symmetric). An example of a persistently right skewed distribution is household income in the United States:\n\n\n\n\n\nIn the US, the richest households have much much higher incomes than most, while the poorest households have incomes that are only a bit lower than most.\nWhen translating a graphical summary of a distribution into words, some degree of judgement must be used. When is a second peak a mode and when is it just a bump in the distribution? When is one of the tails of a distribution long enough to tip the description from being symmetric to being right skewed? You’ll hone your judgement in part through repeated practice: looking at lots of distributions and readings lots of descriptions. You can also let the questions of inclusion and omission be your guide. Is the feature a characteristic relevant to the question you’re answering and the phenomenon you’re studying? Or is it just a distraction from the bigger picture?\nModality and skew capture the shape of the distribution, but how do we describe its center and spread? “Eyeballing it” by looking at a graphic is an option. A more precise option, though, is to calculate a statistic.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Numerical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html#constructing-numerical-summaries",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html#constructing-numerical-summaries",
    "title": "Summarizing Numerical Data",
    "section": "Constructing Numerical Summaries",
    "text": "Constructing Numerical Summaries\nStatistics is both an area of academic study and the object of that study. Any numerical summary of a data set - a mean or median, a count or proportion - is a statistic. A statistic is, fundamentally, a mathematical function where the data is the input and the output is the observed statistic.\n\n\n\nStatisticians don’t just study statistics, though, they construct them. A statistician gets to decide the form of \\(f\\) and, as with graphics, they construct it to fulfill particular needs: the desiderata.\nTo examine the properties of common statistics, let’s move to an even simpler data set: a vector called x that holds 11 integers.\n\\[8, 11, 7, 7, 8, 11,  9,  6,  10,  7,  9\\]\n\n\n\nMeasures of Center\nThe mean, the median, and the mode are the three standard statistics used to measure the center of a distribution. Despite their ubiquity, these three are not carved somewhere on a stone tablet. They’re common because they’re useful and they’re useful because they were constructed very thoughtfully.\nLet’s start by laying out some possible criteria for a measure of center.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nSynthesizes the magnitudes of all \\(n\\) observations.\nAs close as possible to all of the observations.\n\n\n\nThe (arithmetic) mean fulfills all of these needs.\n\n\nThe function in R: mean()\n\\[\n\\frac{8 + 11 + 7 + 7 + 8 + 11 + 9 + 6 + 10 + 7 + 9}{11} = \\frac{93}{11} = 8.45\n\\]\nThe mean synthesizes the magnitudes by taking their sum, then keeps that sum from getting larger than any of the observations by dividing by \\(n\\). In order to express this function more generally, we use the following notation\n\\[\n\\bar{x} = \\frac{x_1 + x_2 + \\ldots + x_n}{n}\n\\]\nwhere \\(x_1\\) is the first observation, \\(x_2\\) is the second observation, and so on till the \\(n^{th}\\) observation, \\(x_n\\); and \\(\\bar{x}\\) is said “x bar”.\nThe mean is the most commonly used measure of center, but has one notable drawback. What if one of our observations is an outlier, that is, has a value far more extreme than the rest of the data? Let’s change the \\(6\\) to \\(-200\\) and see what happens.\n\\[\n\\frac{8 + 11 + 7 + 7 + 8 + 11 + 9 - 200 + 10 + 7 + 9}{11} = \\frac{-113}{11} = -10.27\n\\]\nThe mean has plummeted to -10.27, dragged down by this very low outlier. While it is doing it’s best to stay “as close as possible to all of the observations”, it isn’t doing a very good job of representing 10 of the 11 values.\nWith this in mind, let’s alter the first criterion to inspire a different statistic.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nSynthesize the order of all \\(n\\) observations.\nAs close as possible to all of the observations.\n\n\n\nIf we put the numbers in order from smallest to largest, then the number that is as close as possible to all observations will be the middle number, the median.\n\n\nThe function in R: median()\n\\[ 6 \\quad 7 \\quad 7 \\quad  7 \\quad 8 \\quad {\\Large 8} \\quad 9 \\quad 9 \\quad 10 \\quad 11  \\quad 11\\]\nAs measured by the median, the center of this distribution is 8 (recall the mean measured 8.45). If there were an even number of observations, the convention is to take the mean of the middle two values.\nThe median has the desirable property of being resistant (or “robust”) to the presence of outliers. See how it responds to the inclusion of -200.\n\\[ -200 \\quad 7 \\quad 7 \\quad  7 \\quad 8 \\quad {\\Large 8} \\quad 9 \\quad 9 \\quad 10 \\quad 11  \\quad 11\\]\nWith this outlier, the median remains at 8 while the mean had dropped to -10.27. This property makes the median the favored statistic for capturing the center of a skewed distribution.\nWhat if we took a stricter notion of “closeness”?\n\n\n\n\n\n\nDesiderata\n\n\n\n\nIs identical to as many observations as possible.\n\n\n\nThat leads us to the measure of the mode, or the most common observation. For our example data set, the mode is \\(7\\).\n\\[ 6 \\quad {\\Large 7 \\quad 7 \\quad  7} \\quad 8 \\quad 8 \\quad 9 \\quad 9 \\quad 10 \\quad 11  \\quad 11\\]\nWhile using the mode for this data set is sensible, it is common in numerical data for each value to be unique3. In that case, there are no repeated values, and no identifiable mode. For that reason, it is unusual to calculate the mode to describe the center of a numerical variable.\nFor categorical data, however, the mode is very useful. The mode of the species variable among the Palmer penguins is “Adelie”. Trying to compute the mean or the median species will leave you empty-handed. This is one of the lingering lessons of the Taxonomy of Data: the type of variable guides how it is analyzed.\n\n\nMeasures of Spread\nThere are many different ways to capture spread or dispersion of data. Here are some basic desiderata we might hope to achieve with a measure of spread.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nThe statistic should be low when the numbers are the same or very similar to one another.\nThe statistic should be high when the numbers are very different.\nThe statistic should not grow or shrink with the sample size ( n ).\n\n\n\nThese desiderata are not met by every measure of spread. Here is one such measure of spread which does not meet all three!\n\nRange\nThe range of a set of numbers is simply the maximum number in the dataset minus its minimum.\n\\[ range = max - min\\]\nFor our set of numbers, the range will be \\(5\\).\n\\[ {\\Large 6} \\quad {\\ 7 \\quad 7 \\quad  7} \\quad 8 \\quad 8 \\quad 9 \\quad 9 \\quad 10 \\quad 11  \\quad {\\Large 11}\\]\nAlthough the first two criterion above are met by the range, the third one is not. The reason is that if we add a number to our set which is greater than the maximum value, or smaller than the minimum value, the value of range will change. Therefore, increasing our sample size \\(n\\) could cause our statistic to grow or shrink.\nHere are some measures of spread which do meet all three criterion. They do this by using incorporating some of the measures of center that we have already talked about, such as the mean and the median.\n\n\nThe Sample Variance\nThe sample variance:\n\nTakes the differences from each observation, \\(x_i\\), to the sample mean \\(\\bar{x}\\);\nsquares them;\nadds them all up;\nthen divides by \\(n - 1\\).\n\n\n\nThe function in R: var()\n\\[ s^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( x_i - \\bar{x}\\right)^2 \\] This formula is rather dense; don’t worry! We won’t ask you to memorize it. The key is that is fits the three criterion we were hoping for.\nThe distance \\((x_i - \\bar{x})^2\\) will be small when \\(x_i\\) is close to the mean, and large when it’s not. This means the first two criterion are met. Additionally, because we are dividing by a number close to \\(n\\), (\\(n-1\\)), the statistic does not grow or shrink with \\(n\\). This means the last criterion is met!\nOne other question you might have: why the square?\nThe reason is that spread/distance is a positive quantity. Recall that the mean of our list was \\(8.45\\). Two numbers in our list, \\(7\\) and \\(9\\), are both \\(1.45\\) units away from the mean. However, \\(7-8.45 = -1.45\\) and \\(9-8.45 = 1.45\\). As part of the sum, we will have to add up \\(-1.45\\) and \\(1.45\\), which comes out to \\(0\\).\nThis means that the information from the two data points \\(7\\) and \\(9\\) have been canceled out! We don’t want this to happen, so we need to make all of the terms in the sum positive. The square takes care of that.\nFor our list:\n\\[ s^2 =  2.87 \\]\n\n\nThe Sample Standard Deviation\n\\[ s = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( x_i - \\bar{x}\\right)^2} \\]\n\n\nThe function in R: sd()\nThe sample standard deviation \\(s\\) is the same as the sample variance \\(s^2\\): we’ve just taken the square root. One reason for using the sample standard deviation is that it at times is more interpret-able than the sample variance, since it’s measured in units rather than units squared.\nFor our list:\n\\[ s =  1.69 \\] We can say, therefore, that each data point is about 1.7 units apart from each other.\n\nWhile the sample variance and sample standard deviation are great for measuring symmetric data (which appear enough in statistics) and also show up a lot in the theory of some topics that we will later discover, they do have their faults.\nNamely, when data is not symmetric, the square around \\(x_i - \\bar{x}\\) can cause some issues. Asymmetrical data will have many values \\(x_i\\) (large or small) which are far from the mean \\(\\bar{x}\\).\nIf \\(x_i - \\bar{x}\\) is large, then \\((x_i - \\bar{x})^2\\) will be very large. Therefore, \\(s^2\\) and \\(s\\) can produce values that are overestimates of the spread of most of the data. Here are two measures of spread which counter-act this.\n\n\nIQR (Interquartile Range)\nThe IQR is the difference between the median of the larger half of the sorted data set, \\(Q3\\), and the median of the smaller half, \\(Q1\\).\n\n\nThe function in R: IQR()\n \n\\[ IQR = Q_3 - Q_1 \\]  \nLet’s calculate the IQR for the list of eleven numbers we’ve been working with. First, we find the median of our dataset. That’s \\(8\\). Then, we split the data into two halves of five. Then we find the medians of these halves. and take the difference. These steps are visualized below.\n \n\\[ 6 \\quad 7 \\quad {\\Large 7 \\quad  7} \\quad 8 \\quad  {\\large 8} \\quad 9 \\quad {\\Large 9 \\quad 10} \\quad 11  \\quad 11\\]  \nWe have that \\(Q_3 = 9.5\\) and \\(Q_1 = 7\\). Then:\n \n\\[IQR =  9.5−7 = 2.5\\]\nThe reason the IQR works well for asymmetric data is because the measure of center it’s based on is the median, not the mean. The median, being just the middle point of the data and not a value obtained by calculation of all the numbers in a list, is not impacted when extreme values are tacked onto the end of the list.\n\n\n\n\n\n\nAdditional Desiderata\n\n\n\n\nIs robust to extreme values (outliers).\n\n\n\nOur final measure of spread is another option which is resillent against outliers, but is based off of the mean instead of the median.\n\n\nMean Absolute Deviation (MAD)\nThe \\(MAD\\) is very similar to the sample variance \\(s^2\\), except that:\n\nwe divide by \\(n\\) rather than \\(n-1\\);\nwe take the absolute value of \\(x_i - \\bar{x}\\) instead of squaring it.\n\n\\[\nMAD: \\quad \\frac{1}{n}\\sum_{i = 1}^n |x_i - \\bar{x}|\n\\]\nThe key difference is the second one. The MAD is great for summarizing skewed distributions because it isn’t bothered too much by the presence of extreme values in a set of numbers. That’s because the absolute value bar just ensures a number is positive; it doesn’t further amplify that number by squaring it.\n\n\n\n\n\n\nAdditional Desiderata\n\n\n\n\nIs robust to extreme values (outliers).",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Numerical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html#summary",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html#summary",
    "title": "Summarizing Numerical Data",
    "section": "Summary",
    "text": "Summary\nA summary of a summaries…this better be brief! Summaries of numerical data - graphical and numerical - often involve choices of what information to include and what information to omit. These choices involve a degree of judgement and knowledge of the criteria that were used to construct the commonly used statistics and graphics.\nContinue on to the tutorial portion of the notes",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Numerical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html#footnotes",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html#footnotes",
    "title": "Summarizing Numerical Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ggplot2 package in R defaults to 30 bins across the range of the data. That’s a very rough rule-of-thumb, so tinkering is always a good idea.↩︎\nTo read more about one common way to construct a box plot, see the ggplot2 documentation.↩︎\nThat is, unless you aggregate! The aggregation performed by a histogram or a density plot is what allows us to describe numerical data as unimodel or bimodal.↩︎",
    "crumbs": [
      "Notes",
      "Summarization",
      "Summarizing Numerical Data"
    ]
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#section",
    "href": "2-summarizing-data/04-conditioning/slides.html#section",
    "title": "Conditioning",
    "section": "",
    "text": "c(\"fruit\", \"fruit\", \"vegetable\") == \"fruit\"\n\n\nWhat will this line of code return?\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#evaluating-equivalence-cont.",
    "href": "2-summarizing-data/04-conditioning/slides.html#evaluating-equivalence-cont.",
    "title": "Conditioning",
    "section": "Evaluating equivalence, cont.",
    "text": "Evaluating equivalence, cont.\nIn R, this evaluation happens element-wise when operating on vectors.\n\nc(\"fruit\", \"fruit\", \"vegetable\") == \"fruit\"\n\n\n\n[1]  TRUE  TRUE FALSE\n\n\n\n\nc(\"fruit\", \"fruit\", \"vegetable\") != \"fruit\"\n\n\n\n\n\n[1] FALSE FALSE  TRUE\n\n\n\n\n\nc(\"fruit\", \"vegetable\", \"boba\") %in% c(\"fruit\", \"vegetable\")\n\n\n\n\n\n[1]  TRUE  TRUE FALSE"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#question-2",
    "href": "2-summarizing-data/04-conditioning/slides.html#question-2",
    "title": "Conditioning",
    "section": "Question 2",
    "text": "Question 2\n\nclass_survey |&gt;\n  filter(coding_exp_scale &lt; 3,\n         olympics %in% c(\"Ice skating\", \"Speed skating\"),\n         is_entrepreneur == TRUE)\n\n\nWhich observations will be included in the following data frame?\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#question-3",
    "href": "2-summarizing-data/04-conditioning/slides.html#question-3",
    "title": "Conditioning",
    "section": "Question 3",
    "text": "Question 3\n\nWhich data frame will have fewer rows?\n\n\n\n# A\nfilter(class_survey, time_at_cal == \"This is my first semester!\")\n\n# B\nclass_survey |&gt;\n  mutate(first_sem = (time_at_cal == \"This is my first semester!\")) |&gt;\n  filter(first_sem)\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#building-data-pipelines",
    "href": "2-summarizing-data/04-conditioning/slides.html#building-data-pipelines",
    "title": "Conditioning",
    "section": "Building data pipelines",
    "text": "Building data pipelines\nConsider the subset of students here:\n\nclass_survey |&gt;\n  filter(coding_exp_scale &lt; 3,\n         olympics %in% c(\"Ice skating\", \"Speed skating\"),\n         is_entrepreneur == TRUE)\n\n\nHow do we extract the average of these students’ chance that class will be disrupted by a new COVID variant?\n\nLet’s look at three different ways to answer this question"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#nesting",
    "href": "2-summarizing-data/04-conditioning/slides.html#nesting",
    "title": "Conditioning",
    "section": "Nesting",
    "text": "Nesting\n\nfilter(class_survey, \n       coding_exp_scale &lt; 3,\n       olympics %in% c(\"Ice skating\", \"Speed skating\"),\n       is_entrepreneur == TRUE)\n\n\nThis set of slides walks through the three different approaches to stringing together functions in R, identifies the pros and cons of each, and encourages students to use the pipe."
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#nesting-1",
    "href": "2-summarizing-data/04-conditioning/slides.html#nesting-1",
    "title": "Conditioning",
    "section": "Nesting",
    "text": "Nesting\n\nselect(filter(class_survey, \n       coding_exp_scale &lt; 3,\n       olympics %in% c(\"Ice skating\", \"Speed skating\"),\n       is_entrepreneur == TRUE),\n       coding_exp_xcale,\n       olympics,\n       is_entrepreneur,\n       covid)"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#nesting-2",
    "href": "2-summarizing-data/04-conditioning/slides.html#nesting-2",
    "title": "Conditioning",
    "section": "Nesting",
    "text": "Nesting\n\nsummarize(select(filter(class_survey, \n       coding_exp_scale &lt; 3,\n       olympics %in% c(\"Ice skating\", \"Speed skating\"),\n       is_entrepreneur == TRUE),\n       coding_exp_scale,\n       olympics,\n       is_entrepreneur,\n       covid),\n       covid_avg = mean(covid))"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#nesting-3",
    "href": "2-summarizing-data/04-conditioning/slides.html#nesting-3",
    "title": "Conditioning",
    "section": "Nesting",
    "text": "Nesting\n\nsummarize(select(filter(class_survey, \n       coding_exp_scale &lt; 3,\n       olympics %in% c(\"Ice skating\", \"Speed skating\"),\n       is_entrepreneur == TRUE),\n       coding_exp_scale,\n       olympics,\n       is_entrepreneur,\n       covid),\n       covid_avg = mean(covid))\n\n# A tibble: 1 × 1\n  covid_avg\n      &lt;dbl&gt;\n1     0.428"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#nesting-4",
    "href": "2-summarizing-data/04-conditioning/slides.html#nesting-4",
    "title": "Conditioning",
    "section": "Nesting",
    "text": "Nesting\n\nsummarize(select(filter(class_survey, \n       coding_exp_scale &lt; 3,\n       olympics %in% c(\"Ice skating\", \"Speed skating\"),\n       is_entrepreneur == TRUE),\n       coding_exp_scale,\n       olympics,\n       is_entrepreneur,\n       covid),\n       covid_avg = mean(covid))\n\n\n\n\nCons\n\nMust be read from inside out 👎\nHard to keep track of arguments 👎\n\n\nPros\n\nAll in one line of code 👍\nOnly refer to one data frame 👍"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#step-by-step",
    "href": "2-summarizing-data/04-conditioning/slides.html#step-by-step",
    "title": "Conditioning",
    "section": "Step-by-step",
    "text": "Step-by-step\n\n\ndf1 &lt;- filter(class_survey, \n              coding_exp_scale &lt; 3,\n              olympics %in% c(\"Ice skating\", \"Speed skating\"),\n              is_entrepreneur == TRUE)\ndf2 &lt;- select(df1, \n              coding_exp_scale,\n              olympics,\n              is_entrepreneur,\n              covid)\nsummarize(df2,\n          covid_avg = mean(covid))\n\n\n\n\n\nCons\n\nHave to repeat data frame names 👎\nCreates unnecessary objects 👎\n\n\nPros\n\nStores intermediate objects 👍\nCan be read top to bottom 👍"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#using-the-pipe-operator",
    "href": "2-summarizing-data/04-conditioning/slides.html#using-the-pipe-operator",
    "title": "Conditioning",
    "section": "Using the pipe operator",
    "text": "Using the pipe operator\n\n\nclass_survey |&gt;\n  filter(coding_exp_scale &lt; 3,\n         olympics %in% c(\"Ice skating\", \"Speed skating\"),\n         is_entrepreneur == TRUE) |&gt;\n  select(coding_exp_scale,\n         olympics,\n         is_entrepreneur,\n         covid) |&gt;\n  summarize(covid_avg = mean(covid))\n\n\n\nCons\n\n🤷\n\n\nPros\n\nCan be read like an english paragraph 👍\nOnly type the data once 👍\nNo leftovers objects 👍"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#understanding-your-pipeline",
    "href": "2-summarizing-data/04-conditioning/slides.html#understanding-your-pipeline",
    "title": "Conditioning",
    "section": "Understanding your pipeline",
    "text": "Understanding your pipeline\n\nIt’s good practice to understand the output of each line of code by breaking the pipe.\n\n\n\n\n\nclass_survey |&gt;\n  select(covid) |&gt;\n  filter(time_at_cal == \"It's my first time_at_cal.\")\n\nError in `filter()`:\nℹ In argument: `time_at_cal == \"It's my first time_at_cal.\"`.\nCaused by error:\n! object 'time_at_cal' not found\n\n\n\n\nclass_survey |&gt;\n  select(covid)\n\n# A tibble: 816 × 1\n   covid\n   &lt;dbl&gt;\n 1  0   \n 2  0.5 \n 3  0.6 \n 4  0.7 \n 5 NA   \n 6  0.15\n 7  0.7 \n 8  0   \n 9  0.8 \n10 NA   \n# ℹ 806 more rows"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#concept-question",
    "href": "2-summarizing-data/04-conditioning/slides.html#concept-question",
    "title": "Conditioning",
    "section": "Concept Question",
    "text": "Concept Question\n\nclass_survey |&gt; # A #&lt;&lt;\n  filter(coding_exp_scale &lt; 3,\n         olympics %in% c(\"Ice skating\", \n                         \"Speed skating\"),\n         is_entrepreneur == TRUE) |&gt; # B #&lt;&lt;\n  select(coding_exp_scale,\n         olympics,\n         is_entrepreneur,\n         covid) |&gt; # C #&lt;&lt;\n  summarize(covid_avg = mean(covid)) # D #&lt;&lt;\n\n\n# note\ndim(class_survey)\n\n[1] 816  30\n\n\n\nWhat are the dimensions (rows x columns) of the data frames output at each stage of this pipe?\n\n\n  \n    −\n    +\n \n 01:00"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#section-2",
    "href": "2-summarizing-data/04-conditioning/slides.html#section-2",
    "title": "Conditioning",
    "section": "",
    "text": "summarize(class_survey,\n          mean(time_at_cal == \"I'm in my first year.\", na.rm = TRUE))\n\n\nWhat is will this line of code return?\n\n\n  \n    −\n    +\n \n 01:00\n \n\nThis will return the proportion of students who answered “I’m in my first year”. This is surprising to students because it’s not immediately clear that a mean of a logical vector is the same as the proportion of cases that are true. ::\n\nBoolean Algebra\nLogical vectors have a dual representation as TRUE FALSE and 1, 0, so you can do math on logicals accordingly.\n\nTRUE + TRUE\n\n[1] 2\n\n\n\n\nTRUE * TRUE\n\n[1] 1\n\n\n\n\n\nTaking the mean of a logical vector is equivalent to find the proportion of rows that are TRUE (i.e. the proportion of rows that meet the condition).\n\n\n\n\nWorksheet: Conditioning\n\n  \n    −\n    +\n \n 20:00\n \n\n\n\nBreak\n\n  \n    −\n    +\n \n 05:00\n \n\n\n\nLab Part I: Flights\n\n  \n    −\n    +\n \n 25:00"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#boolean-algebra",
    "href": "2-summarizing-data/04-conditioning/slides.html#boolean-algebra",
    "title": "Conditioning",
    "section": "Boolean Algebra",
    "text": "Boolean Algebra\nLogical vectors have a dual representation as TRUE FALSE and 1, 0, so you can do math on logicals accordingly.\n\nTRUE + TRUE\n\n[1] 2\n\n\n\n\nTRUE * TRUE\n\n[1] 1\n\n\n\n\n\nTaking the mean of a logical vector is equivalent to find the proportion of rows that are TRUE (i.e. the proportion of rows that meet the condition)."
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html",
    "href": "2-summarizing-data/04-conditioning/notes.html",
    "title": "Conditioning",
    "section": "",
    "text": "In the world of data, bigger is not always better. Sometimes there are real benefits to working with a subset of your observations that meet some particular condition. One use of conditioning is to add specificity to a claim. Another use of conditioning is to illuminate the relationship between variables.\nTo gain practice with conditioning, let’s turn to a data set that begins with a very general focus. In 2007, Savage and West published A qualitative, theoretical framework for understanding mammalian sleep1, wherein they “develop a general, quantitative theory for mammalian sleep that relates many of its fundamental parameters to metabolic rate and body size”. Characterizing the sleep patterns of all mammals is a broad task and their data set is corresponding diverse. Take a look at the first ten rows of their data below.\n# A tibble: 83 × 5\n   name                       sleep_total log_bodywt vore  conservation\n   &lt;chr&gt;                            &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n 1 Cheetah                           12.1      10.8  carni lc          \n 2 Owl monkey                        17         6.17 omni  &lt;NA&gt;        \n 3 Mountain beaver                   14.4       7.21 herbi nt          \n 4 Greater short-tailed shrew        14.9       2.94 omni  lc          \n 5 Cow                                4        13.3  herbi domesticated\n 6 Three-toed sloth                  14.4       8.26 herbi &lt;NA&gt;        \n 7 Northern fur seal                  8.7       9.93 carni vu          \n 8 Vesper mouse                       7         3.81 &lt;NA&gt;  &lt;NA&gt;        \n 9 Dog                               10.1       9.55 carni domesticated\n10 Roe deer                           3         9.60 herbi lc          \n# ℹ 73 more rows\nIn this data set, the unit of observation is a single species and the variables observed on each are its name, the average length of sleep each day, the natural log of the average weight, its dietary pattern, and its conservation status. We can visualize the relationship between sleep and body size in all 83 species using a scatter plot.\nThe mammals vary from the wee brown bat, slumbering for nearly 20 hours a day, to the massive African elephant, nodding off for less than five. That is quite a range! Lets drill down to smaller subsets of this data frame to gain a more nuanced sense of what is going on.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Conditioning"
    ]
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html#filtering",
    "href": "2-summarizing-data/04-conditioning/notes.html#filtering",
    "title": "Conditioning",
    "section": "Filtering",
    "text": "Filtering\nIf you think about the shape of a data frame, there are two basic ways you might go about slicing and dicing it into smaller subsets.\nOne way is to go at it is column-by-column. The act of selecting a subset of the columns of a data frame is called, well, selecting. When you select a column, you can do so either by its name or by its column number (or index). Selecting columns by name is more useful because their order tends to be arbitrary and might change over the course of an analysis.\nThe other way to go at it is row-by-row. The act of subsetting the rows of the data frame based on their row number is called slicing. As with columns, the order of the rows is also often arbitrary, so this is of limited use. Much more useful is filtering.\n\n\nIn the tidyverse, these functions are named select(), slice(), and filter().\n\nFiltering\n\nThe act of subsetting the rows of a data frame based on the values of one or more variables to extract the observations of interest.\n\n\nFilters are powerful because they comb through the values of the data frame, which is where most of the information is. The key part of any filter is the condition that you assert for the rows that are retained in your data frame. Let’s set up a filter to return only the little brown bat.\n\nfilter(msleep, name == \"Little brown bat\")\n\n# A tibble: 1 × 5\n  name             sleep_total log_bodywt vore    conservation\n  &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       \n1 Little brown bat        19.9       2.30 insecti &lt;NA&gt;        \n\n\nHere name == \"Little brown bat\" is the condition that must be met by any row in the data set to be retained. The syntax used to set up the condition is a comparison between a column in the data frame on the left and a possible value of that column on the right.\n\nComparison Operators\nThe filter above uses the most direct condition: it retains the rows that have a value in the name variable that is precisely \"Little brown bat\". In this case, there is only one such row. There are a range of different comparisons that can be made, though, and each has its own operator.\n\n\n\n\n\n\n\nOperator\nTranslation\n\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&lt;\nless than\n\n\n&gt;\ngreater than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\ngreater than or equal to\n\n\n\n\n\n\n\nAt first, the == operator looks like a typo. Why doesn’t we use =? The reason is that a single equals sign is already busy at work in R: it sets the values of arguments inside a function. Instead of assignment, we want to determine whether the thing on the left holds the same value as the thing on the right, so we use ==. It might help you keep things straight if you read it in your head as “is exactly equal to”.\nLet’s return only the rows with large animals, defined as those with a log body weight greater than 12.\n\nfilter(msleep, log_bodywt &gt; 12)\n\n# A tibble: 9 × 5\n  name                 sleep_total log_bodywt vore  conservation\n  &lt;chr&gt;                      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n1 Cow                          4         13.3 herbi domesticated\n2 Asian elephant               3.9       14.8 herbi en          \n3 Horse                        2.9       13.2 herbi domesticated\n4 Donkey                       3.1       12.1 herbi domesticated\n5 Giraffe                      1.9       13.7 herbi cd          \n6 Pilot whale                  2.7       13.6 carni cd          \n7 African elephant             3.3       15.7 herbi vu          \n8 Brazilian tapir              4.4       12.2 herbi vu          \n9 Bottle-nosed dolphin         5.2       12.1 carni &lt;NA&gt;        \n\n\nThere were 9 such animals and you can see all of them are large.\n\n\nLogical Operators\nWhat if you want both the little brown bat and the African elephant? What if you want both the large creatures as well as those that sleep only briefly? These are tasks that call for multiple comparisons composed together with the logical operators &, |, and %in%.\nThis filter returns the creatures who are large and who sleep little.\n\nfilter(msleep, log_bodywt &gt; 12 & sleep_total &lt; 5)\n\n# A tibble: 8 × 5\n  name             sleep_total log_bodywt vore  conservation\n  &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n1 Cow                      4         13.3 herbi domesticated\n2 Asian elephant           3.9       14.8 herbi en          \n3 Horse                    2.9       13.2 herbi domesticated\n4 Donkey                   3.1       12.1 herbi domesticated\n5 Giraffe                  1.9       13.7 herbi cd          \n6 Pilot whale              2.7       13.6 carni cd          \n7 African elephant         3.3       15.7 herbi vu          \n8 Brazilian tapir          4.4       12.2 herbi vu          \n\n\nThis can be read as “filter the msleep data frame to return the rows where both the log body weight is greater than 12 and the sleep total is less than 5”. We see that there are 8 such creatures, one fewer than the data frame with only the body weight filter (bottle-nosed dolphins sleep, on average, 5.2 hrs).\nUsing & to represent “and” is common across most computer languages but you can alternatively use the somewhat more compact syntax of simply adding the second filter after a comma.\n\nfilter(msleep, log_bodywt &gt; 12, sleep_total &lt; 5)\n\n# A tibble: 8 × 5\n  name             sleep_total log_bodywt vore  conservation\n  &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n1 Cow                      4         13.3 herbi domesticated\n2 Asian elephant           3.9       14.8 herbi en          \n3 Horse                    2.9       13.2 herbi domesticated\n4 Donkey                   3.1       12.1 herbi domesticated\n5 Giraffe                  1.9       13.7 herbi cd          \n6 Pilot whale              2.7       13.6 carni cd          \n7 African elephant         3.3       15.7 herbi vu          \n8 Brazilian tapir          4.4       12.2 herbi vu          \n\n\nThese two methods are equivalent.\nTo return all rows that either have a high body weight or low sleep time or both, use the | operator (sometimes called “vertical bar”).\n\nfilter(msleep, log_bodywt &gt; 12 | sleep_total &lt; 5)\n\n# A tibble: 12 × 5\n   name                 sleep_total log_bodywt vore  conservation\n   &lt;chr&gt;                      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n 1 Cow                          4        13.3  herbi domesticated\n 2 Roe deer                     3         9.60 herbi lc          \n 3 Asian elephant               3.9      14.8  herbi en          \n 4 Horse                        2.9      13.2  herbi domesticated\n 5 Donkey                       3.1      12.1  herbi domesticated\n 6 Giraffe                      1.9      13.7  herbi cd          \n 7 Pilot whale                  2.7      13.6  carni cd          \n 8 African elephant             3.3      15.7  herbi vu          \n 9 Sheep                        3.8      10.9  herbi domesticated\n10 Caspian seal                 3.5      11.4  carni vu          \n11 Brazilian tapir              4.4      12.2  herbi vu          \n12 Bottle-nosed dolphin         5.2      12.1  carni &lt;NA&gt;        \n\n\nBe cautious in deciding whether you want to use & or |. While | is generally read as “or”, we could also describe the above filter as one that returns the rows that have a high body weight and the rows that have low sleep times.\nOne way to keep them straight is to keep an eye on the number of observations that are returned. The intersection of multiple conditions (using &) should result in the same or fewer rows (the orange area) than the union of multiple conditions (using |) (the blue area).\n\n\n\n\n\n\n\n\n\nWhen working with nominal categorical variables, the only operator that you’ll be using is ==. You can return a union like normal using |,\n\nfilter(msleep, name == \"Little brown bat\" | name == \"African elephant\")\n\n# A tibble: 2 × 5\n  name             sleep_total log_bodywt vore    conservation\n  &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       \n1 African elephant         3.3      15.7  herbi   vu          \n2 Little brown bat        19.9       2.30 insecti &lt;NA&gt;        \n\n\nOr you can save some typing (and craft more readable code) by using %in% instead:\n\nfilter(msleep, name %in% c(\"Little brown bat\", \"African elephant\"))\n\n# A tibble: 2 × 5\n  name             sleep_total log_bodywt vore    conservation\n  &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       \n1 African elephant         3.3      15.7  herbi   vu          \n2 Little brown bat        19.9       2.30 insecti &lt;NA&gt;        \n\n\n\n\nTaxonomy of Data: Logicals\nIt is useful to pause here to look under the hood of this code. Once you get accustomed to the comparison operators and the syntax, the R code reads very similarly to the equivalent English command. But how are those comparisons being represented in terms of data?\nTo answer this question, consider a simple numeric vector of four integers.\n\na &lt;- c(2, 4, 6, 8)\n\nWe can apply a comparison operator to this vector using the same syntax as above. Let’s compare each value in this vector to see if its less than 5.\n\na &lt; 5\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nThe result is a vector of the same length as a where each value indicates whether the comparison to each element was true or false. While it looks like a factor or a character vector TRUE and FALSE, this is actually our newest entry into the Taxonomy of Data: the logical vector.\n\nclass(a &lt; 5)\n\n[1] \"logical\"\n\n\nA logical vector can only take two values, TRUE and FALSE (R also recognizes T and F but not True or true). While it might seem like a categorical variable with only two levels, a logical vector has an important property that makes it behave like a numerical variable.\n\nsum(a &lt; 5)\n\n[1] 2\n\n\nIn a logical vector, a value of true is represented both by TRUE and by the number 1 and false by FALSE and the number 0. This integer representation is why TRUE + TRUE will work (it’s 2!) but \"TRUE\" + \"TRUE\" will not.\nThis dual representation is very useful because it allows us to compute a proportion using, paradoxically, the mean() function.\n\nmean(a &lt; 5)\n\n[1] 0.5\n\n\na &lt; 5 results in a vector with two 1s and two 0s. When you take the mean like this, you’re really finding the proportion of the elements that meet the condition that you laid out in your comparison. This is a very handy trick. We’ll use it more in a moment.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Conditioning"
    ]
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html#data-pipelines",
    "href": "2-summarizing-data/04-conditioning/notes.html#data-pipelines",
    "title": "Conditioning",
    "section": "Data Pipelines",
    "text": "Data Pipelines\nAt this stage in the course, the number of functions that you are familiar with has grown dramatically. To do truly powerful things with data, you need to not just call one of these functions, but string together many of them in a thoughtful and organized manner.\nAn an example, to create a sorted data frame containing just the large animals, we need to take the original data frame and\n\nfilter() such that log_bodywt &gt; 12 and then\narrange() in descending order of weight (desc(log_bodywt)).\n\nA conventional approach breaks this process into two distinct lines of code and saves the output mid-way through.\n\nmsleep_large &lt;- filter(msleep, log_bodywt &gt; 12)\narrange(msleep_large, desc(log_bodywt))\n\n# A tibble: 9 × 5\n  name                 sleep_total log_bodywt vore  conservation\n  &lt;chr&gt;                      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n1 African elephant             3.3       15.7 herbi vu          \n2 Asian elephant               3.9       14.8 herbi en          \n3 Giraffe                      1.9       13.7 herbi cd          \n4 Pilot whale                  2.7       13.6 carni cd          \n5 Cow                          4         13.3 herbi domesticated\n6 Horse                        2.9       13.2 herbi domesticated\n7 Brazilian tapir              4.4       12.2 herbi vu          \n8 Donkey                       3.1       12.1 herbi domesticated\n9 Bottle-nosed dolphin         5.2       12.1 carni &lt;NA&gt;        \n\n\nAn approach that is more concise, easier to read, and generally faster to run is to compose these functions together with “the pipe”, written |&gt;. If you have two functions, f1 and f2, both of which take a data frame as the first argument, you can pipe the output of f1 directly into f2 using.\n\nf1(DF) |&gt; f2()\n\nLet’s use the pipe to rewrite the code shown above.\n\nfilter(msleep, log_bodywt &gt; 12) |&gt; arrange(desc(log_bodywt))\n\n# A tibble: 9 × 5\n  name                 sleep_total log_bodywt vore  conservation\n  &lt;chr&gt;                      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n1 African elephant             3.3       15.7 herbi vu          \n2 Asian elephant               3.9       14.8 herbi en          \n3 Giraffe                      1.9       13.7 herbi cd          \n4 Pilot whale                  2.7       13.6 carni cd          \n5 Cow                          4         13.3 herbi domesticated\n6 Horse                        2.9       13.2 herbi domesticated\n7 Brazilian tapir              4.4       12.2 herbi vu          \n8 Donkey                       3.1       12.1 herbi domesticated\n9 Bottle-nosed dolphin         5.2       12.1 carni &lt;NA&gt;        \n\n\nWhat has changed? Most immediately, we have reduced two lines of code to one. The first function, filter(), is unchanged however the second function, arrange(), is now missing its first argument, the data frame. That is because it is being piped directly in from the output of the first function.\nWhile this is a fine way to use the pipe, your code is made much more readable if you format it like this:\n\nmsleep |&gt;\n    filter(log_bodywt &gt; 12) |&gt; \n    arrange(desc(log_bodywt))\n\n# A tibble: 9 × 5\n  name                 sleep_total log_bodywt vore  conservation\n  &lt;chr&gt;                      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n1 African elephant             3.3       15.7 herbi vu          \n2 Asian elephant               3.9       14.8 herbi en          \n3 Giraffe                      1.9       13.7 herbi cd          \n4 Pilot whale                  2.7       13.6 carni cd          \n5 Cow                          4         13.3 herbi domesticated\n6 Horse                        2.9       13.2 herbi domesticated\n7 Brazilian tapir              4.4       12.2 herbi vu          \n8 Donkey                       3.1       12.1 herbi domesticated\n9 Bottle-nosed dolphin         5.2       12.1 carni &lt;NA&gt;        \n\n\nThis code results in the same output as the first version, but it now reads a bit like a poem: “Take the msleep data frame then filter it such that the log body weight is greater than twelve then arrange it in descending order by log body weight”.\n\n\nThis poem is admittedly not particularly poetic.\nLet’s look at an example to understand the power of such a simple piece of syntax.\n\nExample 2\nWhat proportion of carnivores sleep more than 8 hours per night?\nAnswering this requires two steps: filter()ing to focus on carnivores and summarize()ing with a proportion that meet a condition (recall that a comparison results in a logical vector of 0s and 1s). It is often a good idea to record the number of observations that go into a summary statistic, which we do here with n().\n\nmsleep |&gt;\n    filter(vore == \"carni\") |&gt;\n    summarize(p_gt_8hrs = mean(sleep_total &gt; 8),\n              n = n())\n\n# A tibble: 1 × 2\n  p_gt_8hrs     n\n      &lt;dbl&gt; &lt;int&gt;\n1     0.684    19\n\n\nWhat year had the greatest total number of christenings?\nThe original arbuthnot data frame, which captures birth records in 17th century London, in fact records the numbers of boys and girls names that appear in church christening records. To find the year with the greatest total number of christenings requires first the creation of a new column with mutate(), then arrange()ing the rows of that data frame, then select()ing just the rows of interst. As one pipeline, that is:\n\nlibrary(stat20data)\n\narbuthnot |&gt;\n    mutate(total = boys + girls) |&gt;\n    arrange(desc(total)) |&gt;\n    select(year, total)\n\n# A tibble: 82 × 2\n    year total\n   &lt;int&gt; &lt;int&gt;\n 1  1705 16145\n 2  1707 16066\n 3  1698 16052\n 4  1708 15862\n 5  1697 15829\n 6  1702 15687\n 7  1701 15616\n 8  1703 15448\n 9  1706 15369\n10  1699 15363\n# ℹ 72 more rows\n\n\nWhat is the trend in the total number of christenings over time?\n\narbuthnot |&gt;\n    mutate(total = boys + girls) |&gt;\n    ggplot(aes(x = year, y = total)) +\n    geom_line()\n\n\n\n\n\n\n\n\nThis demonstrates that you can pipe a data frame directly into a ggplot - the first argument is a data frame after all! The main thing to note is that when moving into a ggplot, the layers are added with the + operator instead of the pipe, |&gt;.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Conditioning"
    ]
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html#groupwise-operations-revisited",
    "href": "2-summarizing-data/04-conditioning/notes.html#groupwise-operations-revisited",
    "title": "Conditioning",
    "section": "Groupwise Operations Revisited…",
    "text": "Groupwise Operations Revisited…\nThe last example above demonstrates a very common scenario: you want to perform some calculations on one particular group of observations in your data set. But what if you want to do that same calculation for every group?\nThe vore variable has four levels: carni, herbi, insecti, and omni. It would not be too difficult to copy and paste the above pipeline four times and modify each filter function to focus on a different group. But what if there were a dozen different levels?\nThis task - performing an operation on all groups of a data set one-by-one - is such a common data science task that nearly every software tool has a good solution. In the tidyverse, the solution is the group_by() function. Let’s see it in action.\n\nmsleep |&gt;\n    group_by(vore) |&gt;\n    summarize(p_gt_8hrs = mean(sleep_total &gt; 8),\n              n = n())\n\n# A tibble: 5 × 3\n  vore    p_gt_8hrs     n\n  &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;\n1 carni       0.684    19\n2 herbi       0.594    32\n3 insecti     1         5\n4 omni        0.95     20\n5 &lt;NA&gt;        0.714     7\n\n\nLike most tidyverse functions, the first argument to group_by() is a data frame, so it can be slotted directly into the pipeline. The second argument, the one that shows up in the code above, is the name of the variable that you want to use to delineate the groups. This is generally a factor, character, or logical vector.\ngroup_by() is an incredibly powerful function because it changes the behavior of downstream functions. Lets break our pipeline and inspect the data frame that comes out of it.\n\nmsleep |&gt;\n    group_by(vore)\n\n# A tibble: 83 × 5\n# Groups:   vore [5]\n   name                       sleep_total log_bodywt vore  conservation\n   &lt;chr&gt;                            &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n 1 Cheetah                           12.1      10.8  carni lc          \n 2 Owl monkey                        17         6.17 omni  &lt;NA&gt;        \n 3 Mountain beaver                   14.4       7.21 herbi nt          \n 4 Greater short-tailed shrew        14.9       2.94 omni  lc          \n 5 Cow                                4        13.3  herbi domesticated\n 6 Three-toed sloth                  14.4       8.26 herbi &lt;NA&gt;        \n 7 Northern fur seal                  8.7       9.93 carni vu          \n 8 Vesper mouse                       7         3.81 &lt;NA&gt;  &lt;NA&gt;        \n 9 Dog                               10.1       9.55 carni domesticated\n10 Roe deer                           3         9.60 herbi lc          \n# ℹ 73 more rows\n\n\nThis looks . . . exactly like the original data frame.\nWell, not exactly like it: there is now a note at the top that the data frame now has the notion of groups based on vore. In effect, group_by() has taken the generic data frame and turned it into the one in the middle below: the same data frame but with rows now flagged as belonging to one group or another. When we pipe this grouped data frame into summarize(), summarize() collapses that data frame down into a single row for each group and creates a new column for each new summary statistic.",
    "crumbs": [
      "Notes",
      "Summarization",
      "Conditioning"
    ]
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html#summary",
    "href": "2-summarizing-data/04-conditioning/notes.html#summary",
    "title": "Conditioning",
    "section": "Summary",
    "text": "Summary\nThere are several ways to subset a data frame but the most important for data analysis is filtering: subsetting the rows according to a condition. In R, that condition is framed in terms of a comparison between a variable and a value (or set of values). Comparisons take many forms and can be combined using logical operators. The result is a logical vector that can be used for filtering or computing summary statistics. You can perform simultaneous analyses on multiple subsets by doing groupwise operations with group_by().\nAs we begin to do analyses that require multiple operations, the pipe operator, |&gt;, can be used to stitch the functions together into a single pipeline.\nIf you’re thinking, 😬 , yikes there was a lot of coding in these notes, you’re right. Keep reading, you’ll have an opportunity for some practice in the tutorial.\nContinue on to the tutorial portion of the notes",
    "crumbs": [
      "Notes",
      "Summarization",
      "Conditioning"
    ]
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html#footnotes",
    "href": "2-summarizing-data/04-conditioning/notes.html#footnotes",
    "title": "Conditioning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nV. M. Savage and G. B. West. A quantitative, theoretical framework for understanding mammalian sleep. Proceedings of the National Academy of Sciences, 104 (3):1051-1056, 2007.↩︎",
    "crumbs": [
      "Notes",
      "Summarization",
      "Conditioning"
    ]
  }
]